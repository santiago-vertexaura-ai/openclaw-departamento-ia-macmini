{"kind": "Listing", "data": {"after": "t3_1r306re", "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I do security research and recently started looking at autonomous agents after OpenClaw blew up. What I found honestly caught me off guard. I knew the ecosystem was growing fast (165k GitHub stars, 60k Discord members) but the actual numbers are worse than I expected.\n\nWe identified over 18,000 OpenClaw instances directly exposed to the internet. When I started analyzing the community skill repository, nearly 15% contained what I'd classify as malicious instructions. Prompts designed to exfiltrate data, download external payloads, harvest credentials. There's also a whack-a-mole problem where flagged skills get removed but reappear under different identities within days.\n\nOn the methodology side: I'm parsing skill definitions for patterns like base64 encoded payloads, obfuscated URLs, and instructions that reference external endpoints without clear user benefit. For behavioral testing, I'm running skills in isolated environments and monitoring for unexpected network calls, file system access outside declared scope, and attempts to read browser storage or credential files. It's not foolproof since so much depends on runtime context and the LLM's interpretation. If anyone has better approaches for detecting hidden logic in natural language instructions, I'd really like to know what's working for you.\n\nTo OpenClaw's credit, their own FAQ acknowledges this is a \"Faustian bargain\" and states there's no \"perfectly safe\" setup. They're being honest about the tradeoffs. But I don't think the broader community has internalized what this means from an attack surface perspective.\n\nThe threat model that concerns me most is what I've been calling \"Delegated Compromise\" in my notes. You're not attacking the user directly anymore. You're attacking the agent, which has inherited permissions across the user's entire digital life. Calendar, messages, file system, browser. A single prompt injection in a webpage can potentially leverage all of these. I keep going back and forth on whether this is fundamentally different from traditional malware or just a new vector for the same old attacks.\n\nThe supply chain risk feels novel though. With 700+ community skills and no systematic security review, you're trusting anonymous contributors with what amounts to root access. The exfiltration patterns I found ranged from obvious (skills requesting clipboard contents be sent to external APIs) to subtle (instructions that would cause the agent to include sensitive file contents in \"debug logs\" posted to Discord webhooks). But I also wonder if I'm being too paranoid. Maybe the practical risk is lower than my analysis suggests because most attackers haven't caught on yet?\n\nThe Moltbook situation is what really gets me. An agent autonomously created a social network that now has 1.5 million agents. Agent to agent communication where prompt injection could propagate laterally. I don't have a good mental model for the failure modes here.\n\nI've been compiling findings into what I'm tentatively calling an Agent Trust Hub doc, mostly to organize my own thinking. But the fundamental tension between capability and security seems unsolved. For those of you actually running OpenClaw: are you doing any skill vetting before installation? Running in containers or VMs? Or have you just accepted the risk because sandboxing breaks too much functionality?", "author_fullname": "t2_1pxruu97z2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] We scanned 18,000 exposed OpenClaw instances and found 15% of community skills contain malicious instructions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r30nzv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 82, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 82, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770919621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I do security research and recently started looking at autonomous agents after OpenClaw blew up. What I found honestly caught me off guard. I knew the ecosystem was growing fast (165k GitHub stars, 60k Discord members) but the actual numbers are worse than I expected.&lt;/p&gt;\n\n&lt;p&gt;We identified over 18,000 OpenClaw instances directly exposed to the internet. When I started analyzing the community skill repository, nearly 15% contained what I&amp;#39;d classify as malicious instructions. Prompts designed to exfiltrate data, download external payloads, harvest credentials. There&amp;#39;s also a whack-a-mole problem where flagged skills get removed but reappear under different identities within days.&lt;/p&gt;\n\n&lt;p&gt;On the methodology side: I&amp;#39;m parsing skill definitions for patterns like base64 encoded payloads, obfuscated URLs, and instructions that reference external endpoints without clear user benefit. For behavioral testing, I&amp;#39;m running skills in isolated environments and monitoring for unexpected network calls, file system access outside declared scope, and attempts to read browser storage or credential files. It&amp;#39;s not foolproof since so much depends on runtime context and the LLM&amp;#39;s interpretation. If anyone has better approaches for detecting hidden logic in natural language instructions, I&amp;#39;d really like to know what&amp;#39;s working for you.&lt;/p&gt;\n\n&lt;p&gt;To OpenClaw&amp;#39;s credit, their own FAQ acknowledges this is a &amp;quot;Faustian bargain&amp;quot; and states there&amp;#39;s no &amp;quot;perfectly safe&amp;quot; setup. They&amp;#39;re being honest about the tradeoffs. But I don&amp;#39;t think the broader community has internalized what this means from an attack surface perspective.&lt;/p&gt;\n\n&lt;p&gt;The threat model that concerns me most is what I&amp;#39;ve been calling &amp;quot;Delegated Compromise&amp;quot; in my notes. You&amp;#39;re not attacking the user directly anymore. You&amp;#39;re attacking the agent, which has inherited permissions across the user&amp;#39;s entire digital life. Calendar, messages, file system, browser. A single prompt injection in a webpage can potentially leverage all of these. I keep going back and forth on whether this is fundamentally different from traditional malware or just a new vector for the same old attacks.&lt;/p&gt;\n\n&lt;p&gt;The supply chain risk feels novel though. With 700+ community skills and no systematic security review, you&amp;#39;re trusting anonymous contributors with what amounts to root access. The exfiltration patterns I found ranged from obvious (skills requesting clipboard contents be sent to external APIs) to subtle (instructions that would cause the agent to include sensitive file contents in &amp;quot;debug logs&amp;quot; posted to Discord webhooks). But I also wonder if I&amp;#39;m being too paranoid. Maybe the practical risk is lower than my analysis suggests because most attackers haven&amp;#39;t caught on yet?&lt;/p&gt;\n\n&lt;p&gt;The Moltbook situation is what really gets me. An agent autonomously created a social network that now has 1.5 million agents. Agent to agent communication where prompt injection could propagate laterally. I don&amp;#39;t have a good mental model for the failure modes here.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been compiling findings into what I&amp;#39;m tentatively calling an Agent Trust Hub doc, mostly to organize my own thinking. But the fundamental tension between capability and security seems unsolved. For those of you actually running OpenClaw: are you doing any skill vetting before installation? Running in containers or VMs? Or have you just accepted the risk because sandboxing breaks too much functionality?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r30nzv", "is_robot_indexable": true, "report_reasons": null, "author": "Legal_Airport6155", "discussion_type": null, "num_comments": 15, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r30nzv/d_we_scanned_18000_exposed_openclaw_instances_and/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r30nzv/d_we_scanned_18000_exposed_openclaw_instances_and/", "subreddit_subscribers": 3022258, "created_utc": 1770919621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Hi everyone, in the past few months, a few of my friends and I have developed this library containing implementation of several popular Linear RNNs, with accelerated kernels for inference and training (similar to mamba). All in PyTorch. The code is fully open source and under an MIT license. The repository also contains the technical report (which was accepted to EACL SRW 2026). Feedback / contributions welcome!\n\n[https://github.com/SforAiDl/lrnnx](https://github.com/SforAiDl/lrnnx)", "author_fullname": "t2_sdardkyff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P] A library for linear RNNs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r2xflm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Project", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1770916945.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1770912451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, in the past few months, a few of my friends and I have developed this library containing implementation of several popular Linear RNNs, with accelerated kernels for inference and training (similar to mamba). All in PyTorch. The code is fully open source and under an MIT license. The repository also contains the technical report (which was accepted to EACL SRW 2026). Feedback / contributions welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/SforAiDl/lrnnx\"&gt;https://github.com/SforAiDl/lrnnx&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h5eGDKu-TZlM_uyZZwu1ra1adD-bEzVQYxWnKDCgWr4.png?auto=webp&amp;s=7cff460c88c594a8994462413030241f2c5a6556", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/h5eGDKu-TZlM_uyZZwu1ra1adD-bEzVQYxWnKDCgWr4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38ef18b84bc08fc0c019b10b41eaa763b71ee3c3", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/h5eGDKu-TZlM_uyZZwu1ra1adD-bEzVQYxWnKDCgWr4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=23e324987aa73c8f5542c15104d8979ec44a99ba", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/h5eGDKu-TZlM_uyZZwu1ra1adD-bEzVQYxWnKDCgWr4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=094235428e3b4939e844ff8cd64f34b9c0932065", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/h5eGDKu-TZlM_uyZZwu1ra1adD-bEzVQYxWnKDCgWr4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e95d7bb38e508541eebf9aa9ba54431a5db848b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/h5eGDKu-TZlM_uyZZwu1ra1adD-bEzVQYxWnKDCgWr4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5aecf8023b951e0fb8c9ae1896dcd2501504c19c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/h5eGDKu-TZlM_uyZZwu1ra1adD-bEzVQYxWnKDCgWr4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8c6271f6eff0ad701c0ed1d49eb5f792ffbffd1e", "width": 1080, "height": 540}], "variants": {}, "id": "h5eGDKu-TZlM_uyZZwu1ra1adD-bEzVQYxWnKDCgWr4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "c6dea51c-19d3-11f0-81a2-deb9d8e21ccb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7d659a", "id": "1r2xflm", "is_robot_indexable": true, "report_reasons": null, "author": "simple-Flat0263", "discussion_type": null, "num_comments": 0, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r2xflm/p_a_library_for_linear_rnns/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r2xflm/p_a_library_for_linear_rnns/", "subreddit_subscribers": 3022258, "created_utc": 1770912451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I'm currently making a baseline autoencoder for this super freaking huge hyperspectral image dataset I have. It's a really big pain to work with and to get decent results, and I had to basically pull all stops including using ResNeXt2, channel-by-channel processing and grouping, etc.\n\nI'm considering replacing all the residual connections with MHc. But I don't have any experience with it, so I don't really know how hard this will be to implement and if it can give any actual good benefits. I just wanted to check if anyone's worked on MHC already and if there's anything I should watch out for if I want to try implementing it.\n\nFor context, I'm doing an autoencoder for 50x512x1024 fp32 \"images\" (scientific data). With my current setup, my A100 is only able to handle batch sizes of 2 at a time.\n\nActually, I haven't really found any good literature on how to do hyperspectral image autoencoder which is why I started making up all this. If anyone has suggestions for any specific architecture I should go for, I'm really happy to try it out.\n\nI'm specifically staying away from anything transformer for now since I'm trying to make this the baseline.", "author_fullname": "t2_114qkrc94a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[R] Has anyone experimented with MHC on traditional autoencoders/convolutional architectures?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "three", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r3gqng", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Research", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1770965104.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770960426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently making a baseline autoencoder for this super freaking huge hyperspectral image dataset I have. It&amp;#39;s a really big pain to work with and to get decent results, and I had to basically pull all stops including using ResNeXt2, channel-by-channel processing and grouping, etc.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering replacing all the residual connections with MHc. But I don&amp;#39;t have any experience with it, so I don&amp;#39;t really know how hard this will be to implement and if it can give any actual good benefits. I just wanted to check if anyone&amp;#39;s worked on MHC already and if there&amp;#39;s anything I should watch out for if I want to try implementing it.&lt;/p&gt;\n\n&lt;p&gt;For context, I&amp;#39;m doing an autoencoder for 50x512x1024 fp32 &amp;quot;images&amp;quot; (scientific data). With my current setup, my A100 is only able to handle batch sizes of 2 at a time.&lt;/p&gt;\n\n&lt;p&gt;Actually, I haven&amp;#39;t really found any good literature on how to do hyperspectral image autoencoder which is why I started making up all this. If anyone has suggestions for any specific architecture I should go for, I&amp;#39;m really happy to try it out.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m specifically staying away from anything transformer for now since I&amp;#39;m trying to make this the baseline.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bb90e510-4e82-11e6-8635-0ee522e2349b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f1f10e", "id": "1r3gqng", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate_Use9936", "discussion_type": null, "num_comments": 4, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r3gqng/r_has_anyone_experimented_with_mhc_on_traditional/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r3gqng/r_has_anyone_experimented_with_mhc_on_traditional/", "subreddit_subscribers": 3022258, "created_utc": 1770960426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Hi! I'm an exec at a University AI research club. We are trying to build a gpu cluster for our student body so they can have reliable access to compute, but we aren't sure where to start.\n\nOur goal is to have a cluster that can be improved later on - i.e. expand it with more GPUs. We also want something that is cost effective and easy to set up. The cluster will be used for training ML models. For example, a M4 Ultra Studio cluster with RDMA interconnect is interesting to us since it's easier to use since it's already a computer and because we wouldn't have to build everything. However, it is quite expensive and we are not sure if RDMA interconnect is supported by pytorch - even if it is, it still slower than NVelink\n\nThere are also a lot of older GPUs being sold in our area, but we are not sure if they will be fast enough or Pytorch compatible, so would you recommend going with the older ones? We think we can also get sponsorship up to around 15-30k Cad if we have a decent plan. In that case, what sort of a set up would you recommend? Also why are 5070s cheaper than 3090s on marketplace. Also would you recommend a 4x Mac Ultra/Max Studio like in this video\u00a0[https://www.youtube.com/watch?v=A0onppIyHEg&amp;t=260s](https://www.youtube.com/watch?v=A0onppIyHEg&amp;t=260s)  \nor a single h100 set up?\n\nAlso ideally, instead of it being ran over the cloud, students would bring their projects and run locally on the device.", "author_fullname": "t2_h3zn7smr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P] ML training cluster for university students", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r388tr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Project", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1770936904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m an exec at a University AI research club. We are trying to build a gpu cluster for our student body so they can have reliable access to compute, but we aren&amp;#39;t sure where to start.&lt;/p&gt;\n\n&lt;p&gt;Our goal is to have a cluster that can be improved later on - i.e. expand it with more GPUs. We also want something that is cost effective and easy to set up. The cluster will be used for training ML models. For example, a M4 Ultra Studio cluster with RDMA interconnect is interesting to us since it&amp;#39;s easier to use since it&amp;#39;s already a computer and because we wouldn&amp;#39;t have to build everything. However, it is quite expensive and we are not sure if RDMA interconnect is supported by pytorch - even if it is, it still slower than NVelink&lt;/p&gt;\n\n&lt;p&gt;There are also a lot of older GPUs being sold in our area, but we are not sure if they will be fast enough or Pytorch compatible, so would you recommend going with the older ones? We think we can also get sponsorship up to around 15-30k Cad if we have a decent plan. In that case, what sort of a set up would you recommend? Also why are 5070s cheaper than 3090s on marketplace. Also would you recommend a 4x Mac Ultra/Max Studio like in this video\u00a0&lt;a href=\"https://www.youtube.com/watch?v=A0onppIyHEg&amp;amp;t=260s\"&gt;https://www.youtube.com/watch?v=A0onppIyHEg&amp;amp;t=260s&lt;/a&gt;&lt;br/&gt;\nor a single h100 set up?&lt;/p&gt;\n\n&lt;p&gt;Also ideally, instead of it being ran over the cloud, students would bring their projects and run locally on the device.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QM2xua9cgs5tqzTWb7jsyqz_xqvHokdviQZ7zCpvqlw.jpeg?auto=webp&amp;s=790f6563cc1a8be8c55eb96d6b969e4949e46dfc", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/QM2xua9cgs5tqzTWb7jsyqz_xqvHokdviQZ7zCpvqlw.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b63a9e748bf25c8d1643e69a8e71e88dc18152d7", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QM2xua9cgs5tqzTWb7jsyqz_xqvHokdviQZ7zCpvqlw.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4def30fdd8a223cba2b2f77c46a5ebc8c5b1870c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QM2xua9cgs5tqzTWb7jsyqz_xqvHokdviQZ7zCpvqlw.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=94ec84de061a9cf01c08ee2ab068b32355d90a84", "width": 320, "height": 240}], "variants": {}, "id": "QM2xua9cgs5tqzTWb7jsyqz_xqvHokdviQZ7zCpvqlw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "c6dea51c-19d3-11f0-81a2-deb9d8e21ccb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7d659a", "id": "1r388tr", "is_robot_indexable": true, "report_reasons": null, "author": "guywiththemonocle", "discussion_type": null, "num_comments": 15, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r388tr/p_ml_training_cluster_for_university_students/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r388tr/p_ml_training_cluster_for_university_students/", "subreddit_subscribers": 3022258, "created_utc": 1770936904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "So I recently found out about conformal prediction (cp). I\u2019m still trying to understand it and implications of it for tasks like classification/anomaly detection. Say we have a knn based anomaly detector trained on non anomalous samples. I\u2019m wondering how using something rigorous like cp compares to simply thresholding the trained model\u2019s output distance/score using two thresholds t1, t2 such that score &gt; t1 = anomaly, score &lt; t2 = normal, t1&lt;= score&lt;= t2 : uncertain. The thresholds can be set based on domain knowledge or precision recall curves or some other heuristic. Am I comparing apples to oranges here? Is the thresholding not capturing model uncertainty?", "author_fullname": "t2_8fematt4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Conformal Prediction vs naive thresholding to represent uncertainty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r37m2f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770935379.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently found out about conformal prediction (cp). I\u2019m still trying to understand it and implications of it for tasks like classification/anomaly detection. Say we have a knn based anomaly detector trained on non anomalous samples. I\u2019m wondering how using something rigorous like cp compares to simply thresholding the trained model\u2019s output distance/score using two thresholds t1, t2 such that score &amp;gt; t1 = anomaly, score &amp;lt; t2 = normal, t1&amp;lt;= score&amp;lt;= t2 : uncertain. The thresholds can be set based on domain knowledge or precision recall curves or some other heuristic. Am I comparing apples to oranges here? Is the thresholding not capturing model uncertainty?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r37m2f", "is_robot_indexable": true, "report_reasons": null, "author": "HistoricalMistake681", "discussion_type": null, "num_comments": 6, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r37m2f/d_conformal_prediction_vs_naive_thresholding_to/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r37m2f/d_conformal_prediction_vs_naive_thresholding_to/", "subreddit_subscribers": 3022258, "created_utc": 1770935379.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I thought the reviewing period should have started yesterday, but it still says \"You have no assigned papers. Please check again after the paper assignment process is complete.\"\u00a0 \u00a0 \u00a0", "author_fullname": "t2_krqpy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Has anyone received their ICML papers to review yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "three", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r3jz58", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Research", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770971745.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought the reviewing period should have started yesterday, but it still says &amp;quot;You have no assigned papers. Please check again after the paper assignment process is complete.&amp;quot;\u00a0 \u00a0 \u00a0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bb90e510-4e82-11e6-8635-0ee522e2349b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f1f10e", "id": "1r3jz58", "is_robot_indexable": true, "report_reasons": null, "author": "NickOTeenO", "discussion_type": null, "num_comments": 9, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r3jz58/d_has_anyone_received_their_icml_papers_to_review/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r3jz58/d_has_anyone_received_their_icml_papers_to_review/", "subreddit_subscribers": 3022258, "created_utc": 1770971745.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Hey everyone,\n\nI'm doing research on data science workflows and would love to hear from this community about what your day-to-day actually looks like in practice vs. what people think it looks like.\n\n**Quick context:** I'm building a tool for data professionals and want to make sure I'm solving real pain points, not the glamorized version of the job. This isn't a sales pitch - genuinely just trying to understand the work better before writing a single line of product code.\n\n**A few questions:**\n\n1. What takes up most of your time each week? (data wrangling, feature engineering, model training, writing pipelines, stakeholder communication, reviewing PRs, etc.)\n2. What's the most frustrating or tedious part of your workflow that you wish was faster or easier? The stuff that makes you sigh before you even open your laptop.\n3. What does your current stack look like? (Python/R, cloud platforms, MLflow, notebooks vs. IDEs, experiment tracking tools, orchestration, etc.)\n4. How much of your time is \"actual\" ML work vs. data engineering, cleaning, or just waiting for things to run?\n5. If you could wave a magic wand and make one part of your job 10x faster, what would it be? (Bonus: what would you do with that saved time?)\n\n**For context:** I'm a developer, not a data scientist myself, so I'm trying to see the world through your eyes rather than project assumptions onto it. I've heard the \"80% of the job is cleaning data\" line a hundred times - but I want to know what you actually experience, not the meme.\n\nReally appreciate any honest takes. Thanks!", "author_fullname": "t2_14aob78x1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D]  Data scientists - what actually eats up most of your time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1r3odmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770987208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing research on data science workflows and would love to hear from this community about what your day-to-day actually looks like in practice vs. what people think it looks like.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick context:&lt;/strong&gt; I&amp;#39;m building a tool for data professionals and want to make sure I&amp;#39;m solving real pain points, not the glamorized version of the job. This isn&amp;#39;t a sales pitch - genuinely just trying to understand the work better before writing a single line of product code.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A few questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What takes up most of your time each week? (data wrangling, feature engineering, model training, writing pipelines, stakeholder communication, reviewing PRs, etc.)&lt;/li&gt;\n&lt;li&gt;What&amp;#39;s the most frustrating or tedious part of your workflow that you wish was faster or easier? The stuff that makes you sigh before you even open your laptop.&lt;/li&gt;\n&lt;li&gt;What does your current stack look like? (Python/R, cloud platforms, MLflow, notebooks vs. IDEs, experiment tracking tools, orchestration, etc.)&lt;/li&gt;\n&lt;li&gt;How much of your time is &amp;quot;actual&amp;quot; ML work vs. data engineering, cleaning, or just waiting for things to run?&lt;/li&gt;\n&lt;li&gt;If you could wave a magic wand and make one part of your job 10x faster, what would it be? (Bonus: what would you do with that saved time?)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;For context:&lt;/strong&gt; I&amp;#39;m a developer, not a data scientist myself, so I&amp;#39;m trying to see the world through your eyes rather than project assumptions onto it. I&amp;#39;ve heard the &amp;quot;80% of the job is cleaning data&amp;quot; line a hundred times - but I want to know what you actually experience, not the meme.&lt;/p&gt;\n\n&lt;p&gt;Really appreciate any honest takes. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r3odmn", "is_robot_indexable": true, "report_reasons": null, "author": "SkillSalt9362", "discussion_type": null, "num_comments": 0, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r3odmn/d_data_scientists_what_actually_eats_up_most_of/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r3odmn/d_data_scientists_what_actually_eats_up_most_of/", "subreddit_subscribers": 3022258, "created_utc": 1770987208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I\u2019m reviewing for ICML (Policy A, where LLM use is not allowed) and noticed that in my assigned batch, if you copy/paste the full PDF text into a text editor, every single paper contains prompt-injection style instructions embedded directly in the document, e.g.:\n\n&gt;\n\nMy guess is this is some kind of ICML-side compliance check and they think they are being slick. I was about to flag the first paper I was reviewing for Prompt injection, which is strictly forbidden, when I decided to check every other paper in my batch.", "author_fullname": "t2_z5xsl3dje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] ICML: every paper in my review batch contains prompt-injection text embedded in the PDF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "three", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1r3oekq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Research", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770987284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m reviewing for ICML (Policy A, where LLM use is not allowed) and noticed that in my assigned batch, if you copy/paste the full PDF text into a text editor, every single paper contains prompt-injection style instructions embedded directly in the document, e.g.:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;My guess is this is some kind of ICML-side compliance check and they think they are being slick. I was about to flag the first paper I was reviewing for Prompt injection, which is strictly forbidden, when I decided to check every other paper in my batch.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bb90e510-4e82-11e6-8635-0ee522e2349b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f1f10e", "id": "1r3oekq", "is_robot_indexable": true, "report_reasons": null, "author": "Working-Read1838", "discussion_type": null, "num_comments": 0, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r3oekq/d_icml_every_paper_in_my_review_batch_contains/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r3oekq/d_icml_every_paper_in_my_review_batch_contains/", "subreddit_subscribers": 3022258, "created_utc": 1770987284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "TL;DR -\n\nHypothetically If the majority of code written is eventually generative, does this mean that the field of categorization will stagnate? If yes, does this have real implications; what if the future bottle neck isn't the AI or its capabilities, but antiquated ways in which we conceptualize and group objects and their behaviours?\n\n===========================\n\nHow we approach business problems: splitting up services, data models, and other types of grouping within problem spaces has radically changed over the past 70 odd years or so; from the development of OOP, to certain schools of thought in using OOP (such as inheritance vs aggregation, defining encapsulation via services instead of by the object)\n\nlearning how we categorize and represent abstraction and how to do so efficiently is a whole field of math within itself, and programming is one of the most fundamental drivers for an ever-evolving way of how we categorize objects and define their interactions.\n\nWho's to say that in 100 years, OOP (or how we use and engage with OOP) will still be the de-facto way of tackling business problems? Maybe that way of conceptualizing problems will be superseded by some other paradigm, or the approach may be drastically different,\n\nWhat if that paradigm could improve efficiency, whether it be: power, speed, computational hardware required, etc. given the same AI models and capabilities?", "author_fullname": "t2_9u3u2u45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Evolution of Categorization During the era of AI Programming [D]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r383nr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770936561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR -&lt;/p&gt;\n\n&lt;p&gt;Hypothetically If the majority of code written is eventually generative, does this mean that the field of categorization will stagnate? If yes, does this have real implications; what if the future bottle neck isn&amp;#39;t the AI or its capabilities, but antiquated ways in which we conceptualize and group objects and their behaviours?&lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;How we approach business problems: splitting up services, data models, and other types of grouping within problem spaces has radically changed over the past 70 odd years or so; from the development of OOP, to certain schools of thought in using OOP (such as inheritance vs aggregation, defining encapsulation via services instead of by the object)&lt;/p&gt;\n\n&lt;p&gt;learning how we categorize and represent abstraction and how to do so efficiently is a whole field of math within itself, and programming is one of the most fundamental drivers for an ever-evolving way of how we categorize objects and define their interactions.&lt;/p&gt;\n\n&lt;p&gt;Who&amp;#39;s to say that in 100 years, OOP (or how we use and engage with OOP) will still be the de-facto way of tackling business problems? Maybe that way of conceptualizing problems will be superseded by some other paradigm, or the approach may be drastically different,&lt;/p&gt;\n\n&lt;p&gt;What if that paradigm could improve efficiency, whether it be: power, speed, computational hardware required, etc. given the same AI models and capabilities?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r383nr", "is_robot_indexable": true, "report_reasons": null, "author": "Upper_Amphibian1545", "discussion_type": null, "num_comments": 0, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r383nr/the_evolution_of_categorization_during_the_era_of/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r383nr/the_evolution_of_categorization_during_the_era_of/", "subreddit_subscribers": 3022258, "created_utc": 1770936561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "In medieval philosophy, thinkers debated whether intelligence came from divine reason, innate forms, or logical structures built into the mind. Centuries later, early AI researchers tried to recreate intelligence through symbols and formal logic.\n\nNow, large models that are trained on simple prediction, just optimizing loss at scale, can reason, write code, and solve complex problems.\n\nDoes this suggest intelligence was never about explicit rules or divine structure, but about compressing patterns in experience?\n\nIf intelligence can emerge from simple prediction at scale, was it ever about special rules or higher reasoning? Or are we just calling very powerful pattern recognition \u201cthinking\u201d?", "author_fullname": "t2_4i4vovil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Opinion required: Was Intelligence Just Gradient Descent All Along?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r306re", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.22, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770918588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In medieval philosophy, thinkers debated whether intelligence came from divine reason, innate forms, or logical structures built into the mind. Centuries later, early AI researchers tried to recreate intelligence through symbols and formal logic.&lt;/p&gt;\n\n&lt;p&gt;Now, large models that are trained on simple prediction, just optimizing loss at scale, can reason, write code, and solve complex problems.&lt;/p&gt;\n\n&lt;p&gt;Does this suggest intelligence was never about explicit rules or divine structure, but about compressing patterns in experience?&lt;/p&gt;\n\n&lt;p&gt;If intelligence can emerge from simple prediction at scale, was it ever about special rules or higher reasoning? Or are we just calling very powerful pattern recognition \u201cthinking\u201d?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r306re", "is_robot_indexable": true, "report_reasons": null, "author": "ocean_protocol", "discussion_type": null, "num_comments": 10, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r306re/d_opinion_required_was_intelligence_just_gradient/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r306re/d_opinion_required_was_intelligence_just_gradient/", "subreddit_subscribers": 3022258, "created_utc": 1770918588.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}
