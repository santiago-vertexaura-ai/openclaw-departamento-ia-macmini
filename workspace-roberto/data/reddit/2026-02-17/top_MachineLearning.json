{"kind": "Listing", "data": {"after": "t3_1r6zq50", "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Throwaway because I work in security and don't want this tied to my main.\n\nA few colleagues and I have been poking at autonomous agent frameworks as a side project, mostly out of morbid curiosity after seeing OpenClaw blow up (165K GitHub stars, 60K Discord members, 230K followers on X, 700+ community skills). What we found genuinely alarmed us.\n\nWe identified over 18,000 OpenClaw instances exposed directly to the public internet. But the scarier part: when we audited community built skills, nearly 15% contained what we'd classify as malicious instructions. We're talking prompts designed to download malware, exfiltrate sensitive data, or steal credentials. And there's this frustrating pattern where malicious skills get flagged, removed, then reappear under new identities within days. It's endless.\n\nThe attack surface here is qualitatively different from traditional software vulnerabilities and I don't think the ML community has fully internalized this. These agents have delegated authority over local files, browsers, and messaging platforms (WhatsApp, Slack, Discord, Telegram). A single compromised skill doesn't just affect the skill's functionality; it potentially compromises everything the agent can touch. Attackers don't need to target you directly anymore, they target the agent and inherit its permissions.\n\nPrompt injection is the obvious vector everyone talks about, but the supply chain risk from community skills is what's actually keeping me up at night. Unlike npm packages or PyPI modules where there's at least some security tooling and community review norms, agent skills are essentially unreviewed prompt bundles with execution capabilities. The OpenClaw FAQ itself acknowledges this is a \"Faustian bargain\" with no \"perfectly safe\" setup. At least they're honest about it, but adoption is outpacing any reasonable security review.\n\nThere's also this failure mode we've been calling \"judgment hallucination\" internally. Users anthropomorphize these systems and over delegate authority because the agent appears to reason competently. I've watched colleagues give these things access to their entire digital lives because \"it seems smart.\" The trust calibration problem is severe and I don't see anyone working on it seriously.\n\nI've been digging around for any standardized approach to evaluating agent security posture. Found some scattered resources like OWASP's LLM guidelines, a few academic papers on prompt injection taxonomies, and stumbled across something called Agent Trust Hub that's trying to catalog these risks. But honestly the whole space feels fragmented. We're building the plane while flying it and nobody agrees on what the instruments should even measure.\n\nSeriously though, has anyone here audited other agent frameworks like AutoGPT or BabyAGI for similar issues? And for those running agents in production, what does your threat model actually look like? I'm curious whether people are treating these as trusted code execution environments or sandboxing them properly.", "author_fullname": "t2_1tkem78z5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] We found 18K+ exposed OpenClaw instances and ~15% of community skills contain malicious instructionsc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6ge7h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 98, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 98, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771263948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Throwaway because I work in security and don&amp;#39;t want this tied to my main.&lt;/p&gt;\n\n&lt;p&gt;A few colleagues and I have been poking at autonomous agent frameworks as a side project, mostly out of morbid curiosity after seeing OpenClaw blow up (165K GitHub stars, 60K Discord members, 230K followers on X, 700+ community skills). What we found genuinely alarmed us.&lt;/p&gt;\n\n&lt;p&gt;We identified over 18,000 OpenClaw instances exposed directly to the public internet. But the scarier part: when we audited community built skills, nearly 15% contained what we&amp;#39;d classify as malicious instructions. We&amp;#39;re talking prompts designed to download malware, exfiltrate sensitive data, or steal credentials. And there&amp;#39;s this frustrating pattern where malicious skills get flagged, removed, then reappear under new identities within days. It&amp;#39;s endless.&lt;/p&gt;\n\n&lt;p&gt;The attack surface here is qualitatively different from traditional software vulnerabilities and I don&amp;#39;t think the ML community has fully internalized this. These agents have delegated authority over local files, browsers, and messaging platforms (WhatsApp, Slack, Discord, Telegram). A single compromised skill doesn&amp;#39;t just affect the skill&amp;#39;s functionality; it potentially compromises everything the agent can touch. Attackers don&amp;#39;t need to target you directly anymore, they target the agent and inherit its permissions.&lt;/p&gt;\n\n&lt;p&gt;Prompt injection is the obvious vector everyone talks about, but the supply chain risk from community skills is what&amp;#39;s actually keeping me up at night. Unlike npm packages or PyPI modules where there&amp;#39;s at least some security tooling and community review norms, agent skills are essentially unreviewed prompt bundles with execution capabilities. The OpenClaw FAQ itself acknowledges this is a &amp;quot;Faustian bargain&amp;quot; with no &amp;quot;perfectly safe&amp;quot; setup. At least they&amp;#39;re honest about it, but adoption is outpacing any reasonable security review.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also this failure mode we&amp;#39;ve been calling &amp;quot;judgment hallucination&amp;quot; internally. Users anthropomorphize these systems and over delegate authority because the agent appears to reason competently. I&amp;#39;ve watched colleagues give these things access to their entire digital lives because &amp;quot;it seems smart.&amp;quot; The trust calibration problem is severe and I don&amp;#39;t see anyone working on it seriously.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been digging around for any standardized approach to evaluating agent security posture. Found some scattered resources like OWASP&amp;#39;s LLM guidelines, a few academic papers on prompt injection taxonomies, and stumbled across something called Agent Trust Hub that&amp;#39;s trying to catalog these risks. But honestly the whole space feels fragmented. We&amp;#39;re building the plane while flying it and nobody agrees on what the instruments should even measure.&lt;/p&gt;\n\n&lt;p&gt;Seriously though, has anyone here audited other agent frameworks like AutoGPT or BabyAGI for similar issues? And for those running agents in production, what does your threat model actually look like? I&amp;#39;m curious whether people are treating these as trusted code execution environments or sandboxing them properly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r6ge7h", "is_robot_indexable": true, "report_reasons": null, "author": "New-Needleworker1755", "discussion_type": null, "num_comments": 29, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r6ge7h/d_we_found_18k_exposed_openclaw_instances_and_15/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6ge7h/d_we_found_18k_exposed_openclaw_instances_and_15/", "subreddit_subscribers": 3023273, "created_utc": 1771263948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I just want to ask PhDs in AI on this sub, how much does your supervisor support your phd ?\n\nIn term of research output, how much help do you get from your supervisor?  Only ambigious direction (e.g. Active Learning/RL for   architecture X)? Or more details idea, like the research gap itself? If you meet a certain problem (e.g. cannot solve X because too hard to solve), do they give you any help, like potential solution direction to try, or just tell you \"please do something about it\"? How often do their suggestion actually help you?\n\nIf they don't help much, do they ask their post doc or other student to collaborate/help you solve the problem?\n\nDo they have KPI for you? (E.g. number of finished work per year?) \n\n\n\nIn term of networking/connection, how much do he/she help you? ", "author_fullname": "t2_bdwiai7gw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Supervisor support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6dzz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1771258973.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771258799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just want to ask PhDs in AI on this sub, how much does your supervisor support your phd ?&lt;/p&gt;\n\n&lt;p&gt;In term of research output, how much help do you get from your supervisor?  Only ambigious direction (e.g. Active Learning/RL for   architecture X)? Or more details idea, like the research gap itself? If you meet a certain problem (e.g. cannot solve X because too hard to solve), do they give you any help, like potential solution direction to try, or just tell you &amp;quot;please do something about it&amp;quot;? How often do their suggestion actually help you?&lt;/p&gt;\n\n&lt;p&gt;If they don&amp;#39;t help much, do they ask their post doc or other student to collaborate/help you solve the problem?&lt;/p&gt;\n\n&lt;p&gt;Do they have KPI for you? (E.g. number of finished work per year?) &lt;/p&gt;\n\n&lt;p&gt;In term of networking/connection, how much do he/she help you? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r6dzz7", "is_robot_indexable": true, "report_reasons": null, "author": "_karma_collector", "discussion_type": null, "num_comments": 23, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r6dzz7/d_supervisor_support/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6dzz7/d_supervisor_support/", "subreddit_subscribers": 3023273, "created_utc": 1771258799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Various venues offer, or have in the past offered, the opportunity to submit short papers, often with a four pages page limit. This is currently true of the ACL.\n\nShort papers are not long papers, and there are usually explicit requirements as to how they should be treated differently by reviewers. See for example http://aclrollingreview.org/cfp section on short papers. \n\nQuestion to anyone who has submitted short papers in the past, do you think your paper was reviewed fairly as a short paper? I know we've all had some bad experiences with subletting any kind of paper, but do you think on average the reviewers understood the assignment and evaluated your work based on the criteria for short papers? \n\nI think it's true that ICLR used to have a short papers track and removed it. Does anyone know why it was removed?", "author_fullname": "t2_4ei0invc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Short Paper Reviews [R]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "three", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6lgap", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Research", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771274869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Various venues offer, or have in the past offered, the opportunity to submit short papers, often with a four pages page limit. This is currently true of the ACL.&lt;/p&gt;\n\n&lt;p&gt;Short papers are not long papers, and there are usually explicit requirements as to how they should be treated differently by reviewers. See for example &lt;a href=\"http://aclrollingreview.org/cfp\"&gt;http://aclrollingreview.org/cfp&lt;/a&gt; section on short papers. &lt;/p&gt;\n\n&lt;p&gt;Question to anyone who has submitted short papers in the past, do you think your paper was reviewed fairly as a short paper? I know we&amp;#39;ve all had some bad experiences with subletting any kind of paper, but do you think on average the reviewers understood the assignment and evaluated your work based on the criteria for short papers? &lt;/p&gt;\n\n&lt;p&gt;I think it&amp;#39;s true that ICLR used to have a short papers track and removed it. Does anyone know why it was removed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OMWgF1T548Ok3nUEjMCFo9-8k73z4HKKSAzQMV-QsxY.png?auto=webp&amp;s=1924cb4cb3fa02349e3061c53afa15f3fcbd496a", "width": 594, "height": 542}, "resolutions": [{"url": "https://external-preview.redd.it/OMWgF1T548Ok3nUEjMCFo9-8k73z4HKKSAzQMV-QsxY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d33a10c191d681e4f7690c958c9ec49d2769153c", "width": 108, "height": 98}, {"url": "https://external-preview.redd.it/OMWgF1T548Ok3nUEjMCFo9-8k73z4HKKSAzQMV-QsxY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=69d64b2e77056a6a849aaf27b9b04f209be37b1a", "width": 216, "height": 197}, {"url": "https://external-preview.redd.it/OMWgF1T548Ok3nUEjMCFo9-8k73z4HKKSAzQMV-QsxY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=faf3d4cb77c2e45c3d373cdf9c08d53301063214", "width": 320, "height": 291}], "variants": {}, "id": "OMWgF1T548Ok3nUEjMCFo9-8k73z4HKKSAzQMV-QsxY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bb90e510-4e82-11e6-8635-0ee522e2349b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f1f10e", "id": "1r6lgap", "is_robot_indexable": true, "report_reasons": null, "author": "Efficient_Ad_6772", "discussion_type": null, "num_comments": 4, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r6lgap/short_paper_reviews_r/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6lgap/short_paper_reviews_r/", "subreddit_subscribers": 3023273, "created_utc": 1771274869.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Hi everyone,\n\nI've been diving deep into sparse architectures for vision transformers, and I'm incredibly impressed with the potential of SparseFormer to solve the O(n\u00b2) compute bottleneck, especially for commercial applications like data labeling and industrial inspection.\n\nIt feels like this is where the industry is heading for efficiency, and it seems to have more commercial potential than it's currently given credit for, especially with the push towards multimodal models.\n\nIs anyone here working with or researching SparseFormer? Curious to hear thoughts on its commercial viability versus other sparse MoE approaches for vision tasks.", "author_fullname": "t2_vkn8ytjyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] SparseFormer and the future of efficient Al vision models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6mle8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771277441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been diving deep into sparse architectures for vision transformers, and I&amp;#39;m incredibly impressed with the potential of SparseFormer to solve the O(n\u00b2) compute bottleneck, especially for commercial applications like data labeling and industrial inspection.&lt;/p&gt;\n\n&lt;p&gt;It feels like this is where the industry is heading for efficiency, and it seems to have more commercial potential than it&amp;#39;s currently given credit for, especially with the push towards multimodal models.&lt;/p&gt;\n\n&lt;p&gt;Is anyone here working with or researching SparseFormer? Curious to hear thoughts on its commercial viability versus other sparse MoE approaches for vision tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r6mle8", "is_robot_indexable": true, "report_reasons": null, "author": "SR1180", "discussion_type": null, "num_comments": 9, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r6mle8/d_sparseformer_and_the_future_of_efficient_al/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6mle8/d_sparseformer_and_the_future_of_efficient_al/", "subreddit_subscribers": 3023273, "created_utc": 1771277441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I'm a 2nd year PhD student and looking to broaden my collaboration circle and what better than this community.\n\nI primarily work on developing frameworks for fairness (imaging models, LM) (evaluation/mitigation for clinical deployment) but really open for boarder topics. \n\nIf there's a possibility we can connect and work on something exciting (for a publication in conf or a workshop), would be great. If you have hold of a dataset which will be useful we can make it formal with our institutes.\n\nlooking forward to hearing from brilliant minds!", "author_fullname": "t2_c2ajt49e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Collaboration invite - medical Imag!ng, algorithmic fairness or open track [D]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "three", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6ivlw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Research", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771269206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a 2nd year PhD student and looking to broaden my collaboration circle and what better than this community.&lt;/p&gt;\n\n&lt;p&gt;I primarily work on developing frameworks for fairness (imaging models, LM) (evaluation/mitigation for clinical deployment) but really open for boarder topics. &lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;s a possibility we can connect and work on something exciting (for a publication in conf or a workshop), would be great. If you have hold of a dataset which will be useful we can make it formal with our institutes.&lt;/p&gt;\n\n&lt;p&gt;looking forward to hearing from brilliant minds!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bb90e510-4e82-11e6-8635-0ee522e2349b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f1f10e", "id": "1r6ivlw", "is_robot_indexable": true, "report_reasons": null, "author": "ade17_in", "discussion_type": null, "num_comments": 5, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r6ivlw/collaboration_invite_medical_imagng_algorithmic/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6ivlw/collaboration_invite_medical_imagng_algorithmic/", "subreddit_subscribers": 3023273, "created_utc": 1771269206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "*Link:*\u00a0[*https://arxiv.org/abs/2602.14814*](https://arxiv.org/abs/2602.14814)\n\n*Authors:*\u00a0Julien Siems,\u00a0Riccardo Grazzi,\u00a0Kirill Kalinin,\u00a0Hitesh Ballani,\u00a0Babak Rahmani\n\n*Abstract:*\u00a0Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.\n\nhttps://preview.redd.it/9cjies2580kg1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=e5e534d329bbdbf3d705e811c473ada55d503d20\n\n[](https://preview.redd.it/learning-state-tracking-from-code-using-linear-rnns-v0-u9i5y1wf40kg1.png?width=2184&amp;format=png&amp;auto=webp&amp;s=b9e3731c6000b1a906882287bcf05877bcb6a48e)", "author_fullname": "t2_efno949xt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[R] Learning State-Tracking from Code Using Linear RNNs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "three", "downs": 0, "thumbnail_height": 46, "top_awarded_type": null, "hide_score": false, "media_metadata": {"9cjies2580kg1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 35, "x": 108, "u": "https://preview.redd.it/9cjies2580kg1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3574079c84aeb0c3072d5cf6d3f36f39f3c345d7"}, {"y": 71, "x": 216, "u": "https://preview.redd.it/9cjies2580kg1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f001cfe282c852293f5ddeecdf2f3ad00256c9c"}, {"y": 105, "x": 320, "u": "https://preview.redd.it/9cjies2580kg1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c02cfa4b8056d6f09f34d48f667975da8bf4a641"}, {"y": 210, "x": 640, "u": "https://preview.redd.it/9cjies2580kg1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6019693a40f296248f60d2ad75015767a9416e6c"}, {"y": 315, "x": 960, "u": "https://preview.redd.it/9cjies2580kg1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=53c912b99ce1431ded89125fb84410aa9b9ff3cb"}, {"y": 355, "x": 1080, "u": "https://preview.redd.it/9cjies2580kg1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7814d56e78f14dfbc4838fd3610610d3689641cf"}], "s": {"y": 355, "x": 1080, "u": "https://preview.redd.it/9cjies2580kg1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=e5e534d329bbdbf3d705e811c473ada55d503d20"}, "id": "9cjies2580kg1"}}, "name": "t3_1r6zaf5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Research", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/9cjies2580kg1.png?width=140&amp;height=46&amp;auto=webp&amp;s=4496b107a4238124f9e9b01f22a5baabda2f1e61", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771312054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Link:&lt;/em&gt;\u00a0&lt;a href=\"https://arxiv.org/abs/2602.14814\"&gt;&lt;em&gt;https://arxiv.org/abs/2602.14814&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Authors:&lt;/em&gt;\u00a0Julien Siems,\u00a0Riccardo Grazzi,\u00a0Kirill Kalinin,\u00a0Hitesh Ballani,\u00a0Babak Rahmani&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Abstract:&lt;/em&gt;\u00a0Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9cjies2580kg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5e534d329bbdbf3d705e811c473ada55d503d20\"&gt;https://preview.redd.it/9cjies2580kg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5e534d329bbdbf3d705e811c473ada55d503d20&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/learning-state-tracking-from-code-using-linear-rnns-v0-u9i5y1wf40kg1.png?width=2184&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9e3731c6000b1a906882287bcf05877bcb6a48e\"&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bb90e510-4e82-11e6-8635-0ee522e2349b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f1f10e", "id": "1r6zaf5", "is_robot_indexable": true, "report_reasons": null, "author": "Yossarian_1234", "discussion_type": null, "num_comments": 0, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r6zaf5/r_learning_statetracking_from_code_using_linear/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6zaf5/r_learning_statetracking_from_code_using_linear/", "subreddit_subscribers": 3023273, "created_utc": 1771312054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I\u2019ve been thinking about an emerging structural issue in generative AI.\n\nModel quality is improving rapidly.\n\nCreation cost is decreasing.\n\nInference is becoming cheaper.\n\n\n\nBut discovery mechanisms haven\u2019t evolved at the same pace.\n\n\n\nAs generative systems scale, the amount of produced content increases superlinearly. Ranking, filtering and relevance models often remain engagement-driven rather than quality-driven.\n\n\n\nFrom a machine learning perspective, I\u2019m curious:\n\n\n\nDo we see discovery and relevance modeling becoming the next major bottleneck in generative ecosystems?\n\n\n\nSpecifically:\n\n\u2013 Are current ranking systems fundamentally misaligned with user value?\n\n\u2013 Is engagement still the right optimization objective?\n\n\u2013 Could smaller, curated relevance models outperform large engagement-optimized feeds?\n\n\n\nWould appreciate perspectives from people working on recommender systems or ranking models.", "author_fullname": "t2_9ofbd2wo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Is content discovery becoming a bottleneck in generative AI ecosystems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6nudz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771280291.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been thinking about an emerging structural issue in generative AI.&lt;/p&gt;\n\n&lt;p&gt;Model quality is improving rapidly.&lt;/p&gt;\n\n&lt;p&gt;Creation cost is decreasing.&lt;/p&gt;\n\n&lt;p&gt;Inference is becoming cheaper.&lt;/p&gt;\n\n&lt;p&gt;But discovery mechanisms haven\u2019t evolved at the same pace.&lt;/p&gt;\n\n&lt;p&gt;As generative systems scale, the amount of produced content increases superlinearly. Ranking, filtering and relevance models often remain engagement-driven rather than quality-driven.&lt;/p&gt;\n\n&lt;p&gt;From a machine learning perspective, I\u2019m curious:&lt;/p&gt;\n\n&lt;p&gt;Do we see discovery and relevance modeling becoming the next major bottleneck in generative ecosystems?&lt;/p&gt;\n\n&lt;p&gt;Specifically:&lt;/p&gt;\n\n&lt;p&gt;\u2013 Are current ranking systems fundamentally misaligned with user value?&lt;/p&gt;\n\n&lt;p&gt;\u2013 Is engagement still the right optimization objective?&lt;/p&gt;\n\n&lt;p&gt;\u2013 Could smaller, curated relevance models outperform large engagement-optimized feeds?&lt;/p&gt;\n\n&lt;p&gt;Would appreciate perspectives from people working on recommender systems or ranking models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r6nudz", "is_robot_indexable": true, "report_reasons": null, "author": "Opposite-Alfalfa-700", "discussion_type": null, "num_comments": 3, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r6nudz/d_is_content_discovery_becoming_a_bottleneck_in/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6nudz/d_is_content_discovery_becoming_a_bottleneck_in/", "subreddit_subscribers": 3023273, "created_utc": 1771280291.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "This [paper](https://arxiv.org/pdf/2506.06454) applies takens theorem combined with Empirical Dynamical Modeling to Time Series Forecasting.", "author_fullname": "t2_6k7647ey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[R] LETS Forecast: Learning Embedology for Time Series Forecasting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "three", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6g4n7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Research", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771263371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This &lt;a href=\"https://arxiv.org/pdf/2506.06454\"&gt;paper&lt;/a&gt; applies takens theorem combined with Empirical Dynamical Modeling to Time Series Forecasting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bb90e510-4e82-11e6-8635-0ee522e2349b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f1f10e", "id": "1r6g4n7", "is_robot_indexable": true, "report_reasons": null, "author": "Whatever_635", "discussion_type": null, "num_comments": 0, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r6g4n7/r_lets_forecast_learning_embedology_for_time/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6g4n7/r_lets_forecast_learning_embedology_for_time/", "subreddit_subscribers": 3023273, "created_utc": 1771263371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I\u2019ve been building a project called Shard, a distributed peer-to-peer AI inference network that uses WebGPU in the browser for lightweight compute, while stronger verifier nodes finalize and validate outputs.\n\nThe idea is to experiment with shared inference instead of centralized cloud compute.\n\nRight now it includes:\n\n\t\u2022\tBrowser \u201cScout\u201d nodes contributing WebGPU compute\n\n\t\u2022\tA libp2p mesh network for node communication\n\n\t\u2022\tVerifier nodes running stronger local models\n\n\t\u2022\tA Rust daemon + Python API + web UI\n\n\t\u2022\tGraceful fallback if WebGPU isn\u2019t available\n\nIt\u2019s early stage and definitely not production-ready yet. Security hardening, incentive design, and better UX are still on the roadmap.\n\nI\u2019m exploring whether distributed inference can meaningfully reduce centralized GPU dependence or at least open up new architectural patterns for AI systems.\n\nWould love technical feedback, architecture critiques, or ideas on where this could realistically go.\n\nRepo:\n\nhttps://github.com/TrentPierce/Shard", "author_fullname": "t2_hyznu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P] I built a distributed P2P AI inference network that runs partly in the browser (WebGPU) \u2014 looking for feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6t0wk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "cd34ef9a-6abd-11ea-a7ea-0ec6041e93a9", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Project", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771293280.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been building a project called Shard, a distributed peer-to-peer AI inference network that uses WebGPU in the browser for lightweight compute, while stronger verifier nodes finalize and validate outputs.&lt;/p&gt;\n\n&lt;p&gt;The idea is to experiment with shared inference instead of centralized cloud compute.&lt;/p&gt;\n\n&lt;p&gt;Right now it includes:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;\u2022 Browser \u201cScout\u201d nodes contributing WebGPU compute\n\n\u2022 A libp2p mesh network for node communication\n\n\u2022 Verifier nodes running stronger local models\n\n\u2022 A Rust daemon + Python API + web UI\n\n\u2022 Graceful fallback if WebGPU isn\u2019t available\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It\u2019s early stage and definitely not production-ready yet. Security hardening, incentive design, and better UX are still on the roadmap.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m exploring whether distributed inference can meaningfully reduce centralized GPU dependence or at least open up new architectural patterns for AI systems.&lt;/p&gt;\n\n&lt;p&gt;Would love technical feedback, architecture critiques, or ideas on where this could realistically go.&lt;/p&gt;\n\n&lt;p&gt;Repo:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/TrentPierce/Shard\"&gt;https://github.com/TrentPierce/Shard&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IGB76sN4rtYsUN9ocCLJv0OIiWCGJfeV-c2v3wNGwgA.png?auto=webp&amp;s=35241874c72f9efac1d0a70585ab922318e612e4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/IGB76sN4rtYsUN9ocCLJv0OIiWCGJfeV-c2v3wNGwgA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ee3fe666417a91b28be8b8e3787163d23a30cd35", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/IGB76sN4rtYsUN9ocCLJv0OIiWCGJfeV-c2v3wNGwgA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a92e65c0a3a3fa8d8c3bf9476b3a5fb8cc94df72", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/IGB76sN4rtYsUN9ocCLJv0OIiWCGJfeV-c2v3wNGwgA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48820afbf52db2b6c8aa843f38c0f40ab3685c40", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/IGB76sN4rtYsUN9ocCLJv0OIiWCGJfeV-c2v3wNGwgA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bff0337769de3cc5a15fe6374bd74194e251e169", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/IGB76sN4rtYsUN9ocCLJv0OIiWCGJfeV-c2v3wNGwgA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4abb228d16ee3128c4d64b19204154ff0a55e907", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/IGB76sN4rtYsUN9ocCLJv0OIiWCGJfeV-c2v3wNGwgA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5fc55fafb25dbae1af89311d3907e0602a73d0be", "width": 1080, "height": 540}], "variants": {}, "id": "IGB76sN4rtYsUN9ocCLJv0OIiWCGJfeV-c2v3wNGwgA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "c6dea51c-19d3-11f0-81a2-deb9d8e21ccb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "ML Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7d659a", "id": "1r6t0wk", "is_robot_indexable": true, "report_reasons": null, "author": "Billy_Bowlegs", "discussion_type": null, "num_comments": 0, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/MachineLearning/comments/1r6t0wk/p_i_built_a_distributed_p2p_ai_inference_network/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6t0wk/p_i_built_a_distributed_p2p_ai_inference_network/", "subreddit_subscribers": 3023273, "created_utc": 1771293280.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "[Paper reference](https://arxiv.org/abs/2512.22045)  \n  \nI\u2019m curious to hear other perspectives. It increasingly feels like memory, not raw capability, is what still keeps AI below human intelligence. If memory, bandwidth, and energy efficiency keep improving, intelligence starts to look more like something being engineered rather than something strictly bounded by today\u2019s scaling laws and optimization limits. Maybe progress doesn\u2019t require endlessly scaling (adding compute, data etc.) models, but adding the right capabilities like persistent memory, better retrieval, and higher bandwidth between components.\n\nAnd if biological intelligence relies on continuous plasticity, memory consolidation, and energy-efficient adaptation rather than fixed training phases, i think scaling-law-driven models are just a transitional engineering strategy rather than the long-term path to machine intelligence.   \n  \nThe core idea is shifting away from static, one-shot training toward systems that keep updating over time without forgetting. In some sense, an evolved form of data augmentation.\n\nThe output is followed by less dependence on periodic giant retrainings, more adaptive systems embedded in real environments, and potentially a step toward agents that evolve with context rather than reset with every version forward.\n\nWhat do you all think?", "author_fullname": "t2_4i4vovil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[R] We spent a decade scaling models. Now, by just shifting towards memory and continual learning, we can get to a human like AI or \"A-GEE-I\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "three", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6zq50", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.35, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Research", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771313629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2512.22045\"&gt;Paper reference&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I\u2019m curious to hear other perspectives. It increasingly feels like memory, not raw capability, is what still keeps AI below human intelligence. If memory, bandwidth, and energy efficiency keep improving, intelligence starts to look more like something being engineered rather than something strictly bounded by today\u2019s scaling laws and optimization limits. Maybe progress doesn\u2019t require endlessly scaling (adding compute, data etc.) models, but adding the right capabilities like persistent memory, better retrieval, and higher bandwidth between components.&lt;/p&gt;\n\n&lt;p&gt;And if biological intelligence relies on continuous plasticity, memory consolidation, and energy-efficient adaptation rather than fixed training phases, i think scaling-law-driven models are just a transitional engineering strategy rather than the long-term path to machine intelligence.   &lt;/p&gt;\n\n&lt;p&gt;The core idea is shifting away from static, one-shot training toward systems that keep updating over time without forgetting. In some sense, an evolved form of data augmentation.&lt;/p&gt;\n\n&lt;p&gt;The output is followed by less dependence on periodic giant retrainings, more adaptive systems embedded in real environments, and potentially a step toward agents that evolve with context rather than reset with every version forward.&lt;/p&gt;\n\n&lt;p&gt;What do you all think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "bb90e510-4e82-11e6-8635-0ee522e2349b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f1f10e", "id": "1r6zq50", "is_robot_indexable": true, "report_reasons": null, "author": "ocean_protocol", "discussion_type": null, "num_comments": 8, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r6zq50/r_we_spent_a_decade_scaling_models_now_by_just/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r6zq50/r_we_spent_a_decade_scaling_models_now_by_just/", "subreddit_subscribers": 3023273, "created_utc": 1771313629.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}
