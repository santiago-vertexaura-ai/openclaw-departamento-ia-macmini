{"kind": "Listing", "data": {"after": "t3_1r3yahe", "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "", "author_fullname": "t2_58qturpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 47, "top_awarded_type": null, "hide_score": false, "name": "t3_1r44fzk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": "", "ups": 497, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 497, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/4rozb901icjg1.jpeg?width=140&amp;height=47&amp;auto=webp&amp;s=f96d650ead621fb43132d530748e8893257a231b", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771024810.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4rozb901icjg1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4rozb901icjg1.jpeg?auto=webp&amp;s=61e34c45ee45db5443a9190597965b6c4124115a", "width": 4096, "height": 1381}, "resolutions": [{"url": "https://preview.redd.it/4rozb901icjg1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81cde0d35e05dd1f927076476231d61cbed3ec13", "width": 108, "height": 36}, {"url": "https://preview.redd.it/4rozb901icjg1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b4864c071f7dfc43c0cbf9ad77069fda24e121b5", "width": 216, "height": 72}, {"url": "https://preview.redd.it/4rozb901icjg1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c08166792e3f096ad528fd0ba359590a58e077c5", "width": 320, "height": 107}, {"url": "https://preview.redd.it/4rozb901icjg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0af0fe460ed577cfa1d0490e0386a39aa78b986f", "width": 640, "height": 215}, {"url": "https://preview.redd.it/4rozb901icjg1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3216e7a283fa882d8904eee6c63e1f84faa238c6", "width": 960, "height": 323}, {"url": "https://preview.redd.it/4rozb901icjg1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dd314930cd048efb3e14e6914f06eb4a51e0c3b5", "width": 1080, "height": 364}], "variants": {}, "id": "4rozb901icjg1"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r44fzk", "is_robot_indexable": true, "report_reasons": null, "author": "abdouhlili", "discussion_type": null, "num_comments": 120, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/", "stickied": false, "url": "https://i.redd.it/4rozb901icjg1.jpeg", "subreddit_subscribers": 624487, "created_utc": 1771024810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "You can monitor quants begin to appear with this search: [https://huggingface.co/models?sort=modified&amp;search=minimax+m2.5](https://huggingface.co/models?sort=modified&amp;search=minimax+m2.5)", "author_fullname": "t2_12aeph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MiniMaxAI/MiniMax-M2.5 \u00b7 Hugging Face", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_1r3pxy7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": "", "ups": 375, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 375, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=140&amp;height=75&amp;auto=webp&amp;s=9df0c9b6d6d5a3484084660de4d2a101b49642f4", "edited": 1770995203.0, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1770991312.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "huggingface.co", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can monitor quants begin to appear with this search: &lt;a href=\"https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5\"&gt;https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://huggingface.co/MiniMaxAI/MiniMax-M2.5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?auto=webp&amp;s=4d447ba7f79c894ed41912590bb4a6a79f5bc2d9", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64033d5fe6d12704ac16ef5f9a5d936f02f22066", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf91d74facd76a3501eec409e9e6a4587a859438", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbe37f59e591378ee409f69ee988008f13f83a2a", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de0bab4be78008336f973196f0ed98e2bbe49764", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=39827b9f194c5cbf3c87173bab764e81beb3ff1f", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=53a3b66b37018fbb799f4b445a9aa2860bca532a", "width": 1080, "height": 583}], "variants": {}, "id": "U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r3pxy7", "is_robot_indexable": true, "report_reasons": null, "author": "rerri", "discussion_type": null, "num_comments": 100, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/", "stickied": false, "url": "https://huggingface.co/MiniMaxAI/MiniMax-M2.5", "subreddit_subscribers": 624487, "created_utc": 1770991312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Hey everyone, made an uncensored version of GPT-OSS 120B.\n\n\n\n  Quick specs: 117B total params, \\~5.1B active (MoE with 128 experts, top-4 routing), 128K context. MXFP4 is the model's native precision - this isn't a quantization, it's how it was trained. No overall quality loss, though you can see CoT behave differently at times.\n\n\n\n  This is the aggressive variant - **observed 0 refusals to any query during testing.**\n\n  **Completely uncensored while keeping full model capabilities intact.**\n\n\n\n  Link: [https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive](https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive)\n\n\n\n  Sampling settings:\n\n  \\- --temp 1.0 --top-k 40\n\n  \\- Disable everything else (top\\_p, min\\_p, repeat penalty, etc.) - some clients turn\n\n  these on by default\n\n  \\- llama.cpp users: --jinja is required for the Harmony response format or the model won't work right\n\n  \\- Example: llama-server -m model.gguf --jinja -fa -b 2048 -ub 2048\n\n\n\n  Single 61GB file. Fits on one H100. For lower VRAM, use --n-cpu-moe N in llama.cpp to offload MoE layers to CPU.\n\n\n\n  Works with llama.cpp, LM Studio, Ollama, etc.\n\n\n\n  If you want smaller models, I also have GPT-OSS 20B, GLM 4.7 Flash and Qwen3 8b VL uncensored:\n\n  \\- [https://huggingface.co/HauhauCS/models/](https://huggingface.co/HauhauCS/models/)\n\n\n\n  As with all my releases, the goal is effectively lossless uncensoring - no dataset changes and no capability loss.\n\n", "author_fullname": "t2_jvdlzz1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT-OSS 120b Uncensored Aggressive Release (MXFP4 GGUF)", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r3zuuf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 286, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 286, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771013733.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, made an uncensored version of GPT-OSS 120B.&lt;/p&gt;\n\n&lt;p&gt;Quick specs: 117B total params, ~5.1B active (MoE with 128 experts, top-4 routing), 128K context. MXFP4 is the model&amp;#39;s native precision - this isn&amp;#39;t a quantization, it&amp;#39;s how it was trained. No overall quality loss, though you can see CoT behave differently at times.&lt;/p&gt;\n\n&lt;p&gt;This is the aggressive variant - &lt;strong&gt;observed 0 refusals to any query during testing.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Completely uncensored while keeping full model capabilities intact.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive\"&gt;https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sampling settings:&lt;/p&gt;\n\n&lt;p&gt;- --temp 1.0 --top-k 40&lt;/p&gt;\n\n&lt;p&gt;- Disable everything else (top_p, min_p, repeat penalty, etc.) - some clients turn&lt;/p&gt;\n\n&lt;p&gt;these on by default&lt;/p&gt;\n\n&lt;p&gt;- llama.cpp users: --jinja is required for the Harmony response format or the model won&amp;#39;t work right&lt;/p&gt;\n\n&lt;p&gt;- Example: llama-server -m model.gguf --jinja -fa -b 2048 -ub 2048&lt;/p&gt;\n\n&lt;p&gt;Single 61GB file. Fits on one H100. For lower VRAM, use --n-cpu-moe N in llama.cpp to offload MoE layers to CPU.&lt;/p&gt;\n\n&lt;p&gt;Works with llama.cpp, LM Studio, Ollama, etc.&lt;/p&gt;\n\n&lt;p&gt;If you want smaller models, I also have GPT-OSS 20B, GLM 4.7 Flash and Qwen3 8b VL uncensored:&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://huggingface.co/HauhauCS/models/\"&gt;https://huggingface.co/HauhauCS/models/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As with all my releases, the goal is effectively lossless uncensoring - no dataset changes and no capability loss.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QZffkKXjStXvdHv-2EojgSarRAJg5k8b1E5-4PAV9-k.png?auto=webp&amp;s=692192b714fbcda2d92ba30046a72b74860f2510", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/QZffkKXjStXvdHv-2EojgSarRAJg5k8b1E5-4PAV9-k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=07a2e07aef74f1516db3300b0da74cd85365f6d1", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/QZffkKXjStXvdHv-2EojgSarRAJg5k8b1E5-4PAV9-k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=28ddcc3bf2404e1e3e959fe30ecb134f166e84f3", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/QZffkKXjStXvdHv-2EojgSarRAJg5k8b1E5-4PAV9-k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=949cd5862c2776e5e386d439388f031f043d89da", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/QZffkKXjStXvdHv-2EojgSarRAJg5k8b1E5-4PAV9-k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f803f352392f163c324ac54d6e0774fc0b2d4b5", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/QZffkKXjStXvdHv-2EojgSarRAJg5k8b1E5-4PAV9-k.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7d5ae9b67c83df3feb504c8b80e566cc04b6039", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/QZffkKXjStXvdHv-2EojgSarRAJg5k8b1E5-4PAV9-k.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ade756f862714d42bdec34751b1eea6dd7cbf0e5", "width": 1080, "height": 583}], "variants": {}, "id": "QZffkKXjStXvdHv-2EojgSarRAJg5k8b1E5-4PAV9-k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r3zuuf", "is_robot_indexable": true, "report_reasons": null, "author": "hauhau901", "discussion_type": null, "num_comments": 26, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/", "subreddit_subscribers": 624487, "created_utc": 1771013733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Hi all, I\u2019m Anton from Nebius.\n\nWe\u2019ve updated the\u00a0**SWE-rebench leaderboard**\u00a0with our\u00a0**January runs**\u00a0on\u00a0**48 fresh GitHub PR tasks**\u00a0(PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.\n\nKey observations:\n\n* **Claude Code (Opus 4.6)**\u00a0leads this snapshot at\u00a0**52.9% resolved rate**\u00a0and also achieves the highest\u00a0**pass@5 (70.8%)**.\n* **Claude Opus 4.6**\u00a0and\u00a0**gpt-5.2-xhigh**\u00a0follow very closely (51.7%), making the top tier extremely tight.\n* **gpt-5.2-medium (51.0%)**\u00a0performs surprisingly close to the frontier configuration.\n* Among open models,\u00a0**Kimi K2 Thinking (43.8%)**,\u00a0**GLM-5 (42.1%)**, and\u00a0**Qwen3-Coder-Next (40.0%)**\u00a0lead the pack.\n* **MiniMax M2.5 (39.6%)**\u00a0continues to show strong performance while remaining one of the cheapest options.\n* Clear gap between Kimi variants:\u00a0**K2 Thinking (43.8%)**\u00a0vs\u00a0**K2.5 (37.9%)**.\n* Newer smaller/flash variants (e.g., GLM-4.7 Flash, gpt-5-mini-medium) trade performance for efficiency, landing in the 25\u201331% range.\n\nLooking forward to your thoughts and feedback.\n\n", "author_fullname": "t2_1saiwbbgv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SWE-rebench Jan 2026: GLM-5, MiniMax M2.5, Qwen3-Coder-Next, Opus 4.6, Codex Performance", "link_flair_richtext": [{"e": "text", "t": "Other"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r3weq3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "", "subreddit_type": "public", "ups": 254, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Other", "can_mod_post": false, "score": 254, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1771006000.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "swe-rebench.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I\u2019m Anton from Nebius.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve updated the\u00a0&lt;strong&gt;SWE-rebench leaderboard&lt;/strong&gt;\u00a0with our\u00a0&lt;strong&gt;January runs&lt;/strong&gt;\u00a0on\u00a0&lt;strong&gt;48 fresh GitHub PR tasks&lt;/strong&gt;\u00a0(PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.&lt;/p&gt;\n\n&lt;p&gt;Key observations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Claude Code (Opus 4.6)&lt;/strong&gt;\u00a0leads this snapshot at\u00a0&lt;strong&gt;52.9% resolved rate&lt;/strong&gt;\u00a0and also achieves the highest\u00a0&lt;strong&gt;pass@5 (70.8%)&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Claude Opus 4.6&lt;/strong&gt;\u00a0and\u00a0&lt;strong&gt;gpt-5.2-xhigh&lt;/strong&gt;\u00a0follow very closely (51.7%), making the top tier extremely tight.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;gpt-5.2-medium (51.0%)&lt;/strong&gt;\u00a0performs surprisingly close to the frontier configuration.&lt;/li&gt;\n&lt;li&gt;Among open models,\u00a0&lt;strong&gt;Kimi K2 Thinking (43.8%)&lt;/strong&gt;,\u00a0&lt;strong&gt;GLM-5 (42.1%)&lt;/strong&gt;, and\u00a0&lt;strong&gt;Qwen3-Coder-Next (40.0%)&lt;/strong&gt;\u00a0lead the pack.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MiniMax M2.5 (39.6%)&lt;/strong&gt;\u00a0continues to show strong performance while remaining one of the cheapest options.&lt;/li&gt;\n&lt;li&gt;Clear gap between Kimi variants:\u00a0&lt;strong&gt;K2 Thinking (43.8%)&lt;/strong&gt;\u00a0vs\u00a0&lt;strong&gt;K2.5 (37.9%)&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Newer smaller/flash variants (e.g., GLM-4.7 Flash, gpt-5-mini-medium) trade performance for efficiency, landing in the 25\u201331% range.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Looking forward to your thoughts and feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://swe-rebench.com/?insight=jan_2026", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#94e044", "id": "1r3weq3", "is_robot_indexable": true, "report_reasons": null, "author": "CuriousPlatypus1881", "discussion_type": null, "num_comments": 71, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r3weq3/swerebench_jan_2026_glm5_minimax_m25/", "stickied": false, "url": "https://swe-rebench.com/?insight=jan_2026", "subreddit_subscribers": 624487, "created_utc": 1771006000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Hi [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)! We\u2019re really excited to be here, thanks for having us.\n\nWe're **MiniMax**, the lab behind:\n\n* [MiniMax-M2](https://x.com/MiniMax__AI/status/1982674798649160175?s=20).5\n* [Hailuo](https://x.com/Hailuo_AI/status/1983382728343994414)\n* [MiniMax Speech](https://x.com/Hailuo_AI/status/1983661667872600296)\n* [MiniMax Music](https://x.com/Hailuo_AI/status/1983964920493568296)\n\nJoining the channel today are:\n\n* u/Top_Cattle_2098 \u2014 Founder of MiniMax\n* u/Wise_Evidence9973 \u2014 Head of LLM Research\n* u/ryan85127704 \u2014 Head of Engineering\n* u/HardToVary \u2014 LLM Researcher\n\nhttps://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;format=pjpg&amp;auto=webp&amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c\n\nP.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.", "author_fullname": "t2_9y2yk2w7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AMA with MiniMax \u2014 Ask Us Anything!", "link_flair_richtext": [{"e": "text", "t": "Question | Help"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5z2li1ntcajg1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 45, "x": 108, "u": "https://preview.redd.it/5z2li1ntcajg1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=67ad467c83e292fdbca249262b9621fffa4c95e4"}, {"y": 91, "x": 216, "u": "https://preview.redd.it/5z2li1ntcajg1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a2cc950f7682ec86e0754293c72bda25d1aab58d"}, {"y": 136, "x": 320, "u": "https://preview.redd.it/5z2li1ntcajg1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=685a5442ba46eeaa9ae1ccee6dd8c8280581a0ce"}, {"y": 272, "x": 640, "u": "https://preview.redd.it/5z2li1ntcajg1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=58af3a8a830778c9fe1c5230289a6f97961dadca"}, {"y": 408, "x": 960, "u": "https://preview.redd.it/5z2li1ntcajg1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5659f3a5ef56e55e52ca10796656c8796b5ced19"}, {"y": 459, "x": 1080, "u": "https://preview.redd.it/5z2li1ntcajg1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f75dff445a494bfbb1a5739f4955fc59d825fd2c"}], "s": {"y": 1500, "x": 3525, "u": "https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;format=pjpg&amp;auto=webp&amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"}, "id": "5z2li1ntcajg1"}}, "name": "t3_1r3t775", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 229, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question | Help", "can_mod_post": false, "score": 229, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;height=59&amp;auto=webp&amp;s=ce3340cd37b7e9408878509be00dd7b871efebde", "edited": 1771014366.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770998874.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/\"&gt;r/LocalLLaMA&lt;/a&gt;! We\u2019re really excited to be here, thanks for having us.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://x.com/MiniMax__AI/status/1982674798649160175?s=20\"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://x.com/Hailuo_AI/status/1983382728343994414\"&gt;Hailuo&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://x.com/Hailuo_AI/status/1983661667872600296\"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://x.com/Hailuo_AI/status/1983964920493568296\"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Joining the channel today are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"/u/Top_Cattle_2098\"&gt;u/Top_Cattle_2098&lt;/a&gt; \u2014 Founder of MiniMax&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"/u/Wise_Evidence9973\"&gt;u/Wise_Evidence9973&lt;/a&gt; \u2014 Head of LLM Research&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"/u/ryan85127704\"&gt;u/ryan85127704&lt;/a&gt; \u2014 Head of Engineering&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"/u/HardToVary\"&gt;u/HardToVary&lt;/a&gt; \u2014 LLM Researcher&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c\"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;P.S. We&amp;#39;ll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#5a74cc", "id": "1r3t775", "is_robot_indexable": true, "report_reasons": null, "author": "HardToVary", "discussion_type": null, "num_comments": 229, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/", "stickied": true, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/", "subreddit_subscribers": 624487, "created_utc": 1770998874.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "  Nvidia developed a new technique called Dynamic Memory Sparsification (DMS) that vastly improves how LLMs manage their KV cache during inference. It accomplishes this by retrofitting existing models so that the attention layers output a **learned keep or evict** signal for each token in the KV cache. \n\n  In addition, they've added a \"delayed eviction\" that marks a token as low-importance, but doesn't delete it immediately. Instead, it remains accessible for a short time and allows the model to extract any useful information into newer tokens before it's discarded.\n\n  These advancements reduce KV memory usage by up to **8x**, allowing the model to think longer, run faster and handle more concurrent requests.\n\n  Definitely recommend reading the full article. Looking forward to seeing this on self hosted hardware.\n\n[VentureBeat Article](https://venturebeat.com/orchestration/nvidias-new-technique-cuts-llm-reasoning-costs-by-8x-without-losing-accuracy)", "author_fullname": "t2_2879gx4iur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nvidia\u2019s new technique cuts LLM reasoning costs by 8x without losing accuracy", "link_flair_richtext": [{"e": "text", "t": "News"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r3t8ro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 188, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 188, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1770998971.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Nvidia developed a new technique called Dynamic Memory Sparsification (DMS) that vastly improves how LLMs manage their KV cache during inference. It accomplishes this by retrofitting existing models so that the attention layers output a &lt;strong&gt;learned keep or evict&lt;/strong&gt; signal for each token in the KV cache. &lt;/p&gt;\n\n&lt;p&gt;In addition, they&amp;#39;ve added a &amp;quot;delayed eviction&amp;quot; that marks a token as low-importance, but doesn&amp;#39;t delete it immediately. Instead, it remains accessible for a short time and allows the model to extract any useful information into newer tokens before it&amp;#39;s discarded.&lt;/p&gt;\n\n&lt;p&gt;These advancements reduce KV memory usage by up to &lt;strong&gt;8x&lt;/strong&gt;, allowing the model to think longer, run faster and handle more concurrent requests.&lt;/p&gt;\n\n&lt;p&gt;Definitely recommend reading the full article. Looking forward to seeing this on self hosted hardware.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://venturebeat.com/orchestration/nvidias-new-technique-cuts-llm-reasoning-costs-by-8x-without-losing-accuracy\"&gt;VentureBeat Article&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#cc3600", "id": "1r3t8ro", "is_robot_indexable": true, "report_reasons": null, "author": "Mission-Street4214", "discussion_type": null, "num_comments": 36, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/", "subreddit_subscribers": 624487, "created_utc": 1770998971.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "[https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters](https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters)\n\nThey were about to present the name of alibaba and Baidu as a potential threat or issue for helping chinese military in the Pentagon, but ultimately took their names off the list\n\nWould love to hear what y'all think about this!", "author_fullname": "t2_10v7r9wnj5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "has it begun?", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1r3yuyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 170, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 170, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=140&amp;height=140&amp;crop=1:1,smart&amp;auto=webp&amp;s=56e73220302b058c409252f3df2052957287ba04", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771011481.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters\"&gt;https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They were about to present the name of alibaba and Baidu as a potential threat or issue for helping chinese military in the Pentagon, but ultimately took their names off the list&lt;/p&gt;\n\n&lt;p&gt;Would love to hear what y&amp;#39;all think about this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ei9lt0u4ebjg1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ei9lt0u4ebjg1.jpeg?auto=webp&amp;s=3db56763cdf3666a2f1cf50fc34180e9c914e434", "width": 1080, "height": 1173}, "resolutions": [{"url": "https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cff71cf0b24d34f2775cc073cca679386b546f7", "width": 108, "height": 117}, {"url": "https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd14402a70ac523dd42bd05e3885435f8803844a", "width": 216, "height": 234}, {"url": "https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed9891da583f7c87c889594c3762e4ec63b673cf", "width": 320, "height": 347}, {"url": "https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=36034757efbb832ba75f43ed04c4dc8c7bb34675", "width": 640, "height": 695}, {"url": "https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=86f6cf38b93f9bea3165bc6ffd25bd3b26209c59", "width": 960, "height": 1042}, {"url": "https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=234ac986e48c8063080efc3ed456e55e5d83d5e6", "width": 1080, "height": 1173}], "variants": {}, "id": "ei9lt0u4ebjg1"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r3yuyd", "is_robot_indexable": true, "report_reasons": null, "author": "Acceptable_Home_", "discussion_type": null, "num_comments": 136, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/", "stickied": false, "url": "https://i.redd.it/ei9lt0u4ebjg1.jpeg", "subreddit_subscribers": 624487, "created_utc": 1771011481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Today, I released a demo showcasing GPT-OSS (20B) running 100% locally in-browser on WebGPU, powered by Transformers.js v4 (preview) and ONNX Runtime Web. Hope you like it!  \n  \nLinks:  \n\\- Demo (+ source code): [https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU](https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU)  \n\\- Optimized ONNX model: [https://huggingface.co/onnx-community/gpt-oss-20b-ONNX](https://huggingface.co/onnx-community/gpt-oss-20b-ONNX)", "author_fullname": "t2_mizchr3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT-OSS (20B) running 100% locally in your browser on WebGPU", "link_flair_richtext": [{"e": "text", "t": "Other"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "name": "t3_1r3uixu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "", "ups": 125, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/ioqb4q8jkajg1/CMAF_1080.mp4?source=fallback", "has_audio": false, "height": 1080, "width": 1896, "scrubber_media_url": "https://v.redd.it/ioqb4q8jkajg1/CMAF_96.mp4", "dash_url": "https://v.redd.it/ioqb4q8jkajg1/DASHPlaylist.mpd?a=1773666111%2CMDk5ZjFjODUxMjNjZDFhOGVkNDEyZTY4MTRmMWE4OGFkNGJmNDZhNGY2ZDhmOGQxMTY5ZTU3ZGJiMjQ3MDkwMA%3D%3D&amp;v=1&amp;f=sd", "duration": 83, "hls_url": "https://v.redd.it/ioqb4q8jkajg1/HLSPlaylist.m3u8?a=1773666111%2CMWI5YTA0OGM1MTc4MzI5YzU3YjM3ZjBjZGZiYTcyYTg4YjhhMGFhNzRiOGNmZjEwMDRjMTFmZTM3YmM1MDI3ZA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Other", "can_mod_post": false, "score": 125, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=140&amp;height=79&amp;format=jpg&amp;auto=webp&amp;s=c0a5fbc8a772c16a2f706dc418b7322fceda48b7", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"e": "text", "t": "\ud83e\udd17"}], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771001813.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today, I released a demo showcasing GPT-OSS (20B) running 100% locally in-browser on WebGPU, powered by Transformers.js v4 (preview) and ONNX Runtime Web. Hope you like it!  &lt;/p&gt;\n\n&lt;p&gt;Links:&lt;br/&gt;\n- Demo (+ source code): &lt;a href=\"https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU\"&gt;https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU&lt;/a&gt;&lt;br/&gt;\n- Optimized ONNX model: &lt;a href=\"https://huggingface.co/onnx-community/gpt-oss-20b-ONNX\"&gt;https://huggingface.co/onnx-community/gpt-oss-20b-ONNX&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/ioqb4q8jkajg1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?format=pjpg&amp;auto=webp&amp;s=6f8f9119956750dc858126ce8bbc6f749ef7ce48", "width": 3114, "height": 1774}, "resolutions": [{"url": "https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dac41acc591c3eadd99caa4d776f5772df4f3bb2", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0910581bd9541a3a7d808417e32a854bdee77d93", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ee7f2e3676a2b045944e0e61435c0b3ee16c84dc", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b3264ddcc2bf3bcaeb0ca6deed00af6790aea8ee", "width": 640, "height": 364}, {"url": "https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=938dcea7efb7cea43d2462dce6ff60f28c983eaf", "width": 960, "height": 546}, {"url": "https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6c3b0e0538729df077f26e3c2e4c7c5c03a93be7", "width": 1080, "height": 615}], "variants": {}, "id": "azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "\ud83e\udd17", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#94e044", "id": "1r3uixu", "is_robot_indexable": true, "report_reasons": null, "author": "xenovatech", "discussion_type": null, "num_comments": 23, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/", "stickied": false, "url": "https://v.redd.it/ioqb4q8jkajg1", "subreddit_subscribers": 624487, "created_utc": 1771001813.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/ioqb4q8jkajg1/CMAF_1080.mp4?source=fallback", "has_audio": false, "height": 1080, "width": 1896, "scrubber_media_url": "https://v.redd.it/ioqb4q8jkajg1/CMAF_96.mp4", "dash_url": "https://v.redd.it/ioqb4q8jkajg1/DASHPlaylist.mpd?a=1773666111%2CMDk5ZjFjODUxMjNjZDFhOGVkNDEyZTY4MTRmMWE4OGFkNGJmNDZhNGY2ZDhmOGQxMTY5ZTU3ZGJiMjQ3MDkwMA%3D%3D&amp;v=1&amp;f=sd", "duration": 83, "hls_url": "https://v.redd.it/ioqb4q8jkajg1/HLSPlaylist.m3u8?a=1773666111%2CMWI5YTA0OGM1MTc4MzI5YzU3YjM3ZjBjZGZiYTcyYTg4YjhhMGFhNzRiOGNmZjEwMDRjMTFmZTM3YmM1MDI3ZA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "**Background**: I am a developer with over two decades of experience. I use LLMs heavily day to day from all of the major providers. Since the first Llama models came out I've been toying with local models, benchmarking them on real-world heavy use cases.\n\n**Long story short:** GLM-5 is the first model I've been able to run locally that's actually impressed me. In 3 'shots' I was able to make a retro styled flappy clone AND deploy it to AWS with a cost assessment if it went viral.\n\n**My prompt**: Please generate a GPU accelerated clone of the game \u2018Flappy Bird\u2019 where using the spacebar causes the bird to \u2018flap\u2019, give it a 'retro inspired' design.\n\n**My Setup**:   \n\\- Dual RTX 6000 PRO MaxQ GPUs  \n\\- 128gb of DDR5   \n\\- AMD Ryzen Threadripper PRO 7975WX  \n\\- GLM-5-744B served over vLLM with 128k context at IQ2\\_M\n\n**Caveats**: Even with my decently powerful hardware, the token output was painfully slow at 16.5t/s. IMO, completely worth the wait though. The same test with Qwen3-Next-80b, GPT-OSS-120b and a few other leaders was unimpressive.\n\n[https://flappy.tjameswilliams.com/](https://flappy.tjameswilliams.com/) ", "author_fullname": "t2_8ogipvb8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GLM-5 Is a local GOAT", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1r41013", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 110, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 800, "fallback_url": "https://v.redd.it/7l7iri95rbjg1/CMAF_360.mp4?source=fallback", "has_audio": false, "height": 514, "width": 360, "scrubber_media_url": "https://v.redd.it/7l7iri95rbjg1/CMAF_96.mp4", "dash_url": "https://v.redd.it/7l7iri95rbjg1/DASHPlaylist.mpd?a=1773666111%2COTdmNWRmMjEwMGQxZTJhODdhYWMwMTkzN2MzYTU3NjU1OGYzMzlhYmFiYjcwMzJiYzg0ODdiY2NlNjk1NGU1OA%3D%3D&amp;v=1&amp;f=sd", "duration": 13, "hls_url": "https://v.redd.it/7l7iri95rbjg1/HLSPlaylist.m3u8?a=1773666111%2COTIxOGI0OTU5NGY1ZTJjOTE3MjIwYjY1MDEzODIyMGIwZjNhMjRlMWY5ZGJmMTJkYTQ1NDBjZjg4N2YzMjI4MQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 110, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/MTlvZ25qOTVyYmpnMet_8L-GzQ_poWye6LYGoFL5kcPokh15ZfJ1OHhOgrf9.png?width=140&amp;height=140&amp;crop=1:1,smart&amp;format=jpg&amp;auto=webp&amp;s=b6a945be9815c8caf9aeb9b06bddcca16810e3ad", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771016408.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;: I am a developer with over two decades of experience. I use LLMs heavily day to day from all of the major providers. Since the first Llama models came out I&amp;#39;ve been toying with local models, benchmarking them on real-world heavy use cases.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Long story short:&lt;/strong&gt; GLM-5 is the first model I&amp;#39;ve been able to run locally that&amp;#39;s actually impressed me. In 3 &amp;#39;shots&amp;#39; I was able to make a retro styled flappy clone AND deploy it to AWS with a cost assessment if it went viral.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My prompt&lt;/strong&gt;: Please generate a GPU accelerated clone of the game \u2018Flappy Bird\u2019 where using the spacebar causes the bird to \u2018flap\u2019, give it a &amp;#39;retro inspired&amp;#39; design.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Setup&lt;/strong&gt;:&lt;br/&gt;\n- Dual RTX 6000 PRO MaxQ GPUs&lt;br/&gt;\n- 128gb of DDR5&lt;br/&gt;\n- AMD Ryzen Threadripper PRO 7975WX&lt;br/&gt;\n- GLM-5-744B served over vLLM with 128k context at IQ2_M&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;: Even with my decently powerful hardware, the token output was painfully slow at 16.5t/s. IMO, completely worth the wait though. The same test with Qwen3-Next-80b, GPT-OSS-120b and a few other leaders was unimpressive.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://flappy.tjameswilliams.com/\"&gt;https://flappy.tjameswilliams.com/&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/7l7iri95rbjg1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MTlvZ25qOTVyYmpnMet_8L-GzQ_poWye6LYGoFL5kcPokh15ZfJ1OHhOgrf9.png?format=pjpg&amp;auto=webp&amp;s=f37c9a7f2dce423938d9cdb652c08cfd7bc2d747", "width": 442, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/MTlvZ25qOTVyYmpnMet_8L-GzQ_poWye6LYGoFL5kcPokh15ZfJ1OHhOgrf9.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7654ae812f97999f90c9810a927b757034300ed9", "width": 108, "height": 153}, {"url": "https://external-preview.redd.it/MTlvZ25qOTVyYmpnMet_8L-GzQ_poWye6LYGoFL5kcPokh15ZfJ1OHhOgrf9.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=684bf89bbc6fe1b8afdf977181ec53887ce3e9c8", "width": 216, "height": 307}, {"url": "https://external-preview.redd.it/MTlvZ25qOTVyYmpnMet_8L-GzQ_poWye6LYGoFL5kcPokh15ZfJ1OHhOgrf9.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4dc77fc9856b77d9873846245f984c7fda171d9a", "width": 320, "height": 456}], "variants": {}, "id": "MTlvZ25qOTVyYmpnMet_8L-GzQ_poWye6LYGoFL5kcPokh15ZfJ1OHhOgrf9"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r41013", "is_robot_indexable": true, "report_reasons": null, "author": "FineClassroom2085", "discussion_type": null, "num_comments": 66, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/", "stickied": false, "url": "https://v.redd.it/7l7iri95rbjg1", "subreddit_subscribers": 624487, "created_utc": 1771016408.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 800, "fallback_url": "https://v.redd.it/7l7iri95rbjg1/CMAF_360.mp4?source=fallback", "has_audio": false, "height": 514, "width": 360, "scrubber_media_url": "https://v.redd.it/7l7iri95rbjg1/CMAF_96.mp4", "dash_url": "https://v.redd.it/7l7iri95rbjg1/DASHPlaylist.mpd?a=1773666111%2COTdmNWRmMjEwMGQxZTJhODdhYWMwMTkzN2MzYTU3NjU1OGYzMzlhYmFiYjcwMzJiYzg0ODdiY2NlNjk1NGU1OA%3D%3D&amp;v=1&amp;f=sd", "duration": 13, "hls_url": "https://v.redd.it/7l7iri95rbjg1/HLSPlaylist.m3u8?a=1773666111%2COTIxOGI0OTU5NGY1ZTJjOTE3MjIwYjY1MDEzODIyMGIwZjNhMjRlMWY5ZGJmMTJkYTQ1NDBjZjg4N2YzMjI4MQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "LLaDA2.1 builds on LLaDA2.0 by introducing Token-to-Token (T2T) editing alongside the standard Mask-to-Token decoding. Instead of locking in tokens once generated, the model can now retroactively correct errors during inference \u2014 enabling much more aggressive parallel drafting.\n\nTwo decoding modes:\n\n* S Mode (Speedy): Aggressively low masking threshold + T2T correction. On coding tasks, LLaDA2.1-flash (100B) hits 892 TPS on HumanEval+, 801 TPS on BigCodeBench, 663 TPS on LiveCodeBench.\n* Q Mode (Quality): Conservative thresholds for best benchmark scores \u2014 surpasses LLaDA2.0 on both Mini and Flash.\n\nOther highlights:\n\n* First large-scale RL framework for diffusion LLMs (EBPO), improving reasoning and instruction following\n* Multi-Block Editing (MBE): revisit and revise previously generated blocks, consistent gains on reasoning/coding at modest speed cost\n* LLaDA2.1-mini (16B) peaks at \\~1587 TPS on HumanEval+\n\nHugging Face: [https://huggingface.co/collections/inclusionAI/llada21](https://huggingface.co/collections/inclusionAI/llada21)\n\nGitHub: [https://github.com/inclusionAI/LLaDA2.X](https://github.com/inclusionAI/LLaDA2.X)\n\nTech Report: [https://huggingface.co/papers/2602.08676](https://huggingface.co/papers/2602.08676)", "author_fullname": "t2_1pvd5635i6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLaDA2.1 (100B/16B) released \u2014 now with token editing for massive speed gains", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r3yahe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 87, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 87, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771010199.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;LLaDA2.1 builds on LLaDA2.0 by introducing Token-to-Token (T2T) editing alongside the standard Mask-to-Token decoding. Instead of locking in tokens once generated, the model can now retroactively correct errors during inference \u2014 enabling much more aggressive parallel drafting.&lt;/p&gt;\n\n&lt;p&gt;Two decoding modes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;S Mode (Speedy): Aggressively low masking threshold + T2T correction. On coding tasks, LLaDA2.1-flash (100B) hits 892 TPS on HumanEval+, 801 TPS on BigCodeBench, 663 TPS on LiveCodeBench.&lt;/li&gt;\n&lt;li&gt;Q Mode (Quality): Conservative thresholds for best benchmark scores \u2014 surpasses LLaDA2.0 on both Mini and Flash.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Other highlights:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;First large-scale RL framework for diffusion LLMs (EBPO), improving reasoning and instruction following&lt;/li&gt;\n&lt;li&gt;Multi-Block Editing (MBE): revisit and revise previously generated blocks, consistent gains on reasoning/coding at modest speed cost&lt;/li&gt;\n&lt;li&gt;LLaDA2.1-mini (16B) peaks at ~1587 TPS on HumanEval+&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Hugging Face: &lt;a href=\"https://huggingface.co/collections/inclusionAI/llada21\"&gt;https://huggingface.co/collections/inclusionAI/llada21&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/inclusionAI/LLaDA2.X\"&gt;https://github.com/inclusionAI/LLaDA2.X&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tech Report: &lt;a href=\"https://huggingface.co/papers/2602.08676\"&gt;https://huggingface.co/papers/2602.08676&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/82mCsHP2SIMtQkvDhPErINi3uSkeLyzxZS6d7hE1-9Y.png?auto=webp&amp;s=2ada4559605b14d05f887cf0ddfba7e9fbf19860", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/82mCsHP2SIMtQkvDhPErINi3uSkeLyzxZS6d7hE1-9Y.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1dea6c4f9a7ede7945fd5f5cb86fb641425e4d28", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/82mCsHP2SIMtQkvDhPErINi3uSkeLyzxZS6d7hE1-9Y.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=291643ccc9a6bd782065acd23ee48d8a13b4e133", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/82mCsHP2SIMtQkvDhPErINi3uSkeLyzxZS6d7hE1-9Y.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eefa3fc50f101a5813eb9808f17eb4dfc1c2d3b5", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/82mCsHP2SIMtQkvDhPErINi3uSkeLyzxZS6d7hE1-9Y.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc25cd076ed48183f581abd7b4d0c1719d77fb65", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/82mCsHP2SIMtQkvDhPErINi3uSkeLyzxZS6d7hE1-9Y.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8e5c687facec3c671c61bfd81d7ff82234c7959", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/82mCsHP2SIMtQkvDhPErINi3uSkeLyzxZS6d7hE1-9Y.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb2281d08d40b3a8f2467c40cc4b9326d741c7ae", "width": 1080, "height": 583}], "variants": {}, "id": "82mCsHP2SIMtQkvDhPErINi3uSkeLyzxZS6d7hE1-9Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r3yahe", "is_robot_indexable": true, "report_reasons": null, "author": "FeelingWatercress871", "discussion_type": null, "num_comments": 2, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/", "subreddit_subscribers": 624487, "created_utc": 1771010199.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}
