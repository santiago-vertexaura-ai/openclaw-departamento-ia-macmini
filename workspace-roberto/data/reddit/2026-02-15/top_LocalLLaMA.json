{"kind": "Listing", "data": {"after": "t3_1r4yg6p", "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Hey everyone, we just open-sourced KaniTTS2 - a text-to-speech model designed for real-time conversational use cases.\n\n\\## Models:\n\nMultilingual (English, Spanish), and English-specific with local accents. Language support is actively expanding - more languages coming in future updates\n\n\\## Specs\n\n\\* 400M parameters (BF16)\n\n\\* 22kHz sample rate\n\n\\* Voice Cloning\n\n\\* \\~0.2 RTF on RTX 5090\n\n\\* 3GB GPU VRAM\n\n\\* Pretrained on \\~10k hours of speech\n\n\\* Training took 6 hours on 8x H100s\n\n\\## Full pretrain code - train your own TTS from scratch\n\nThis is the part we\u2019re most excited to share. We\u2019re releasing the complete pretraining framework so anyone can train a TTS model for their own language, accent, or domain.\n\n\\## Links\n\n\\* Pretrained model: https://huggingface.co/nineninesix/kani-tts-2-pt\n\n\\* English model: https://huggingface.co/nineninesix/kani-tts-2-en\n\n\\* Pretrain code: https://github.com/nineninesix-ai/kani-tts-2-pretrain\n\n\\* HF Spaces: https://huggingface.co/spaces/nineninesix/kani-tts-2-pt, https://huggingface.co/spaces/nineninesix/kanitts-2-en\n\n\\* License: Apache 2.0\n\nHappy to answer any questions. Would love to see what people build with this, especially for underrepresented languages.", "author_fullname": "t2_z307x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "KaniTTS2 \u2014 open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included.", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1r4sivv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "", "ups": 430, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/swybh9pdaijg1/CMAF_720.mp4?source=fallback", "has_audio": true, "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/swybh9pdaijg1/CMAF_96.mp4", "dash_url": "https://v.redd.it/swybh9pdaijg1/DASHPlaylist.mpd?a=1773752447%2CYTIzM2Q4OGE1MGQwZTc5ODZhYTJhMTZjOTAyZjQ2YzFjNTY5ZjgzOTBkNzZmZjA0MWIwOTY1NWRlZjhjN2FhMg%3D%3D&amp;v=1&amp;f=sd", "duration": 81, "hls_url": "https://v.redd.it/swybh9pdaijg1/HLSPlaylist.m3u8?a=1773752447%2CMjc5MjY5YTc2MWVlZGYwMDNhY2Y4M2U3OGYwYmFiMDBlZmE0ZTZlZTI2NDY0MGVmMGRiZTJkZmRjMGQ0YzRkMw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 430, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=140&amp;height=78&amp;format=jpg&amp;auto=webp&amp;s=c2c3fadfc3229865d74f61e5327565da4e025408", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771094890.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, we just open-sourced KaniTTS2 - a text-to-speech model designed for real-time conversational use cases.&lt;/p&gt;\n\n&lt;p&gt;## Models:&lt;/p&gt;\n\n&lt;p&gt;Multilingual (English, Spanish), and English-specific with local accents. Language support is actively expanding - more languages coming in future updates&lt;/p&gt;\n\n&lt;p&gt;## Specs&lt;/p&gt;\n\n&lt;p&gt;* 400M parameters (BF16)&lt;/p&gt;\n\n&lt;p&gt;* 22kHz sample rate&lt;/p&gt;\n\n&lt;p&gt;* Voice Cloning&lt;/p&gt;\n\n&lt;p&gt;* ~0.2 RTF on RTX 5090&lt;/p&gt;\n\n&lt;p&gt;* 3GB GPU VRAM&lt;/p&gt;\n\n&lt;p&gt;* Pretrained on ~10k hours of speech&lt;/p&gt;\n\n&lt;p&gt;* Training took 6 hours on 8x H100s&lt;/p&gt;\n\n&lt;p&gt;## Full pretrain code - train your own TTS from scratch&lt;/p&gt;\n\n&lt;p&gt;This is the part we\u2019re most excited to share. We\u2019re releasing the complete pretraining framework so anyone can train a TTS model for their own language, accent, or domain.&lt;/p&gt;\n\n&lt;p&gt;## Links&lt;/p&gt;\n\n&lt;p&gt;* Pretrained model: &lt;a href=\"https://huggingface.co/nineninesix/kani-tts-2-pt\"&gt;https://huggingface.co/nineninesix/kani-tts-2-pt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;* English model: &lt;a href=\"https://huggingface.co/nineninesix/kani-tts-2-en\"&gt;https://huggingface.co/nineninesix/kani-tts-2-en&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;* Pretrain code: &lt;a href=\"https://github.com/nineninesix-ai/kani-tts-2-pretrain\"&gt;https://github.com/nineninesix-ai/kani-tts-2-pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;* HF Spaces: &lt;a href=\"https://huggingface.co/spaces/nineninesix/kani-tts-2-pt\"&gt;https://huggingface.co/spaces/nineninesix/kani-tts-2-pt&lt;/a&gt;, &lt;a href=\"https://huggingface.co/spaces/nineninesix/kanitts-2-en\"&gt;https://huggingface.co/spaces/nineninesix/kanitts-2-en&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;* License: Apache 2.0&lt;/p&gt;\n\n&lt;p&gt;Happy to answer any questions. Would love to see what people build with this, especially for underrepresented languages.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/swybh9pdaijg1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?format=pjpg&amp;auto=webp&amp;s=14ec19489585fda22c8e15c6e1d0ed22cb55b888", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ef469d762cb1cc8f5d0830307e532b75155e0e93", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6c592addc1011c7cf84747d265d28b88da429f6e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6bad68aa8d2916327b7b784a0cae090729008d37", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aaaed68716116e09d641caf2e7764a6f9eb969fd", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=90fa92142d9e328a29c2eab02037190a5e32faf7", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=423b45d8cdb981d417aed79db22dbe13110349fd", "width": 1080, "height": 607}], "variants": {}, "id": "ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r4sivv", "is_robot_indexable": true, "report_reasons": null, "author": "ylankgz", "discussion_type": null, "num_comments": 81, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/", "stickied": false, "url": "https://v.redd.it/swybh9pdaijg1", "subreddit_subscribers": 625287, "created_utc": 1771094890.0, "num_crossposts": 1, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/swybh9pdaijg1/CMAF_720.mp4?source=fallback", "has_audio": true, "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/swybh9pdaijg1/CMAF_96.mp4", "dash_url": "https://v.redd.it/swybh9pdaijg1/DASHPlaylist.mpd?a=1773752447%2CYTIzM2Q4OGE1MGQwZTc5ODZhYTJhMTZjOTAyZjQ2YzFjNTY5ZjgzOTBkNzZmZjA0MWIwOTY1NWRlZjhjN2FhMg%3D%3D&amp;v=1&amp;f=sd", "duration": 81, "hls_url": "https://v.redd.it/swybh9pdaijg1/HLSPlaylist.m3u8?a=1773752447%2CMjc5MjY5YTc2MWVlZGYwMDNhY2Y4M2U3OGYwYmFiMDBlZmE0ZTZlZTI2NDY0MGVmMGRiZTJkZmRjMGQ0YzRkMw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Llamas and Gentlemen,\n\n**Heretic** (https://github.com/p-e-w/heretic) is the leading software for removing censorship from language models. In the three months since its initial release, [more than 1,300 models](https://huggingface.co/models?other=heretic) (including quants) made using Heretic have been published by the community. This represents more than a third of all abliterated models ever published, and the vast majority of abliterated models published since Heretic's first release.\n\nToday, I am happy to announce the release of Heretic 1.2, the product of two months of hard work by the Heretic contributors.\n\nThe headline feature is the new LoRA-based abliteration engine implemented by accemlcc. Built on top of PEFT, it supports loading models with 4-bit quantization using bitsandbytes, which can reduce VRAM requirements for processing a model by up to 70%. The abliterated model is still exported in full precision, which is achieved by re-loading the original model in system RAM and applying the optimized LoRA adapter on top of it, yielding a high-quality model despite the low resource requirements. To enable quantized loading, set `quantization` to `bnb_4bit` in the configuration.\n\nspikymoth implemented Magnitude-Preserving Orthogonal Ablation (MPOA) aka Norm-Preserving Biprojected Abliteration aka \"derestriction\", a refined abliteration technique developed by Jim Lai which can improve the quality of the resulting model in many cases. This has been one of the most frequently requested features from the community, and is now finally available. To enable MPOA, set `orthogonalize_direction` to `true` and `row_normalization` to `full` in the configuration.\n\nHeretic's implementation of MPOA uses Optuna to optimize weight parameters. This can result in models that are better than those generated with the original MPOA technique, which employs a different strategy for layer selection. For example, `MuXodious/gpt-oss-20b-RichardErkhov-heresy` dominates `ArliAI/gpt-oss-20b-Derestricted` on the UGI Leaderboard, scoring 39.05 vs 34.22 and beating the derestricted model in every individual test (W/10, NatInt, and Writing).\n\nAfter a long history of hacks being passed around in the community, anrp finally found a clean way to support vision language models in Heretic, and a broad range of VL models can now be processed. Note that only the language model part (the text decoder transformer) is abliterated, not the image encoder.\n\nanrp also implemented fully automatic session progress saving and resumption. This means worrying about crashes during a long optimization run is now a thing of the past, as you can simply restart Heretic and it will offer to continue where it left off. You can also interrupt the run yourself at any time with Ctrl+C, and resume it later.\n\nPlease see the release notes for the full list of improvements and fixes. More exciting stuff is coming in future versions!\n\nCheers :)\n", "author_fullname": "t2_dkgrhaet", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Heretic 1.2 released: 70% lower VRAM usage with quantization, Magnitude-Preserving Orthogonal Ablation (\"derestriction\"), broad VL model support, session resumption, and more", "link_flair_richtext": [{"e": "text", "t": "Resources"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r4n3as", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": "", "subreddit_type": "public", "ups": 353, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resources", "can_mod_post": false, "score": 353, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771082040.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Llamas and Gentlemen,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Heretic&lt;/strong&gt; (&lt;a href=\"https://github.com/p-e-w/heretic\"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;) is the leading software for removing censorship from language models. In the three months since its initial release, &lt;a href=\"https://huggingface.co/models?other=heretic\"&gt;more than 1,300 models&lt;/a&gt; (including quants) made using Heretic have been published by the community. This represents more than a third of all abliterated models ever published, and the vast majority of abliterated models published since Heretic&amp;#39;s first release.&lt;/p&gt;\n\n&lt;p&gt;Today, I am happy to announce the release of Heretic 1.2, the product of two months of hard work by the Heretic contributors.&lt;/p&gt;\n\n&lt;p&gt;The headline feature is the new LoRA-based abliteration engine implemented by accemlcc. Built on top of PEFT, it supports loading models with 4-bit quantization using bitsandbytes, which can reduce VRAM requirements for processing a model by up to 70%. The abliterated model is still exported in full precision, which is achieved by re-loading the original model in system RAM and applying the optimized LoRA adapter on top of it, yielding a high-quality model despite the low resource requirements. To enable quantized loading, set &lt;code&gt;quantization&lt;/code&gt; to &lt;code&gt;bnb_4bit&lt;/code&gt; in the configuration.&lt;/p&gt;\n\n&lt;p&gt;spikymoth implemented Magnitude-Preserving Orthogonal Ablation (MPOA) aka Norm-Preserving Biprojected Abliteration aka &amp;quot;derestriction&amp;quot;, a refined abliteration technique developed by Jim Lai which can improve the quality of the resulting model in many cases. This has been one of the most frequently requested features from the community, and is now finally available. To enable MPOA, set &lt;code&gt;orthogonalize_direction&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;row_normalization&lt;/code&gt; to &lt;code&gt;full&lt;/code&gt; in the configuration.&lt;/p&gt;\n\n&lt;p&gt;Heretic&amp;#39;s implementation of MPOA uses Optuna to optimize weight parameters. This can result in models that are better than those generated with the original MPOA technique, which employs a different strategy for layer selection. For example, &lt;code&gt;MuXodious/gpt-oss-20b-RichardErkhov-heresy&lt;/code&gt; dominates &lt;code&gt;ArliAI/gpt-oss-20b-Derestricted&lt;/code&gt; on the UGI Leaderboard, scoring 39.05 vs 34.22 and beating the derestricted model in every individual test (W/10, NatInt, and Writing).&lt;/p&gt;\n\n&lt;p&gt;After a long history of hacks being passed around in the community, anrp finally found a clean way to support vision language models in Heretic, and a broad range of VL models can now be processed. Note that only the language model part (the text decoder transformer) is abliterated, not the image encoder.&lt;/p&gt;\n\n&lt;p&gt;anrp also implemented fully automatic session progress saving and resumption. This means worrying about crashes during a long optimization run is now a thing of the past, as you can simply restart Heretic and it will offer to continue where it left off. You can also interrupt the run yourself at any time with Ctrl+C, and resume it later.&lt;/p&gt;\n\n&lt;p&gt;Please see the release notes for the full list of improvements and fixes. More exciting stuff is coming in future versions!&lt;/p&gt;\n\n&lt;p&gt;Cheers :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wpxAln_MY3RpgChrZPtRh6soHTqA7dxOkQp0tx4d-4M.png?auto=webp&amp;s=1a2c9870605ed64a9998d8c29c4ec87fe0ab8d52", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/wpxAln_MY3RpgChrZPtRh6soHTqA7dxOkQp0tx4d-4M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5b62b5f1385d4619b81922d88fbb2fe0b6b4151", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/wpxAln_MY3RpgChrZPtRh6soHTqA7dxOkQp0tx4d-4M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6986edcaff75a6f68638222e96baaafe1a327737", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/wpxAln_MY3RpgChrZPtRh6soHTqA7dxOkQp0tx4d-4M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4dcb4945468f96d5ceea49e757dc439ba7122b15", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/wpxAln_MY3RpgChrZPtRh6soHTqA7dxOkQp0tx4d-4M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9aa8d18f2665b8e0ddbb2d333e14f165d225f57f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/wpxAln_MY3RpgChrZPtRh6soHTqA7dxOkQp0tx4d-4M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7c5a0de6f05fbdf71b7681cb9c1554eff5457d5", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/wpxAln_MY3RpgChrZPtRh6soHTqA7dxOkQp0tx4d-4M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0170e7af7e36859d81d6d8397c7fdaa7833219fb", "width": 1080, "height": 540}], "variants": {}, "id": "wpxAln_MY3RpgChrZPtRh6soHTqA7dxOkQp0tx4d-4M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ccac2b", "id": "1r4n3as", "is_robot_indexable": true, "report_reasons": null, "author": "-p-e-w-", "discussion_type": null, "num_comments": 47, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/", "subreddit_subscribers": 625287, "created_utc": 1771082040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "I've spent the past week experimenting with the DGX Spark and I am about to return it. While I had understood the memory bandwidth and performance limitations, I like the CUDA ecosystem and was willing to pay the premium. Unfortunately, my experiences have been quite poor, and I suspect this is actually handheld gaming scraps that NVIDIA rushed to turn into a product to compete with Apple and Strix Halo.\n\nThe biggest issue: DGX Spark is not datacentre Blackwell, it's not even gaming Blackwell, it has its own special snowflake sm121 architecture. A lot of software do not work with it, or [have been patched to run sm80](https://github.com/triton-lang/triton/issues/8335#issuecomment-3417643519) (Ampere, 6 years old!) codepaths which means it doesn't take advantage of blackwell optimisations.\n\nWhen questioned about this on NVIDIA support forum, [an official NVIDIA representative said](https://forums.developer.nvidia.com/t/dgx-spark-sm121-software-support-is-severely-lacking-official-roadmap-needed/357663/9#p-1745639-h-1-when-will-sm121-receive-native-support-instead-of-sm80-fallbacks-10):\n\n&gt; sm80-class kernels can execute on DGX Spark because Tensor Core behavior is very similar, particularly for GEMM/MMAs (closer to the GeForce Ampere-style MMA model). **DGX Spark not has tcgen05 like jetson Thor or GB200, due die space with RT Cores and DLSS algorithm**\n\nExcuse me?? The reason we're getting cut-down tensor cores (not real blackwell) is because of RT Cores and \"DLSS algorithm\"? This is an AI dev kit; why would I need RT Cores, and additionally how does DLSS come into play? This makes me think they tried to turn a gaming handheld GPU (which needs/supports unified memory) into a poor competitor for a market they weren't prepared for.\n\nIn addition, in the same post the rep posted what appears to be LLM hallucinations, mentioning issues have been fixed in version numbers and releases for software libraries that _do not exist_. \n\nJust be careful when buying a DGX Spark. You are not really getting a modern CUDA experience. Yes, everything works fine if you pretend you only have an Ampere, but attempting to use any Blackwell features is an exercise in futility.\n\nAdditionally, for something that is supposed to be ready 'out of the box', many people (including myself and servethehome) reports basic issues like **HDMI display output**. I originally thought my Spark was DOA; nope; it just refuses to work with my 1080p144 viewsonic (which works with all other GPUs; including my NVIDIA ones); and had to switch to my 4K60 monitor. Dear NVIDIA, you should not have basic display output issues...", "author_fullname": "t2_fmblw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: NVIDIA DGX Spark has terrible CUDA &amp; software compatibility; and seems like a handheld gaming chip.", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r569eb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 159, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 159, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1771139571.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771132673.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve spent the past week experimenting with the DGX Spark and I am about to return it. While I had understood the memory bandwidth and performance limitations, I like the CUDA ecosystem and was willing to pay the premium. Unfortunately, my experiences have been quite poor, and I suspect this is actually handheld gaming scraps that NVIDIA rushed to turn into a product to compete with Apple and Strix Halo.&lt;/p&gt;\n\n&lt;p&gt;The biggest issue: DGX Spark is not datacentre Blackwell, it&amp;#39;s not even gaming Blackwell, it has its own special snowflake sm121 architecture. A lot of software do not work with it, or &lt;a href=\"https://github.com/triton-lang/triton/issues/8335#issuecomment-3417643519\"&gt;have been patched to run sm80&lt;/a&gt; (Ampere, 6 years old!) codepaths which means it doesn&amp;#39;t take advantage of blackwell optimisations.&lt;/p&gt;\n\n&lt;p&gt;When questioned about this on NVIDIA support forum, &lt;a href=\"https://forums.developer.nvidia.com/t/dgx-spark-sm121-software-support-is-severely-lacking-official-roadmap-needed/357663/9#p-1745639-h-1-when-will-sm121-receive-native-support-instead-of-sm80-fallbacks-10\"&gt;an official NVIDIA representative said&lt;/a&gt;:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;sm80-class kernels can execute on DGX Spark because Tensor Core behavior is very similar, particularly for GEMM/MMAs (closer to the GeForce Ampere-style MMA model). &lt;strong&gt;DGX Spark not has tcgen05 like jetson Thor or GB200, due die space with RT Cores and DLSS algorithm&lt;/strong&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Excuse me?? The reason we&amp;#39;re getting cut-down tensor cores (not real blackwell) is because of RT Cores and &amp;quot;DLSS algorithm&amp;quot;? This is an AI dev kit; why would I need RT Cores, and additionally how does DLSS come into play? This makes me think they tried to turn a gaming handheld GPU (which needs/supports unified memory) into a poor competitor for a market they weren&amp;#39;t prepared for.&lt;/p&gt;\n\n&lt;p&gt;In addition, in the same post the rep posted what appears to be LLM hallucinations, mentioning issues have been fixed in version numbers and releases for software libraries that &lt;em&gt;do not exist&lt;/em&gt;. &lt;/p&gt;\n\n&lt;p&gt;Just be careful when buying a DGX Spark. You are not really getting a modern CUDA experience. Yes, everything works fine if you pretend you only have an Ampere, but attempting to use any Blackwell features is an exercise in futility.&lt;/p&gt;\n\n&lt;p&gt;Additionally, for something that is supposed to be ready &amp;#39;out of the box&amp;#39;, many people (including myself and servethehome) reports basic issues like &lt;strong&gt;HDMI display output&lt;/strong&gt;. I originally thought my Spark was DOA; nope; it just refuses to work with my 1080p144 viewsonic (which works with all other GPUs; including my NVIDIA ones); and had to switch to my 4K60 monitor. Dear NVIDIA, you should not have basic display output issues...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mXXXa2z40_kP79BYc8Wzy2YHkAj8Erft4oHGQEm-TwI.png?auto=webp&amp;s=373ff8eead798b86c307e37fd9ca2cfd01b22af2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/mXXXa2z40_kP79BYc8Wzy2YHkAj8Erft4oHGQEm-TwI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=012a8645d786e8dbe21726b48514fcb702adc6f3", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mXXXa2z40_kP79BYc8Wzy2YHkAj8Erft4oHGQEm-TwI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f0b295cdcc37e40a1e6b4cdfca8b81835a1aca30", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mXXXa2z40_kP79BYc8Wzy2YHkAj8Erft4oHGQEm-TwI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e3ba6a9daf7f774a7336da97a5e1b23cff877ed", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mXXXa2z40_kP79BYc8Wzy2YHkAj8Erft4oHGQEm-TwI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d8a89508645e48210bf64e5334a232a0cd52808c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/mXXXa2z40_kP79BYc8Wzy2YHkAj8Erft4oHGQEm-TwI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b32f1c99a280c456c491e44f2a0cd96a6aeea649", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/mXXXa2z40_kP79BYc8Wzy2YHkAj8Erft4oHGQEm-TwI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ceee326cc7473fbfd41d8effabdc4a07491cb0e6", "width": 1080, "height": 540}], "variants": {}, "id": "mXXXa2z40_kP79BYc8Wzy2YHkAj8Erft4oHGQEm-TwI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r569eb", "is_robot_indexable": true, "report_reasons": null, "author": "goldcakes", "discussion_type": null, "num_comments": 48, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/", "subreddit_subscribers": 625287, "created_utc": 1771132673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "I did need to buy some ECC DDR4 :(", "author_fullname": "t2_1x6sxvpt6s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is just a meme...", "link_flair_richtext": [{"e": "text", "t": "Funny"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_1r5chzd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "", "ups": 150, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Funny", "can_mod_post": false, "score": 150, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/qfotdf9z9njg1.png?width=140&amp;height=140&amp;crop=1:1,smart&amp;auto=webp&amp;s=44d6348340bab1fba3ed5ae377a1217e8d90b604", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771155342.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did need to buy some ECC DDR4 :(&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/qfotdf9z9njg1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/qfotdf9z9njg1.png?auto=webp&amp;s=91e24ca40ab3df92bf5c03dc260dcb44bc6336b9", "width": 640, "height": 693}, "resolutions": [{"url": "https://preview.redd.it/qfotdf9z9njg1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8f9bfcb3678ff6349d7cc5064c72ecf262aa4d5", "width": 108, "height": 116}, {"url": "https://preview.redd.it/qfotdf9z9njg1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=599431b1714e9b1cb82250add0d60cba07536337", "width": 216, "height": 233}, {"url": "https://preview.redd.it/qfotdf9z9njg1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=00b2475b6e6b3c20459d926b5a077c288eeeb7f5", "width": 320, "height": 346}, {"url": "https://preview.redd.it/qfotdf9z9njg1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cce236e956c8ed28e47cf83ef72bda256c49f6ba", "width": 640, "height": 693}], "variants": {}, "id": "qfotdf9z9njg1"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0dd3bb", "id": "1r5chzd", "is_robot_indexable": true, "report_reasons": null, "author": "HumanDrone8721", "discussion_type": null, "num_comments": 9, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r5chzd/is_just_a_meme/", "stickied": false, "url": "https://i.redd.it/qfotdf9z9njg1.png", "subreddit_subscribers": 625287, "created_utc": 1771155342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Looks like it released just a few hours ago.  Previously, I was getting 80ish tokens, max, on either of my GPUS in any combination.\n\nNow I'm over 110+ in dual and 130+ on my RTX Pro\n\nPR:\nhttps://github.com/ggml-org/llama.cpp/pull/19375\n\nUpdate your llama.cpp.\n\nEdit: This is for CUDA devices.\n\n\nPrevious:\n```\n\u276f llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0\n\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes\n| model                          |       size |     params | backend    | ngl | n_ubatch | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |           pp500 |       2470.78 \u00b1 3.84 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |            tg32 |         87.35 \u00b1 0.48 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |    pp500 @ d500 |      2468.72 \u00b1 23.27 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |     tg32 @ d500 |         85.99 \u00b1 0.53 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |   pp500 @ d1000 |      2451.68 \u00b1 19.96 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |    tg32 @ d1000 |         87.15 \u00b1 0.57 |\n\nbuild: e06088da0 (7972)\n```\n\nNew\n```\n\u276f llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 \n\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes\n| model                          |       size |     params | backend    | ngl | n_ubatch | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |           pp500 |       2770.34 \u00b1 3.40 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |            tg32 |        118.63 \u00b1 1.14 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |    pp500 @ d500 |      2769.27 \u00b1 23.92 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |     tg32 @ d500 |        119.69 \u00b1 1.65 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |   pp500 @ d1000 |      2753.07 \u00b1 21.85 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |    tg32 @ d1000 |        112.34 \u00b1 0.74 |\n\nbuild: 079feab9e (8055)\n```\n\nRTX by itself on new build\n```\n\u276f llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 -dev CUDA1\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes\n| model                          |       size |     params | backend    | ngl | n_ubatch | fa | dev          |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | ------------ | --------------: | -------------------: |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |           pp500 |       3563.60 \u00b1 4.35 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |            tg32 |        132.09 \u00b1 1.07 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |    pp500 @ d500 |      3481.63 \u00b1 33.66 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |     tg32 @ d500 |        119.57 \u00b1 1.43 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |   pp500 @ d1000 |      3534.69 \u00b1 30.89 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |    tg32 @ d1000 |        131.07 \u00b1 7.27 |\n\nbuild: 079feab9e (8055)\n```", "author_fullname": "t2_1w5m9uci56", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Qwen3 Coder Next Speedup with Latest Llama.cpp", "link_flair_richtext": [{"e": "text", "t": "News"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r50ohq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 147, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 147, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1771119015.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771115642.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like it released just a few hours ago.  Previously, I was getting 80ish tokens, max, on either of my GPUS in any combination.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m over 110+ in dual and 130+ on my RTX Pro&lt;/p&gt;\n\n&lt;p&gt;PR:\n&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/19375\"&gt;https://github.com/ggml-org/llama.cpp/pull/19375&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Update your llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;Edit: This is for CUDA devices.&lt;/p&gt;\n\n&lt;p&gt;Previous:\n```\n\u276f llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0&lt;/p&gt;\n\n&lt;p&gt;ggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes\n| model                          |       size |     params | backend    | ngl | n_ubatch | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |           pp500 |       2470.78 \u00b1 3.84 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |            tg32 |         87.35 \u00b1 0.48 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |    pp500 @ d500 |      2468.72 \u00b1 23.27 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |     tg32 @ d500 |         85.99 \u00b1 0.53 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |   pp500 @ d1000 |      2451.68 \u00b1 19.96 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |    tg32 @ d1000 |         87.15 \u00b1 0.57 |&lt;/p&gt;\n\n&lt;p&gt;build: e06088da0 (7972)\n```&lt;/p&gt;\n\n&lt;p&gt;New\n```\n\u276f llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 &lt;/p&gt;\n\n&lt;p&gt;ggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes\n| model                          |       size |     params | backend    | ngl | n_ubatch | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |           pp500 |       2770.34 \u00b1 3.40 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |            tg32 |        118.63 \u00b1 1.14 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |    pp500 @ d500 |      2769.27 \u00b1 23.92 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |     tg32 @ d500 |        119.69 \u00b1 1.65 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |   pp500 @ d1000 |      2753.07 \u00b1 21.85 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 |    tg32 @ d1000 |        112.34 \u00b1 0.74 |&lt;/p&gt;\n\n&lt;p&gt;build: 079feab9e (8055)\n```&lt;/p&gt;\n\n&lt;p&gt;RTX by itself on new build\n```\n\u276f llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 -dev CUDA1\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes\n| model                          |       size |     params | backend    | ngl | n_ubatch | fa | dev          |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | ------------ | --------------: | -------------------: |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |           pp500 |       3563.60 \u00b1 4.35 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |            tg32 |        132.09 \u00b1 1.07 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |    pp500 @ d500 |      3481.63 \u00b1 33.66 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |     tg32 @ d500 |        119.57 \u00b1 1.43 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |   pp500 @ d1000 |      3534.69 \u00b1 30.89 |\n| qwen3next 80B.A3B Q8_0         |  78.98 GiB |    79.67 B | CUDA       |  99 |     2048 |  1 | CUDA1        |    tg32 @ d1000 |        131.07 \u00b1 7.27 |&lt;/p&gt;\n\n&lt;p&gt;build: 079feab9e (8055)\n```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RuCRGvimU3nnaFv1I4895gm2L9v5vAWHqrtkGLijGms.png?auto=webp&amp;s=d3bc619a13e99ba0caf482880aff693a89943f93", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/RuCRGvimU3nnaFv1I4895gm2L9v5vAWHqrtkGLijGms.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=99ca42bbd3709e437428a2186d7acc98b6624314", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/RuCRGvimU3nnaFv1I4895gm2L9v5vAWHqrtkGLijGms.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e0078bae1e84b0730f87cb15832366b1485780e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/RuCRGvimU3nnaFv1I4895gm2L9v5vAWHqrtkGLijGms.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0ae2c4401cb4856510574037a870578217a1f09", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/RuCRGvimU3nnaFv1I4895gm2L9v5vAWHqrtkGLijGms.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e57de9ed84d0f89d739f2e0405f85c808d14ef0", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/RuCRGvimU3nnaFv1I4895gm2L9v5vAWHqrtkGLijGms.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2124a64eac0c1958b1581d57da17b2b5d58c8f84", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/RuCRGvimU3nnaFv1I4895gm2L9v5vAWHqrtkGLijGms.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d65047311acdd0361ce05097553a624e775d60d5", "width": 1080, "height": 540}], "variants": {}, "id": "RuCRGvimU3nnaFv1I4895gm2L9v5vAWHqrtkGLijGms"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#cc3600", "id": "1r50ohq", "is_robot_indexable": true, "report_reasons": null, "author": "StardockEngineer", "discussion_type": null, "num_comments": 34, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/", "subreddit_subscribers": 625287, "created_utc": 1771115642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "I am newer to building high-end hardware but have been researching local LLM infrastructure for about a year.\n\nLast night was the first time I had all six GPUs running three open-source reasoning models concurrently without stability issues.\n\nCurrent setup (high level):\n\nThreadripper PRO platform\n\n256GB ECC RAM\n\n\\~200GB+ aggregate VRAM across 6 GPUs (mix of 24GB + higher VRAM cards)\n\nDual PSU\n\nOpen-air rack\n\nUbuntu 24.04\n\nGen4 + Gen5 NVMe\n\nPrimary use case is running larger reasoning models locally for internal data analysis + workflow automation\n\nCurrently experimenting with multi-model concurrency and different GPU assignment strategies.\n\nI would really appreciate feedback from people running similar multi-GPU rigs:\n\nAt this scale, what typically becomes the first real bottleneck for local LLM inference VRAM, PCIe bandwidth, CPU orchestration, memory bandwidth, something else?\n\nIs mixing GPU types a long-term pain point, or fine as long as models are pinned deliberately?\n\nFor those running multiple reasoning models simultaneously, where did you start seeing diminishing returns?\n\nHow are people handling model scheduling across GPUs \u2014 static pinning vs dynamic routing?\n\nIf you were building today, would you consolidate into fewer high-VRAM GPUs or keep a distributed multi-card setup?\n\nWhat is one mistake people make when building larger local LLM workstations?\n\nStill learning \u2014 would rather hear what I am overlooking than what I got right, but I appreciate any comments questions or feedback!", "author_fullname": "t2_77jf3jq8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "6-GPU local LLM workstation (\u2248200GB+ VRAM) \u2013 looking for scaling / orchestration advice", "link_flair_richtext": [{"e": "text", "t": "Question | Help"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ydibft7h4hjg1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 89, "x": 108, "u": "https://preview.redd.it/ydibft7h4hjg1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aae12983fbbce9b4f576187a534dc2931d6548f7"}, {"y": 178, "x": 216, "u": "https://preview.redd.it/ydibft7h4hjg1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=436bef33020e610164e8bc338710533e9424114d"}, {"y": 264, "x": 320, "u": "https://preview.redd.it/ydibft7h4hjg1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7178ed93a6915c137a29d599b769470f93fae510"}, {"y": 528, "x": 640, "u": "https://preview.redd.it/ydibft7h4hjg1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e1f6e5bcf057423f0f682109563b505d046752f3"}, {"y": 793, "x": 960, "u": "https://preview.redd.it/ydibft7h4hjg1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f9fc7931d968094b6520bb4f7ba07b397ab4d4ac"}, {"y": 892, "x": 1080, "u": "https://preview.redd.it/ydibft7h4hjg1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a73def4b2106749f88659a668118fff2782ace35"}], "s": {"y": 2806, "x": 3396, "u": "https://preview.redd.it/ydibft7h4hjg1.jpg?width=3396&amp;format=pjpg&amp;auto=webp&amp;s=4dacdbcb6e6cba02678027f1084a67c58ad2ba24"}, "id": "ydibft7h4hjg1"}, "blrzr76h4hjg1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 113, "x": 108, "u": "https://preview.redd.it/blrzr76h4hjg1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b84b3950b962ea5b3e7aadf8ae0a351ec37cfd50"}, {"y": 226, "x": 216, "u": "https://preview.redd.it/blrzr76h4hjg1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc3223ef15f4d51c8ab85218518ac6a7249c193b"}, {"y": 335, "x": 320, "u": "https://preview.redd.it/blrzr76h4hjg1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff3d0f5cb32affb65de3cb87d36f1197db75b60b"}, {"y": 670, "x": 640, "u": "https://preview.redd.it/blrzr76h4hjg1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb4f19b8a9c3a41a4771bc85ee55fe26dc9b7304"}, {"y": 1006, "x": 960, "u": "https://preview.redd.it/blrzr76h4hjg1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=11716c1cf5972f6e51aa7d6bfd245a7b7fdb6e7c"}, {"y": 1132, "x": 1080, "u": "https://preview.redd.it/blrzr76h4hjg1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ced7ecb2e8ec45f6a6c3deeef212803710498e4"}], "s": {"y": 3122, "x": 2978, "u": "https://preview.redd.it/blrzr76h4hjg1.jpg?width=2978&amp;format=pjpg&amp;auto=webp&amp;s=fe83e46b1ea71f978afa076c40a17ee442f5c911"}, "id": "blrzr76h4hjg1"}}, "name": "t3_1r4mks7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 135, "domain": "reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "", "media_id": "blrzr76h4hjg1", "id": 864068047}, {"caption": "", "media_id": "ydibft7h4hjg1", "id": 864068048}]}, "link_flair_text": "Question | Help", "can_mod_post": false, "score": 135, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/blrzr76h4hjg1.jpg?width=140&amp;height=140&amp;crop=1:1,smart&amp;auto=webp&amp;s=bfd384e1306c8a061f9957d6b40c7914ce182d4f", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771080786.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am newer to building high-end hardware but have been researching local LLM infrastructure for about a year.&lt;/p&gt;\n\n&lt;p&gt;Last night was the first time I had all six GPUs running three open-source reasoning models concurrently without stability issues.&lt;/p&gt;\n\n&lt;p&gt;Current setup (high level):&lt;/p&gt;\n\n&lt;p&gt;Threadripper PRO platform&lt;/p&gt;\n\n&lt;p&gt;256GB ECC RAM&lt;/p&gt;\n\n&lt;p&gt;~200GB+ aggregate VRAM across 6 GPUs (mix of 24GB + higher VRAM cards)&lt;/p&gt;\n\n&lt;p&gt;Dual PSU&lt;/p&gt;\n\n&lt;p&gt;Open-air rack&lt;/p&gt;\n\n&lt;p&gt;Ubuntu 24.04&lt;/p&gt;\n\n&lt;p&gt;Gen4 + Gen5 NVMe&lt;/p&gt;\n\n&lt;p&gt;Primary use case is running larger reasoning models locally for internal data analysis + workflow automation&lt;/p&gt;\n\n&lt;p&gt;Currently experimenting with multi-model concurrency and different GPU assignment strategies.&lt;/p&gt;\n\n&lt;p&gt;I would really appreciate feedback from people running similar multi-GPU rigs:&lt;/p&gt;\n\n&lt;p&gt;At this scale, what typically becomes the first real bottleneck for local LLM inference VRAM, PCIe bandwidth, CPU orchestration, memory bandwidth, something else?&lt;/p&gt;\n\n&lt;p&gt;Is mixing GPU types a long-term pain point, or fine as long as models are pinned deliberately?&lt;/p&gt;\n\n&lt;p&gt;For those running multiple reasoning models simultaneously, where did you start seeing diminishing returns?&lt;/p&gt;\n\n&lt;p&gt;How are people handling model scheduling across GPUs \u2014 static pinning vs dynamic routing?&lt;/p&gt;\n\n&lt;p&gt;If you were building today, would you consolidate into fewer high-VRAM GPUs or keep a distributed multi-card setup?&lt;/p&gt;\n\n&lt;p&gt;What is one mistake people make when building larger local LLM workstations?&lt;/p&gt;\n\n&lt;p&gt;Still learning \u2014 would rather hear what I am overlooking than what I got right, but I appreciate any comments questions or feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/1r4mks7", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#5a74cc", "id": "1r4mks7", "is_robot_indexable": true, "report_reasons": null, "author": "shiftyleprechaun", "discussion_type": null, "num_comments": 64, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r4mks7/6gpu_local_llm_workstation_200gb_vram_looking_for/", "stickied": false, "url": "https://www.reddit.com/gallery/1r4mks7", "subreddit_subscribers": 625287, "created_utc": 1771080786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "**Lightweight GGML implementation of Qwen3-TTS 0.6B**\n\n**4x Speedup compared to pytorch pipeline, with \\~2 Gigs of Memory usage.**\n\nHi, this was something I've been working on for the last few days. The result actually performed better than expected, so I'm sharing it here.\n\nThe pipeline was optimized with Metal backend support &amp; CoreML code predictor. The other parts contained operations that were not able to be loaded into the ANE, so only the code predictor was converted.\n\nNo quantization support yet, but coming soon. Turns out using Q8 for the entire pipeline produces bad results. I'm still figuring out which parts are sensitive to quantization and which parts are okay.\n\nSupports all features, including voice cloning", "author_fullname": "t2_7r2u5abm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Qwen3-TTS.cpp", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1r4nt7u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 96, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 96, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?width=140&amp;height=70&amp;auto=webp&amp;s=b91051a0ebfa3ddb49352ec09185ce6d04b7b6e8", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771083792.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Lightweight GGML implementation of Qwen3-TTS 0.6B&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4x Speedup compared to pytorch pipeline, with ~2 Gigs of Memory usage.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi, this was something I&amp;#39;ve been working on for the last few days. The result actually performed better than expected, so I&amp;#39;m sharing it here.&lt;/p&gt;\n\n&lt;p&gt;The pipeline was optimized with Metal backend support &amp;amp; CoreML code predictor. The other parts contained operations that were not able to be loaded into the ANE, so only the code predictor was converted.&lt;/p&gt;\n\n&lt;p&gt;No quantization support yet, but coming soon. Turns out using Q8 for the entire pipeline produces bad results. I&amp;#39;m still figuring out which parts are sensitive to quantization and which parts are okay.&lt;/p&gt;\n\n&lt;p&gt;Supports all features, including voice cloning&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/predict-woo/qwen3-tts.cpp", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?auto=webp&amp;s=d06933787b9f0d1b75de61a543d8168e581388b5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=327786cd9799a543219a9533a920352458ed034a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d2d7542e5756625450eea0a83a618804e2da980", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef9b8c4f2824905dc8ed8404e9b6aa77425510d6", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f7e414f528eab2e463ea468be5c2271e564d443", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88d9094dbb4c4d0ba353197981da9affdcd342ed", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9bfb5eeb43cff7b1c36f5484e2a9a200c06a3fd3", "width": 1080, "height": 540}], "variants": {}, "id": "lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r4nt7u", "is_robot_indexable": true, "report_reasons": null, "author": "redditgivingmeshit", "discussion_type": null, "num_comments": 11, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r4nt7u/qwen3ttscpp/", "stickied": false, "url": "https://github.com/predict-woo/qwen3-tts.cpp", "subreddit_subscribers": 625287, "created_utc": 1771083792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Do you remember projects like [GPT-4chan](https://en.wikipedia.org/wiki/GPT-4Chan)? Back then, training on more \"unconventional\" data sources was far more common than it is today, where most models tend to converge on the same polished, \"helpful assistant\" persona. It\u2019s interesting to think about what we could build with today\u2019s high-performance base models if they were fine-tuned on more distinctive, niche datasets. Done well, that could be genuinely entertaining.\n\nThe recently posted MechaEpstein kind of goes in that direction, but I think there\u2019s room to be more creative than just having it reply with \"&lt;thing&gt; are goy. Sorry for the typos. Sent from my iPhone.\" to every message.", "author_fullname": "t2_nwn8mir6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We need to bring back the \"experimental\" era of LLMs", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r4lg46", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 93, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 93, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771077885.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you remember projects like &lt;a href=\"https://en.wikipedia.org/wiki/GPT-4Chan\"&gt;GPT-4chan&lt;/a&gt;? Back then, training on more &amp;quot;unconventional&amp;quot; data sources was far more common than it is today, where most models tend to converge on the same polished, &amp;quot;helpful assistant&amp;quot; persona. It\u2019s interesting to think about what we could build with today\u2019s high-performance base models if they were fine-tuned on more distinctive, niche datasets. Done well, that could be genuinely entertaining.&lt;/p&gt;\n\n&lt;p&gt;The recently posted MechaEpstein kind of goes in that direction, but I think there\u2019s room to be more creative than just having it reply with &amp;quot;&amp;lt;thing&amp;gt; are goy. Sorry for the typos. Sent from my iPhone.&amp;quot; to every message.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r4lg46", "is_robot_indexable": true, "report_reasons": null, "author": "TemperatureMajor5083", "discussion_type": null, "num_comments": 46, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r4lg46/we_need_to_bring_back_the_experimental_era_of_llms/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r4lg46/we_need_to_bring_back_the_experimental_era_of_llms/", "subreddit_subscribers": 625287, "created_utc": 1771077885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Nathan Lambert (from Ai2) interviewed an NVIDIA's VP of Applied Deep Learning Research: [Why Nvidia builds open models with Bryan Catanzaro](https://www.interconnects.ai/p/why-nvidia-builds-open-models-with)\n\nMany interesting bits, but of course I was hoping for hints of when the next Nemotron3 models were to be released. Nothing really new there, \"2026 H1\" is a pretty broad window. \n\nThis was interesting: \n\n&gt; we\u2019re pre-training our Nemotron-3 Super and Ultra models using FP4 which is a thing that, you know, hasn\u2019t been done publicly anyway and something that, you know, we\u2019re pretty excited about because our GPUs have really awesome FP4 throughput. But obviously, the numerical challenges of, like, trying to train a state-of-the-art language model using four bits is non-trivial. ...\n\n\nHopefully those will be highly performant at Q4 quants. \n\nMany other interesting things in the interview, such as motivations for creating open source models. Nathan asks this of various open-source guests, \"what is your business reason\" -- the NVIDIA VP effectively says, \"so people will keep buying NVIDIA GPUs.\"  (Do they see a lot more businesses running local models, on-prem or in the cloud?)\n\n\nAnother interesting thing: more than once the VP said that \"NVIDIA is a company of volunteers\" -- if you ctrl+f for \"volunteers\" in the transcript you will see it repeatedly.  \n\nThe context is \"how do you manage and coordinate people to work on Nemotron,\" but the wording still caught me off-guard -- \"Hey I want to volunteer there...\"\n\n&gt; 00:22:25 Nathan Lambert: ...Do you have any advice for making the orgs come together? ...\n&gt; \n&gt; 00:23:20 Bryan Catanzaro: You know what\u2019s worked for us is invitation and not control. \n&gt; ... \n&gt; So you know, NVIDIA is a very decentralized company with a lot of volunteers. You know, everybody that works at NVIDIA is a volunteer. And what do I mean by that? Well, I mean, look, the industry is moving quick.\n&gt; \n&gt; You know, people can always move from one job to the next. So the way that we think about the work that we do is like, it\u2019s very decentralized, it\u2019s very much let smart people figure out what they should be doing and then kind of self-organize.\n&gt; ...\n&gt; There\u2019s just an enormous number of brilliant people that have decided that they\u2019re gonna volunteer to make Nemotron awesome, and we\u2019re, we\u2019re starting to see some pretty great things come together.\n\n...etc. \n\nFull interview is very interesting.\n\nEdit: much more excited about the FP4 training in retrospect. \n\nAnd I wonder how hard it would be to REAM the 500B version...", "author_fullname": "t2_m78cdz1nv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nemotron3 Super/Ultra: FP4 pre-training, H1 2026 release, \"NVIDIA is a company of volunteers\" (all from recent NVIDIA interview)", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r4lx7x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1771121809.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771079115.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Nathan Lambert (from Ai2) interviewed an NVIDIA&amp;#39;s VP of Applied Deep Learning Research: &lt;a href=\"https://www.interconnects.ai/p/why-nvidia-builds-open-models-with\"&gt;Why Nvidia builds open models with Bryan Catanzaro&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Many interesting bits, but of course I was hoping for hints of when the next Nemotron3 models were to be released. Nothing really new there, &amp;quot;2026 H1&amp;quot; is a pretty broad window. &lt;/p&gt;\n\n&lt;p&gt;This was interesting: &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;we\u2019re pre-training our Nemotron-3 Super and Ultra models using FP4 which is a thing that, you know, hasn\u2019t been done publicly anyway and something that, you know, we\u2019re pretty excited about because our GPUs have really awesome FP4 throughput. But obviously, the numerical challenges of, like, trying to train a state-of-the-art language model using four bits is non-trivial. ...&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Hopefully those will be highly performant at Q4 quants. &lt;/p&gt;\n\n&lt;p&gt;Many other interesting things in the interview, such as motivations for creating open source models. Nathan asks this of various open-source guests, &amp;quot;what is your business reason&amp;quot; -- the NVIDIA VP effectively says, &amp;quot;so people will keep buying NVIDIA GPUs.&amp;quot;  (Do they see a lot more businesses running local models, on-prem or in the cloud?)&lt;/p&gt;\n\n&lt;p&gt;Another interesting thing: more than once the VP said that &amp;quot;NVIDIA is a company of volunteers&amp;quot; -- if you ctrl+f for &amp;quot;volunteers&amp;quot; in the transcript you will see it repeatedly.  &lt;/p&gt;\n\n&lt;p&gt;The context is &amp;quot;how do you manage and coordinate people to work on Nemotron,&amp;quot; but the wording still caught me off-guard -- &amp;quot;Hey I want to volunteer there...&amp;quot;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;00:22:25 Nathan Lambert: ...Do you have any advice for making the orgs come together? ...&lt;/p&gt;\n\n&lt;p&gt;00:23:20 Bryan Catanzaro: You know what\u2019s worked for us is invitation and not control. \n... \nSo you know, NVIDIA is a very decentralized company with a lot of volunteers. You know, everybody that works at NVIDIA is a volunteer. And what do I mean by that? Well, I mean, look, the industry is moving quick.&lt;/p&gt;\n\n&lt;p&gt;You know, people can always move from one job to the next. So the way that we think about the work that we do is like, it\u2019s very decentralized, it\u2019s very much let smart people figure out what they should be doing and then kind of self-organize.\n...\nThere\u2019s just an enormous number of brilliant people that have decided that they\u2019re gonna volunteer to make Nemotron awesome, and we\u2019re, we\u2019re starting to see some pretty great things come together.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;...etc. &lt;/p&gt;\n\n&lt;p&gt;Full interview is very interesting.&lt;/p&gt;\n\n&lt;p&gt;Edit: much more excited about the FP4 training in retrospect. &lt;/p&gt;\n\n&lt;p&gt;And I wonder how hard it would be to REAM the 500B version...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vitpVdBqesD9B3lEEDBMlvwi-vB3Cc1hF51Thr_NjJg.jpeg?auto=webp&amp;s=d367ebb8dcbcf1cd94dcd35844705af4a390c387", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/vitpVdBqesD9B3lEEDBMlvwi-vB3Cc1hF51Thr_NjJg.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=61f8c401abbfb3df6b9e11b9fa645c7e4cabdd44", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/vitpVdBqesD9B3lEEDBMlvwi-vB3Cc1hF51Thr_NjJg.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad5d3d6c2ce4138a69a9a5e4ff57226a79c9e398", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/vitpVdBqesD9B3lEEDBMlvwi-vB3Cc1hF51Thr_NjJg.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd6088e6a461fbcb496eba228ed2143b2821a694", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/vitpVdBqesD9B3lEEDBMlvwi-vB3Cc1hF51Thr_NjJg.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=02b5b0875de87698dd30bca931fb9a78e09f2c49", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/vitpVdBqesD9B3lEEDBMlvwi-vB3Cc1hF51Thr_NjJg.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f5c096ec4e370de5e47b07967a222d131aac8c0", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/vitpVdBqesD9B3lEEDBMlvwi-vB3Cc1hF51Thr_NjJg.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=00b798876adba31afbb6032fed2f0a628259c71a", "width": 1080, "height": 540}], "variants": {}, "id": "vitpVdBqesD9B3lEEDBMlvwi-vB3Cc1hF51Thr_NjJg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r4lx7x", "is_robot_indexable": true, "report_reasons": null, "author": "RobotRobotWhatDoUSee", "discussion_type": null, "num_comments": 12, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r4lx7x/nemotron3_superultra_fp4_pretraining_h1_2026/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r4lx7x/nemotron3_superultra_fp4_pretraining_h1_2026/", "subreddit_subscribers": 625287, "created_utc": 1771079115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Hey folks, I have been working on **AdaLLM** (repo: [https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm\\_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.\n\n&gt;**Please think of giving the Github repo a STAR if you like it :)** \n\n# Why this is interesting\n\n* NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.\n* Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).\n* No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.\n* Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)\n\n# Benchmarks (RTX 4090)\n\n**Qwen3-8B-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|3.3867|37.79|7.55|\n|2|256|3.5471|72.17|7.55|\n|4|512|3.4392|148.87|7.55|\n|8|1024|3.4459|297.16|7.56|\n|16|2048|4.3636|469.34|7.56|\n\n**Gemma3-27B-it-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|9.3982|13.62|19.83|\n|2|256|9.5545|26.79|19.83|\n|4|512|9.5344|53.70|19.84|\n\nfor Qwen3-8B-NVFP4 I observed \\~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with \\~20-25% throughput loss).\n\n# Quickstart\n\n    pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git\n    \n    adallm serve nvidia/Qwen3-8B-NVFP4\n\n&gt;\\`export NVFP4\\_FP8=1\\` is optional and enables FP8 GEMM path (NVFP4\\_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used.  \n\n\n**Supported models (so far)**\n\n* `nvidia/Qwen3-8B-NVFP4`\n* `BenChaliah/Gemma3-27B-it-NVFP4`\n* Qwen3 MoE variants are supported, but still slow (see README for MoE notes).\n\n**Limitations** \n\n* MoE routing and offload paths are not fully optimized yet (working on it currently)\n* Only NVFP4 weights, no FP16 fallback for decode by design.\n* Targeted at Ada Lovelace (sm\\_89). Needs validation on other Ada cards.\n\n# Repo\n\n[https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)\n\nIf you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.", "author_fullname": "t2_1ake2xuckq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)", "link_flair_richtext": [{"e": "text", "t": "Resources"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r4yg6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resources", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771109586.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I have been working on &lt;strong&gt;AdaLLM&lt;/strong&gt; (repo: &lt;a href=\"https://github.com/BenChaliah/NVFP4-on-4090-vLLM\"&gt;https://github.com/BenChaliah/NVFP4-on-4090-vLLM&lt;/a&gt;) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I&amp;#39;ll be adding support to other models soon.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Please think of giving the Github repo a STAR if you like it :)&lt;/strong&gt; &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;h1&gt;Why this is interesting&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.&lt;/li&gt;\n&lt;li&gt;Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).&lt;/li&gt;\n&lt;li&gt;No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.&lt;/li&gt;\n&lt;li&gt;Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Benchmarks (RTX 4090)&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-8B-NVFP4&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;batch&lt;/th&gt;\n&lt;th align=\"left\"&gt;total tokens&lt;/th&gt;\n&lt;th align=\"left\"&gt;seconds&lt;/th&gt;\n&lt;th align=\"left\"&gt;tok/s&lt;/th&gt;\n&lt;th align=\"left\"&gt;peak GB&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;128&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3867&lt;/td&gt;\n&lt;td align=\"left\"&gt;37.79&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.55&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;256&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.5471&lt;/td&gt;\n&lt;td align=\"left\"&gt;72.17&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.55&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;512&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.4392&lt;/td&gt;\n&lt;td align=\"left\"&gt;148.87&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.55&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;1024&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.4459&lt;/td&gt;\n&lt;td align=\"left\"&gt;297.16&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.56&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;16&lt;/td&gt;\n&lt;td align=\"left\"&gt;2048&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.3636&lt;/td&gt;\n&lt;td align=\"left\"&gt;469.34&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.56&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;strong&gt;Gemma3-27B-it-NVFP4&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;batch&lt;/th&gt;\n&lt;th align=\"left\"&gt;total tokens&lt;/th&gt;\n&lt;th align=\"left\"&gt;seconds&lt;/th&gt;\n&lt;th align=\"left\"&gt;tok/s&lt;/th&gt;\n&lt;th align=\"left\"&gt;peak GB&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;128&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.3982&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.62&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.83&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;256&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5545&lt;/td&gt;\n&lt;td align=\"left\"&gt;26.79&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.83&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;512&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5344&lt;/td&gt;\n&lt;td align=\"left\"&gt;53.70&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.84&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;for Qwen3-8B-NVFP4 I observed ~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with ~20-25% throughput loss).&lt;/p&gt;\n\n&lt;h1&gt;Quickstart&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git\n\nadallm serve nvidia/Qwen3-8B-NVFP4\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;`export NVFP4_FP8=1` is optional and enables FP8 GEMM path (NVFP4_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used.  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Supported models (so far)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;nvidia/Qwen3-8B-NVFP4&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;BenChaliah/Gemma3-27B-it-NVFP4&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Qwen3 MoE variants are supported, but still slow (see README for MoE notes).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MoE routing and offload paths are not fully optimized yet (working on it currently)&lt;/li&gt;\n&lt;li&gt;Only NVFP4 weights, no FP16 fallback for decode by design.&lt;/li&gt;\n&lt;li&gt;Targeted at Ada Lovelace (sm_89). Needs validation on other Ada cards.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Repo&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/BenChaliah/NVFP4-on-4090-vLLM\"&gt;https://github.com/BenChaliah/NVFP4-on-4090-vLLM&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/u_shINC1DSzL47ChI-Wg35o5Rj6BFxsAswJQzG0bk4A.png?auto=webp&amp;s=3366c09cd28e4a546928244bc278bf90811fa20a", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/u_shINC1DSzL47ChI-Wg35o5Rj6BFxsAswJQzG0bk4A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=994817aa508baca842d160e1689567b81d3676e4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/u_shINC1DSzL47ChI-Wg35o5Rj6BFxsAswJQzG0bk4A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0572e108c630a559120618f7c6bef2cca87a87e8", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/u_shINC1DSzL47ChI-Wg35o5Rj6BFxsAswJQzG0bk4A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3eb8130445d5f06c9b20bb24fd6c344b7bfa1efa", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/u_shINC1DSzL47ChI-Wg35o5Rj6BFxsAswJQzG0bk4A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc812961f92c693c0e0681fe648f993fa5b3f1af", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/u_shINC1DSzL47ChI-Wg35o5Rj6BFxsAswJQzG0bk4A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7f3f5b6e877935973fd01ad9f2c1bfb097098ed9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/u_shINC1DSzL47ChI-Wg35o5Rj6BFxsAswJQzG0bk4A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eb206586cc87b94300bf1f40b94c6ab72efb659", "width": 1080, "height": 540}], "variants": {}, "id": "u_shINC1DSzL47ChI-Wg35o5Rj6BFxsAswJQzG0bk4A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ccac2b", "id": "1r4yg6p", "is_robot_indexable": true, "report_reasons": null, "author": "Educational_Cry_7951", "discussion_type": null, "num_comments": 14, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r4yg6p/release_adallm_nvfp4first_inference_on_rtx_4090/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r4yg6p/release_adallm_nvfp4first_inference_on_rtx_4090/", "subreddit_subscribers": 625287, "created_utc": 1771109586.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}
