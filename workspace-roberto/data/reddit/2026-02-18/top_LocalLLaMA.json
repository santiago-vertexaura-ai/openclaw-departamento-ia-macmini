{"kind": "Listing", "data": {"after": "t3_1r77fz7", "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Built a business sim where AI agents run a food truck for 30 days \u2014 location, menu, pricing, staff, inventory. Same scenario for all models.\n    \nOpus made $49K. GPT-5.2 $28K. 8 went bankrupt. Every model that took a loan went bankrupt (8/8).\n   \nThere's also a playable mode \u2014 same simulation, same 34 tools, same leaderboard. You either survive 30 days or go bankrupt, get a result card and land on the shared leaderboard. Example result: https://foodtruckbench.com/r/9E6925\n    \nBenchmark + leaderboard: https://foodtruckbench.com\n    \nPlay: https://foodtruckbench.com/play\n\nGemini 3 Flash Thinking \u2014 only model out of 20+ tested that gets stuck in an infinite decision loop, 100% of runs: https://foodtruckbench.com/blog/gemini-flash\n    \nHappy to answer questions about the sim or results.", "author_fullname": "t2_h1o9yxu27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I gave 12 LLMs $2,000 and a food truck. Only 4 survived.", "link_flair_richtext": [{"e": "text", "t": "Resources"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": false, "name": "t3_1r77swh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": "", "ups": 655, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resources", "can_mod_post": false, "score": 655, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/4sewtkexf2kg1.png?width=140&amp;height=89&amp;auto=webp&amp;s=a53461073c7c8d264b8c0e351c15466a4fc5ded2", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771339326.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Built a business sim where AI agents run a food truck for 30 days \u2014 location, menu, pricing, staff, inventory. Same scenario for all models.&lt;/p&gt;\n\n&lt;p&gt;Opus made $49K. GPT-5.2 $28K. 8 went bankrupt. Every model that took a loan went bankrupt (8/8).&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also a playable mode \u2014 same simulation, same 34 tools, same leaderboard. You either survive 30 days or go bankrupt, get a result card and land on the shared leaderboard. Example result: &lt;a href=\"https://foodtruckbench.com/r/9E6925\"&gt;https://foodtruckbench.com/r/9E6925&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Benchmark + leaderboard: &lt;a href=\"https://foodtruckbench.com\"&gt;https://foodtruckbench.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Play: &lt;a href=\"https://foodtruckbench.com/play\"&gt;https://foodtruckbench.com/play&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Gemini 3 Flash Thinking \u2014 only model out of 20+ tested that gets stuck in an infinite decision loop, 100% of runs: &lt;a href=\"https://foodtruckbench.com/blog/gemini-flash\"&gt;https://foodtruckbench.com/blog/gemini-flash&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to answer questions about the sim or results.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4sewtkexf2kg1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4sewtkexf2kg1.png?auto=webp&amp;s=532d22860ce1a009c22822031fee91b14c18b638", "width": 2318, "height": 1474}, "resolutions": [{"url": "https://preview.redd.it/4sewtkexf2kg1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e890e2c5f5ddfcb7f4fe198389a0599af89bcefb", "width": 108, "height": 68}, {"url": "https://preview.redd.it/4sewtkexf2kg1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3970418a988a70263730910b99bbc29312a6c23", "width": 216, "height": 137}, {"url": "https://preview.redd.it/4sewtkexf2kg1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a85e2081df66d318a9f6a9ebdc136d584b775ff", "width": 320, "height": 203}, {"url": "https://preview.redd.it/4sewtkexf2kg1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c0f7608e083eece043f2953690650ad7c16596a5", "width": 640, "height": 406}, {"url": "https://preview.redd.it/4sewtkexf2kg1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=66a0a0fa4eeade55bc0ffd5f29427d6f79e43810", "width": 960, "height": 610}, {"url": "https://preview.redd.it/4sewtkexf2kg1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=513126c0d88493f2c1326bffd784401197a8cab5", "width": 1080, "height": 686}], "variants": {}, "id": "4sewtkexf2kg1"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ccac2b", "id": "1r77swh", "is_robot_indexable": true, "report_reasons": null, "author": "Disastrous_Theme5906", "discussion_type": null, "num_comments": 214, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/", "stickied": false, "url": "https://i.redd.it/4sewtkexf2kg1.png", "subreddit_subscribers": 627928, "created_utc": 1771339326.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Hey everyone,\n\nI promised that I would update you all with what I was going to do next with the DGX Spark GB10 that I won. It's been a few weeks and I have been primarily heads down on fundraising for my startup trying to automatically improve and evaluate Coding Agents.\n\nSince the last time I posted I became a Dell Pro Precision Ambassador after they saw all of the cool hackathons that I won and stuff I am building that can hopefully make a difference in the world (I am trying to create Brain World Models using a bunch of different types of brain scans to do precision therapeutics, diagnostics, etc. as my Magnus Opus). \n\nThey sent me a Dell Pro Max T2 Tower and another DGX Spark GB10 which I have connected to the previous one that I won. This allows me to continue my work with the limited funds that I have to see how far I can really push the limits of what's possible at the intersection of Healthcare and AI.\n\nDuring Superbowl Weekend I took some time to do a 24-hour hackathon solving a problem that I really care about (even if it wasn't related to my startup).\n\nMy most recent job was at UCSF doing applied neuroscience creating a research-backed tool that screened children for Dyslexia since traditional approaches don\u2019t meet learners where they are so I wanted to take the research I did further and actually create solutions that also did computer adaptive learning.\n\nThrough my research I have come to find that the current solutions for learning languages are antiquated often assuming a \u201cstandard\u201d learner: same pace, same sequence, same practice, same assessments.\n\nBut,\u00a0language learning is deeply personalized.\u00a0Two learners can spend the same amount of time on the same content and walk away with totally different outcomes because the feedback they need could be entirely different with the core problem being that language learning isn\u2019t one-size-fits-all.\n\nMost language tools struggle with a few big issues:\n\n* **Single Language**: Most tools are designed specifically for Native English speakers\n* **Culturally insensitive:**\u00a0Even within the same language there can be different dialects and word/phrase utilization\n* **Static Difficulty:**\u00a0content doesn\u2019t adapt when you\u2019re bored or overwhelmed\n* **Delayed Feedback:**\u00a0you don\u2019t always know\u00a0*what*\u00a0you said wrong or\u00a0*why*\n* **Practice \u2260 assessment:**\u00a0testing is often separate from learning, instead of driving it\n* **Speaking is underserved**: it\u2019s hard to get consistent, personalized speaking practice without 1:1 time\n\nFor many learners, especially kids, the result is predictable:\u00a0*frustration, disengagement, or plateauing.*\n\nSo I built a an automated speech recognition app that adapts in real time combining\u00a0computer adaptive testing and computer adaptive learning\u00a0to personalize the experience as you go.\n\nIt not only transcribes speech, but also evaluates\u00a0phoneme-level pronunciation, which lets the system give targeted feedback (and adapt the next prompt) based on\u00a0*which sounds*\u00a0someone struggles with.\n\nI tried to make it as simple as possible because my primary user base would be teachers that didn't have a lot of time to actually learn new tools and were already struggling with teaching an entire class.\n\nIt  uses natural speaking performance to determine what a student should practice next.\n\nSo instead of providing every child a fixed curriculum, the system continuously adjusts difficulty and targets based on how you\u2019re actually doing rather than just on completion.\n\n**How it Built It**\n\n1. I connected two NVIDIA DGX Spark with the GB10 Grace Blackwell Superchip giving me 256 GB LPDDR5x Coherent Unified System Memory to run inference and the entire workflow locally. I also had the Dell Pro Max T2 Tower, but I couldn't physically bring it to the Notion office so I used Tailscale to SSH into it\n2. I utilized CrisperWhisper, faster-whisper, and a custom transformer to get accurate word-level timestamps, verbatim transcriptions, filler detection, and hallucination mitigation\n3. I fed this directly into a Montreal Forced Aligner to get phoneme level dictation\n4. I then used a heuristics detection algorithm to screen for several disfluencies: Prolongnation, replacement, deletion, addition, and repetition\n5. I included stutter and filler analysis/detection using the SEP-28k dataset and PodcastFillers Dataset\n6. I fed these into AI Agents using both local models, Cartesia's Line Agents, and Notion's Custom Agents to do computer adaptive learning and testing\n\nThe result is a workflow where learning content can evolve quickly while the learner experience stays personalized and measurable.\n\nI want to support learners who don\u2019t thrive in rigid systems and need:\n\n* more repetition (without embarrassment)\n* targeted practice on specific sounds/phrases\n* a pace that adapts to attention and confidence\n* immediate feedback that\u2019s actually actionable\n\nThis project is an early prototype, but it\u2019s a direction I\u2019m genuinely excited about:\u00a0speech-first language learning that adapts to the person, rather than the other way around.\n\n[https://www.youtube.com/watch?v=2RYHu1jyFWI](https://www.youtube.com/watch?v=2RYHu1jyFWI)\n\nI wrote something in medium that has a tiny bit more information [https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub](https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub)\n\nFor those that are wondering what the specs are of the Dell Pro T2 Tower that they sent me:\n\n* Intel Core Ultra 9 285K (36 MB cache, 24 cores, 24 threads, 3.2 GHz to 5.7 GHz, 125W)\n* 128GB: 4 x 32 GB, DDR5, 4400 MT/s\n* 2x - 4TB SSD TLC with DRAM M.2 2280 PCIe Gen4 SED Ready\n* NVIDIA RTX PRO 6000 Blackwell Workstation Edition (600W), 96GB GDDR7", "author_fullname": "t2_1wfl24tty1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The guy that won the NVIDIA Hackathon and an NVIDIA DGX Spark GB10 has won another hackathon with it!", "link_flair_richtext": [{"e": "text", "t": "Other"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r7j7kb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": "", "subreddit_type": "public", "ups": 280, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Other", "can_mod_post": false, "score": 280, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771363350.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I promised that I would update you all with what I was going to do next with the DGX Spark GB10 that I won. It&amp;#39;s been a few weeks and I have been primarily heads down on fundraising for my startup trying to automatically improve and evaluate Coding Agents.&lt;/p&gt;\n\n&lt;p&gt;Since the last time I posted I became a Dell Pro Precision Ambassador after they saw all of the cool hackathons that I won and stuff I am building that can hopefully make a difference in the world (I am trying to create Brain World Models using a bunch of different types of brain scans to do precision therapeutics, diagnostics, etc. as my Magnus Opus). &lt;/p&gt;\n\n&lt;p&gt;They sent me a Dell Pro Max T2 Tower and another DGX Spark GB10 which I have connected to the previous one that I won. This allows me to continue my work with the limited funds that I have to see how far I can really push the limits of what&amp;#39;s possible at the intersection of Healthcare and AI.&lt;/p&gt;\n\n&lt;p&gt;During Superbowl Weekend I took some time to do a 24-hour hackathon solving a problem that I really care about (even if it wasn&amp;#39;t related to my startup).&lt;/p&gt;\n\n&lt;p&gt;My most recent job was at UCSF doing applied neuroscience creating a research-backed tool that screened children for Dyslexia since traditional approaches don\u2019t meet learners where they are so I wanted to take the research I did further and actually create solutions that also did computer adaptive learning.&lt;/p&gt;\n\n&lt;p&gt;Through my research I have come to find that the current solutions for learning languages are antiquated often assuming a \u201cstandard\u201d learner: same pace, same sequence, same practice, same assessments.&lt;/p&gt;\n\n&lt;p&gt;But,\u00a0language learning is deeply personalized.\u00a0Two learners can spend the same amount of time on the same content and walk away with totally different outcomes because the feedback they need could be entirely different with the core problem being that language learning isn\u2019t one-size-fits-all.&lt;/p&gt;\n\n&lt;p&gt;Most language tools struggle with a few big issues:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Single Language&lt;/strong&gt;: Most tools are designed specifically for Native English speakers&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Culturally insensitive:&lt;/strong&gt;\u00a0Even within the same language there can be different dialects and word/phrase utilization&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Static Difficulty:&lt;/strong&gt;\u00a0content doesn\u2019t adapt when you\u2019re bored or overwhelmed&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Delayed Feedback:&lt;/strong&gt;\u00a0you don\u2019t always know\u00a0&lt;em&gt;what&lt;/em&gt;\u00a0you said wrong or\u00a0&lt;em&gt;why&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Practice \u2260 assessment:&lt;/strong&gt;\u00a0testing is often separate from learning, instead of driving it&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Speaking is underserved&lt;/strong&gt;: it\u2019s hard to get consistent, personalized speaking practice without 1:1 time&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For many learners, especially kids, the result is predictable:\u00a0&lt;em&gt;frustration, disengagement, or plateauing.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;So I built a an automated speech recognition app that adapts in real time combining\u00a0computer adaptive testing and computer adaptive learning\u00a0to personalize the experience as you go.&lt;/p&gt;\n\n&lt;p&gt;It not only transcribes speech, but also evaluates\u00a0phoneme-level pronunciation, which lets the system give targeted feedback (and adapt the next prompt) based on\u00a0&lt;em&gt;which sounds&lt;/em&gt;\u00a0someone struggles with.&lt;/p&gt;\n\n&lt;p&gt;I tried to make it as simple as possible because my primary user base would be teachers that didn&amp;#39;t have a lot of time to actually learn new tools and were already struggling with teaching an entire class.&lt;/p&gt;\n\n&lt;p&gt;It  uses natural speaking performance to determine what a student should practice next.&lt;/p&gt;\n\n&lt;p&gt;So instead of providing every child a fixed curriculum, the system continuously adjusts difficulty and targets based on how you\u2019re actually doing rather than just on completion.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How it Built It&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I connected two NVIDIA DGX Spark with the GB10 Grace Blackwell Superchip giving me 256 GB LPDDR5x Coherent Unified System Memory to run inference and the entire workflow locally. I also had the Dell Pro Max T2 Tower, but I couldn&amp;#39;t physically bring it to the Notion office so I used Tailscale to SSH into it&lt;/li&gt;\n&lt;li&gt;I utilized CrisperWhisper, faster-whisper, and a custom transformer to get accurate word-level timestamps, verbatim transcriptions, filler detection, and hallucination mitigation&lt;/li&gt;\n&lt;li&gt;I fed this directly into a Montreal Forced Aligner to get phoneme level dictation&lt;/li&gt;\n&lt;li&gt;I then used a heuristics detection algorithm to screen for several disfluencies: Prolongnation, replacement, deletion, addition, and repetition&lt;/li&gt;\n&lt;li&gt;I included stutter and filler analysis/detection using the SEP-28k dataset and PodcastFillers Dataset&lt;/li&gt;\n&lt;li&gt;I fed these into AI Agents using both local models, Cartesia&amp;#39;s Line Agents, and Notion&amp;#39;s Custom Agents to do computer adaptive learning and testing&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The result is a workflow where learning content can evolve quickly while the learner experience stays personalized and measurable.&lt;/p&gt;\n\n&lt;p&gt;I want to support learners who don\u2019t thrive in rigid systems and need:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;more repetition (without embarrassment)&lt;/li&gt;\n&lt;li&gt;targeted practice on specific sounds/phrases&lt;/li&gt;\n&lt;li&gt;a pace that adapts to attention and confidence&lt;/li&gt;\n&lt;li&gt;immediate feedback that\u2019s actually actionable&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This project is an early prototype, but it\u2019s a direction I\u2019m genuinely excited about:\u00a0speech-first language learning that adapts to the person, rather than the other way around.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=2RYHu1jyFWI\"&gt;https://www.youtube.com/watch?v=2RYHu1jyFWI&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I wrote something in medium that has a tiny bit more information &lt;a href=\"https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub\"&gt;https://medium.com/@brandonin/i-just-won-the-cartesia-hackathon-reinforcing-something-ive-believed-in-for-a-long-time-language-dc93525b2e48?postPublishedType=repub&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For those that are wondering what the specs are of the Dell Pro T2 Tower that they sent me:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intel Core Ultra 9 285K (36 MB cache, 24 cores, 24 threads, 3.2 GHz to 5.7 GHz, 125W)&lt;/li&gt;\n&lt;li&gt;128GB: 4 x 32 GB, DDR5, 4400 MT/s&lt;/li&gt;\n&lt;li&gt;2x - 4TB SSD TLC with DRAM M.2 2280 PCIe Gen4 SED Ready&lt;/li&gt;\n&lt;li&gt;NVIDIA RTX PRO 6000 Blackwell Workstation Edition (600W), 96GB GDDR7&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/b-8bFNmpVy6CBQmUQ9yRafNcSOX_nOo-XyZQWFJuPVQ.jpeg?auto=webp&amp;s=1f22be86d0443f18da7035e58c082e2e3a46c69a", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/b-8bFNmpVy6CBQmUQ9yRafNcSOX_nOo-XyZQWFJuPVQ.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36b40803e9ea01ff7fce8b7b1c5bfcc1a61fed73", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/b-8bFNmpVy6CBQmUQ9yRafNcSOX_nOo-XyZQWFJuPVQ.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b8fc81b0e318afbea0dc97d34dc5df5bf7cfa1f", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/b-8bFNmpVy6CBQmUQ9yRafNcSOX_nOo-XyZQWFJuPVQ.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce07704633ef0a4637e8b40f9ad42b9b305ffd58", "width": 320, "height": 240}], "variants": {}, "id": "b-8bFNmpVy6CBQmUQ9yRafNcSOX_nOo-XyZQWFJuPVQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#94e044", "id": "1r7j7kb", "is_robot_indexable": true, "report_reasons": null, "author": "brandon-i", "discussion_type": null, "num_comments": 30, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7j7kb/the_guy_that_won_the_nvidia_hackathon_and_an/", "subreddit_subscribers": 627928, "created_utc": 1771363350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Hey all. I've been experimenting with tiny matmul-free language models that can be trained and run entirely on CPU. Just released the model.\n\nModel:\u00a0[https://huggingface.co/changcheng967/flashlm-v3-13m](https://huggingface.co/changcheng967/flashlm-v3-13m)\n\nQuick stats:\n\n* 13.6M parameters, d\\_model=256\n* Ternary weights ({-1, 0, +1}) \u2014 inference is just adds and subtracts, no multiplies\n* Trained on 2-thread CPU, no GPU, 1.2 hours\n* 32M tokens from FineWeb-Edu\n* Validation loss: 6.80\n* Uses frozen GPT-2 embeddings (SVD projected) so it doesn't waste training time learning an embedding table\n\nThe model produces grammatical-ish English but with zero coherence \u2014 it's learned syntax but not semantics. For 1.2 hours on a CPU, I'll take it.\n\nThe biggest surprise was that 86% of training time was spent on the output layer (projecting 256 dims to 50,257 vocab). The entire matmul-free ternary core only got 14% of compute. So the \"efficient\" part of the model was essentially starved of training signal by the inefficient softmax head.\n\nWorking on v4 that replaces the softmax with a hierarchical tree structure to fix this bottleneck. If it works, it should allow 5-10x more effective training in the same wall clock time.\n\nCode is MIT licensed. Would love feedback from anyone else working on tiny/efficient models.", "author_fullname": "t2_gfspnpteo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I trained a language model on CPU in 1.2 hours with no matrix multiplications \u2014 here's what I learned", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r7mscr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 197, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 197, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771371750.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. I&amp;#39;ve been experimenting with tiny matmul-free language models that can be trained and run entirely on CPU. Just released the model.&lt;/p&gt;\n\n&lt;p&gt;Model:\u00a0&lt;a href=\"https://huggingface.co/changcheng967/flashlm-v3-13m\"&gt;https://huggingface.co/changcheng967/flashlm-v3-13m&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Quick stats:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;13.6M parameters, d_model=256&lt;/li&gt;\n&lt;li&gt;Ternary weights ({-1, 0, +1}) \u2014 inference is just adds and subtracts, no multiplies&lt;/li&gt;\n&lt;li&gt;Trained on 2-thread CPU, no GPU, 1.2 hours&lt;/li&gt;\n&lt;li&gt;32M tokens from FineWeb-Edu&lt;/li&gt;\n&lt;li&gt;Validation loss: 6.80&lt;/li&gt;\n&lt;li&gt;Uses frozen GPT-2 embeddings (SVD projected) so it doesn&amp;#39;t waste training time learning an embedding table&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The model produces grammatical-ish English but with zero coherence \u2014 it&amp;#39;s learned syntax but not semantics. For 1.2 hours on a CPU, I&amp;#39;ll take it.&lt;/p&gt;\n\n&lt;p&gt;The biggest surprise was that 86% of training time was spent on the output layer (projecting 256 dims to 50,257 vocab). The entire matmul-free ternary core only got 14% of compute. So the &amp;quot;efficient&amp;quot; part of the model was essentially starved of training signal by the inefficient softmax head.&lt;/p&gt;\n\n&lt;p&gt;Working on v4 that replaces the softmax with a hierarchical tree structure to fix this bottleneck. If it works, it should allow 5-10x more effective training in the same wall clock time.&lt;/p&gt;\n\n&lt;p&gt;Code is MIT licensed. Would love feedback from anyone else working on tiny/efficient models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/At15Axm24Ga0Gr2LhVPQDqPimzw0xBtibeQK5YTstq0.png?auto=webp&amp;s=cd386f6aa8430872b6f2e2152579c53ee047965b", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/At15Axm24Ga0Gr2LhVPQDqPimzw0xBtibeQK5YTstq0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c91ac5836333ae97209a632a84a4e26e873d7706", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/At15Axm24Ga0Gr2LhVPQDqPimzw0xBtibeQK5YTstq0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=47033106b546782e43ae90af21e79917960df0b3", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/At15Axm24Ga0Gr2LhVPQDqPimzw0xBtibeQK5YTstq0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=793fb99f0103062c54b0e2baec23d45c2b6a868a", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/At15Axm24Ga0Gr2LhVPQDqPimzw0xBtibeQK5YTstq0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ead260a61df59138ceb58cb0c728867ff328bd48", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/At15Axm24Ga0Gr2LhVPQDqPimzw0xBtibeQK5YTstq0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5d13a93eb5844fa4dc8726c1c0aeb24f690797e", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/At15Axm24Ga0Gr2LhVPQDqPimzw0xBtibeQK5YTstq0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8f4a3336f9d1b68e37dadac61ab6ec928824f00a", "width": 1080, "height": 583}], "variants": {}, "id": "At15Axm24Ga0Gr2LhVPQDqPimzw0xBtibeQK5YTstq0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r7mscr", "is_robot_indexable": true, "report_reasons": null, "author": "Own-Albatross868", "discussion_type": null, "num_comments": 63, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7mscr/i_trained_a_language_model_on_cpu_in_12_hours/", "subreddit_subscribers": 627928, "created_utc": 1771371750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Next time you buy subscriptions from Anthropic or pay for their models, keep in mind where some of your money is going.", "author_fullname": "t2_9d1bee6n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anthropic is deploying 20M$ to support AI regulation in sight of 2026 elections", "link_flair_richtext": [{"e": "text", "t": "News"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1r7fb2k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 189, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 189, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?width=140&amp;height=78&amp;auto=webp&amp;s=bd92914a8aaf8bf864d3651bda93ba846fbfa173", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771354935.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cnbc.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Next time you buy subscriptions from Anthropic or pay for their models, keep in mind where some of your money is going.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.cnbc.com/2026/02/12/anthropic-gives-20-million-to-group-pushing-for-ai-regulations-.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?auto=webp&amp;s=198e382013df4159e6d101d3fa52d13520083abf", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=31a7b0299d8f63f429f7884c11607207182bb8b0", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6a3b99a84ed4c62fa7e8e30a9efa079cdc137c7", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce454d9de5ba1a02a9c233e7662e530b3c5d2af2", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=120a000c29bab7e9ed6a3da128fb13fb873db506", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73f1fa84bbaf876bbb42664c15e1bb35c21e5d33", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21059ef85d17887f446b4735a8da8a21c93e0c5e", "width": 1080, "height": 607}], "variants": {}, "id": "YL-V_hu9Gif4FU34F4m4K7lk-m3_3LBagiDGYFEEe4o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#cc3600", "id": "1r7fb2k", "is_robot_indexable": true, "report_reasons": null, "author": "1998marcom", "discussion_type": null, "num_comments": 60, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r7fb2k/anthropic_is_deploying_20m_to_support_ai/", "stickied": false, "url": "https://www.cnbc.com/2026/02/12/anthropic-gives-20-million-to-group-pushing-for-ai-regulations-.html", "subreddit_subscribers": 627928, "created_utc": 1771354935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "This is the tool and their summary:\n\nhttps://github.com/p-e-w/heretic\n\nHeretic is a tool that removes censorship (aka \"safety alignment\") from transformer-based language models without expensive post-training. It combines an advanced implementation of directional ablation, also known as \"abliteration\" ([Arditi et al. 2024](https://arxiv.org/abs/2406.11717), Lai 2025 ([1](https://huggingface.co/blog/grimjim/projected-abliteration),\u00a0[2](https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration))), with a TPE-based parameter optimizer powered by\u00a0[Optuna](https://optuna.org/).\n\nThis approach enables Heretic to work\u00a0**completely automatically.**\u00a0Heretic finds high-quality abliteration parameters by co-minimizing the number of refusals and the KL divergence from the original model. This results in a decensored model that retains as much of the original model's intelligence as possible. Using Heretic does not require an understanding of transformer internals. In fact, anyone who knows how to run a command-line program can use Heretic to decensor language models.", "author_fullname": "t2_1huav90ix8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Team created a methodology to mathematically change the weights on local LLMs to remove the censorship guardrails. HERETIC", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r7bhel", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 186, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 186, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771347090.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is the tool and their summary:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/p-e-w/heretic\"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Heretic is a tool that removes censorship (aka &amp;quot;safety alignment&amp;quot;) from transformer-based language models without expensive post-training. It combines an advanced implementation of directional ablation, also known as &amp;quot;abliteration&amp;quot; (&lt;a href=\"https://arxiv.org/abs/2406.11717\"&gt;Arditi et al. 2024&lt;/a&gt;, Lai 2025 (&lt;a href=\"https://huggingface.co/blog/grimjim/projected-abliteration\"&gt;1&lt;/a&gt;,\u00a0&lt;a href=\"https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration\"&gt;2&lt;/a&gt;)), with a TPE-based parameter optimizer powered by\u00a0&lt;a href=\"https://optuna.org/\"&gt;Optuna&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;This approach enables Heretic to work\u00a0&lt;strong&gt;completely automatically.&lt;/strong&gt;\u00a0Heretic finds high-quality abliteration parameters by co-minimizing the number of refusals and the KL divergence from the original model. This results in a decensored model that retains as much of the original model&amp;#39;s intelligence as possible. Using Heretic does not require an understanding of transformer internals. In fact, anyone who knows how to run a command-line program can use Heretic to decensor language models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6uUPop2-_tuVkp46VKZvABEIfSGBaxRiivaHIVpFk1Y.png?auto=webp&amp;s=a2279e6f13a05caf63965a16f094d2af088cba32", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/6uUPop2-_tuVkp46VKZvABEIfSGBaxRiivaHIVpFk1Y.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a38681d379a281bfbb17f1b036d95164c795f029", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/6uUPop2-_tuVkp46VKZvABEIfSGBaxRiivaHIVpFk1Y.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4e4d2bafc485549820808f60dd210679a40f9c3d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/6uUPop2-_tuVkp46VKZvABEIfSGBaxRiivaHIVpFk1Y.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=83a37b421e99a7ff264240e08c375946b58b4160", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/6uUPop2-_tuVkp46VKZvABEIfSGBaxRiivaHIVpFk1Y.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e363e73ba1c6fa0f39672012e2911d2698e503a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/6uUPop2-_tuVkp46VKZvABEIfSGBaxRiivaHIVpFk1Y.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=274931873f02b219438f454dc39e8373376d9668", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/6uUPop2-_tuVkp46VKZvABEIfSGBaxRiivaHIVpFk1Y.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6081034c5d4cbeb64319a273ab7ca07750601a46", "width": 1080, "height": 540}], "variants": {}, "id": "6uUPop2-_tuVkp46VKZvABEIfSGBaxRiivaHIVpFk1Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r7bhel", "is_robot_indexable": true, "report_reasons": null, "author": "44th--Hokage", "discussion_type": null, "num_comments": 22, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r7bhel/team_created_a_methodology_to_mathematically/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r7bhel/team_created_a_methodology_to_mathematically/", "subreddit_subscribers": 627928, "created_utc": 1771347090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "", "author_fullname": "t2_58qturpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alibaba's new Qwen3.5-397B-A17B is the #3 open weights model in the Artificial Analysis Intelligence Index", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 109, "top_awarded_type": null, "hide_score": false, "name": "t3_1r7bf1l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "", "ups": 185, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 185, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/b5eytfmy33kg1.jpeg?width=140&amp;height=109&amp;auto=webp&amp;s=9b951f8c0eb183563a5d20edd932f1ae5d8c2efb", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771346965.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/b5eytfmy33kg1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/b5eytfmy33kg1.jpeg?auto=webp&amp;s=6ed67f35af0727bd8cf1f9140fdcf3e96164ffa4", "width": 4096, "height": 3199}, "resolutions": [{"url": "https://preview.redd.it/b5eytfmy33kg1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cbe04eac055d388761bf5794c3d9dadd8493427", "width": 108, "height": 84}, {"url": "https://preview.redd.it/b5eytfmy33kg1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=486f9a46c9f53ebf9ea424577d3f1d2f64b5e416", "width": 216, "height": 168}, {"url": "https://preview.redd.it/b5eytfmy33kg1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=217e539e9899310d546c2b0473db5f11ac336be3", "width": 320, "height": 249}, {"url": "https://preview.redd.it/b5eytfmy33kg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a91c254a446d622c39a0be55a5a8d80f79c11886", "width": 640, "height": 499}, {"url": "https://preview.redd.it/b5eytfmy33kg1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d9228c04a1008afea14075b1fbec386f0c4e0ae5", "width": 960, "height": 749}, {"url": "https://preview.redd.it/b5eytfmy33kg1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c45f1538db9554122cf53d0e47be131460dfd13b", "width": 1080, "height": 843}], "variants": {}, "id": "b5eytfmy33kg1"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r7bf1l", "is_robot_indexable": true, "report_reasons": null, "author": "abdouhlili", "discussion_type": null, "num_comments": 46, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r7bf1l/alibabas_new_qwen35397ba17b_is_the_3_open_weights/", "stickied": false, "url": "https://i.redd.it/b5eytfmy33kg1.jpeg", "subreddit_subscribers": 627928, "created_utc": 1771346965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "I rarely post here but after poking at latest Qwen I felt like sharing my \"vibes\". I did bunch of my little tests (thinking under several constraints) and it performed really well.  \nBut what is really good is fact that it is capable of good outputs even without thinking!  \nSome latest models depend on thinking part really much and that makes them ie 2x more expensive.  \nIt also seems this model is capable of cheap inference +- 1$ .  \nDo you agree?", "author_fullname": "t2_ido7e9by", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Qwen 3.5 397B is Strong one!", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r79dcd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 157, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 157, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771342904.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I rarely post here but after poking at latest Qwen I felt like sharing my &amp;quot;vibes&amp;quot;. I did bunch of my little tests (thinking under several constraints) and it performed really well.&lt;br/&gt;\nBut what is really good is fact that it is capable of good outputs even without thinking!&lt;br/&gt;\nSome latest models depend on thinking part really much and that makes them ie 2x more expensive.&lt;br/&gt;\nIt also seems this model is capable of cheap inference +- 1$ .&lt;br/&gt;\nDo you agree?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r79dcd", "is_robot_indexable": true, "report_reasons": null, "author": "Single_Ring4886", "discussion_type": null, "num_comments": 92, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r79dcd/qwen_35_397b_is_strong_one/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r79dcd/qwen_35_397b_is_strong_one/", "subreddit_subscribers": 627928, "created_utc": 1771342904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Presenting the GLM-5 Technical Report!\n\nhttp://arxiv.org/abs/2602.15763\n\nAfter the launch of GLM-5, we\u2019re pulling back the curtain on how it was built. Key innovations include:\n\n\\- DSA Adoption: Significantly reduces training and inference costs while preserving long-context fidelity\n\n\\- Asynchronous RL Infrastructure: Drastically improves post-training efficiency by decoupling generation from training\n\n\\- Agent RL Algorithms: Enables the model to learn from complex, long-horizon interactions more effectively\n\nThrough these innovations, GLM-5 achieves SOTA performance among open-source models, with particularly strong results in real-world software engineering tasks.", "author_fullname": "t2_c705ri9b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GLM-5 Technical Report", "link_flair_richtext": [{"e": "text", "t": "Resources"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_1r7r7zr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": "", "ups": 152, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resources", "can_mod_post": false, "score": 152, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/phk5j82g36kg1.jpeg?width=140&amp;height=103&amp;auto=webp&amp;s=c85d57c1a5853805a3943e2c9b2fa4532f5067c9", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771383112.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Presenting the GLM-5 Technical Report!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://arxiv.org/abs/2602.15763\"&gt;http://arxiv.org/abs/2602.15763&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After the launch of GLM-5, we\u2019re pulling back the curtain on how it was built. Key innovations include:&lt;/p&gt;\n\n&lt;p&gt;- DSA Adoption: Significantly reduces training and inference costs while preserving long-context fidelity&lt;/p&gt;\n\n&lt;p&gt;- Asynchronous RL Infrastructure: Drastically improves post-training efficiency by decoupling generation from training&lt;/p&gt;\n\n&lt;p&gt;- Agent RL Algorithms: Enables the model to learn from complex, long-horizon interactions more effectively&lt;/p&gt;\n\n&lt;p&gt;Through these innovations, GLM-5 achieves SOTA performance among open-source models, with particularly strong results in real-world software engineering tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/phk5j82g36kg1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/phk5j82g36kg1.jpeg?auto=webp&amp;s=5e0d859dec4c4e69391b4b328baf6d103b3de381", "width": 2586, "height": 1920}, "resolutions": [{"url": "https://preview.redd.it/phk5j82g36kg1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e4195a262aacc5cb282e112719838956cef1ca2", "width": 108, "height": 80}, {"url": "https://preview.redd.it/phk5j82g36kg1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a757541b69188de018f04a9482c703155949825", "width": 216, "height": 160}, {"url": "https://preview.redd.it/phk5j82g36kg1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80a08fe8a6f4f85fc79495a7c4fb35a2ef609d86", "width": 320, "height": 237}, {"url": "https://preview.redd.it/phk5j82g36kg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a4da644c3d3988eba39a218faf8a811456998b3", "width": 640, "height": 475}, {"url": "https://preview.redd.it/phk5j82g36kg1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=90b790c490e2f9a09dd5917c0a363aa6011b94b5", "width": 960, "height": 712}, {"url": "https://preview.redd.it/phk5j82g36kg1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=70277c1d653de952b0371fc37796c7d04ee57258", "width": 1080, "height": 801}], "variants": {}, "id": "phk5j82g36kg1"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ccac2b", "id": "1r7r7zr", "is_robot_indexable": true, "report_reasons": null, "author": "ResearchCrafty1804", "discussion_type": null, "num_comments": 16, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r7r7zr/glm5_technical_report/", "stickied": false, "url": "https://i.redd.it/phk5j82g36kg1.jpeg", "subreddit_subscribers": 627928, "created_utc": 1771383112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "INTELLECT-3.1 is a 106B (A12B) parameter Mixture-of-Experts reasoning model built as a continued training of INTELLECT-3 with additional reinforcement learning on math, coding, software engineering, and agentic tasks.\n\nTraining was performed with prime-rl using environments built with the verifiers library. All training and evaluation environments are available on the Environments Hub.\n\nThe model, training frameworks, and environments are open-sourced under fully-permissive licenses (MIT and Apache 2.0).\n\nFor more details, see the technical report.", "author_fullname": "t2_vqgbql9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PrimeIntellect/INTELLECT-3.1 \u00b7 Hugging Face", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_1r7plp1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": "#bbbdbf", "ups": 121, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 121, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?width=140&amp;height=75&amp;auto=webp&amp;s=2663eb085321f88541d2b11bda5e981dd011a840", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"e": "text", "t": "llama.cpp"}], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771378981.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "huggingface.co", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;INTELLECT-3.1 is a 106B (A12B) parameter Mixture-of-Experts reasoning model built as a continued training of INTELLECT-3 with additional reinforcement learning on math, coding, software engineering, and agentic tasks.&lt;/p&gt;\n\n&lt;p&gt;Training was performed with prime-rl using environments built with the verifiers library. All training and evaluation environments are available on the Environments Hub.&lt;/p&gt;\n\n&lt;p&gt;The model, training frameworks, and environments are open-sourced under fully-permissive licenses (MIT and Apache 2.0).&lt;/p&gt;\n\n&lt;p&gt;For more details, see the technical report.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://huggingface.co/PrimeIntellect/INTELLECT-3.1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?auto=webp&amp;s=4d31de475bd2ccb0d3b8d47c5b7c7b847ef2d8ae", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3d74e28b8c41f88ce6c9255775fc023e543ea81f", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f0474a4444a83ce3a197d1e51531126a0ebcc838", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=789dab7f87b00fd49609f72c681fd11ebe1fb043", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b86fe7ea9ee70a3f83ccb5ad48aa01e8fd98f27", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcfa45865b53cb53916ca29d83948b4c8a4eefd6", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5051f7da446d2badbcbdaebb784293622ca45ea", "width": 1080, "height": 583}], "variants": {}, "id": "HlIthhd4_MOQ5SPqMHH4aU80ZJQIA0QmPpZBs5Jd5L0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "llama.cpp", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r7plp1", "is_robot_indexable": true, "report_reasons": null, "author": "jacek2023", "discussion_type": null, "num_comments": 25, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/LocalLLaMA/comments/1r7plp1/primeintellectintellect31_hugging_face/", "stickied": false, "url": "https://huggingface.co/PrimeIntellect/INTELLECT-3.1", "subreddit_subscribers": 627928, "created_utc": 1771378981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Quantized with NVIDIA's Model Optimizer to FP4. Checkpoint is ~224GB total, 17B active parameters. Apache 2.0 license.\n\n**HF:** [vincentzed-hf/Qwen3.5-397B-A17B-NVFP4](https://huggingface.co/vincentzed-hf/Qwen3.5-397B-A17B-NVFP4)\n\n---\n\n**Install**\n\nYou need SGLang from a specific branch that fixes visual encoder weight handling during quantized inference: (Basically, it was trying to quantize the vision weights, we didn't do that).\n\n```\ngit clone -b vz/qwen3-5 git@github.com:bzhng-development/sglang.git\ncd sglang\nuv pip install -e \"python\"\nuv pip install transformers==5.2.0\n```\n\n---\n\n**Launch (B200/B300, TP=4)**\n\n```\npython3 -m sglang.launch_server \\\n    --model-path vincentzed-hf/Qwen3.5-397B-A17B-NVFP4 \\\n    --quantization modelopt_fp4 \\\n    --tp 4 \\\n    --context-length 262144 \\\n    --reasoning-parser qwen3\n```\n\nSet `--tp 8` for RTX PRO 6000s or if you're running into OOM.\n\n---\n\n**Speculative Decoding (Experimental)**\n\nQwen3.5 has a built-in Multi-Token Prediction head. Worth trying if you have few concurrent users:\n\n```\nSGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \\\n    --model-path vincentzed-hf/Qwen3.5-397B-A17B-NVFP4 \\\n    --quantization modelopt_fp4 \\\n    --tp 8 \\\n    --context-length 262144 \\\n    --reasoning-parser qwen3 \\\n    --speculative-algo NEXTN \\\n    --speculative-num-steps 3 \\\n    --speculative-eagle-topk 1 \\\n    --speculative-num-draft-tokens 4\n```\n\nIf you run into issues (i.e server crashes), you also also remove `SGLANG_ENABLE_SPEC_V2=1` but it can boost up to 10% performance by overlapping some CUDA operations, so it's generally helpful.\n\n---\n\n**Hardware Requirements**\n\n| Config | GPUs | VRAM/GPU | Throughput |\n|---|---|---|---|\n| B300 TP=4 | 4x B300 | 288 GB | ~120 tok/s |\n| B200 TP=4 | 4x B200 | 192 GB | \u2014 |\n| RTX PRO 6000 TP=8 | 8x RTX PRO 6000 | 96 GB | \u2014 |\n\nDefault context is 262K tokens. If you hit OOM, reduce it \u2014 but try to keep at least 128K to preserve thinking quality.\nWe are working on the 1M context support.\n\n---\n\n**Key specs:** 397B total params, 17B active (MoE with 512 experts, 10 active per token), 262K native context (extensible to 1M+), multimodal (text + image + video), supports 201 languages, built-in thinking mode, all the good stuff from Qwen3.5 (Nothing changed, ~99% accuracy)", "author_fullname": "t2_72nt170s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Qwen3.5 NVFP4 (Blackwell) is up!", "link_flair_richtext": [{"e": "text", "t": "Resources"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r77fz7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resources", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1771340183.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771338463.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quantized with NVIDIA&amp;#39;s Model Optimizer to FP4. Checkpoint is ~224GB total, 17B active parameters. Apache 2.0 license.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;HF:&lt;/strong&gt; &lt;a href=\"https://huggingface.co/vincentzed-hf/Qwen3.5-397B-A17B-NVFP4\"&gt;vincentzed-hf/Qwen3.5-397B-A17B-NVFP4&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;You need SGLang from a specific branch that fixes visual encoder weight handling during quantized inference: (Basically, it was trying to quantize the vision weights, we didn&amp;#39;t do that).&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\ngit clone -b vz/qwen3-5 git@github.com:bzhng-development/sglang.git\ncd sglang\nuv pip install -e &amp;quot;python&amp;quot;\nuv pip install transformers==5.2.0\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;Launch (B200/B300, TP=4)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\npython3 -m sglang.launch_server \\\n    --model-path vincentzed-hf/Qwen3.5-397B-A17B-NVFP4 \\\n    --quantization modelopt_fp4 \\\n    --tp 4 \\\n    --context-length 262144 \\\n    --reasoning-parser qwen3\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Set &lt;code&gt;--tp 8&lt;/code&gt; for RTX PRO 6000s or if you&amp;#39;re running into OOM.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;Speculative Decoding (Experimental)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen3.5 has a built-in Multi-Token Prediction head. Worth trying if you have few concurrent users:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nSGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \\\n    --model-path vincentzed-hf/Qwen3.5-397B-A17B-NVFP4 \\\n    --quantization modelopt_fp4 \\\n    --tp 8 \\\n    --context-length 262144 \\\n    --reasoning-parser qwen3 \\\n    --speculative-algo NEXTN \\\n    --speculative-num-steps 3 \\\n    --speculative-eagle-topk 1 \\\n    --speculative-num-draft-tokens 4\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;If you run into issues (i.e server crashes), you also also remove &lt;code&gt;SGLANG_ENABLE_SPEC_V2=1&lt;/code&gt; but it can boost up to 10% performance by overlapping some CUDA operations, so it&amp;#39;s generally helpful.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware Requirements&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Config&lt;/th&gt;\n&lt;th&gt;GPUs&lt;/th&gt;\n&lt;th&gt;VRAM/GPU&lt;/th&gt;\n&lt;th&gt;Throughput&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;B300 TP=4&lt;/td&gt;\n&lt;td&gt;4x B300&lt;/td&gt;\n&lt;td&gt;288 GB&lt;/td&gt;\n&lt;td&gt;~120 tok/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;B200 TP=4&lt;/td&gt;\n&lt;td&gt;4x B200&lt;/td&gt;\n&lt;td&gt;192 GB&lt;/td&gt;\n&lt;td&gt;\u2014&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;RTX PRO 6000 TP=8&lt;/td&gt;\n&lt;td&gt;8x RTX PRO 6000&lt;/td&gt;\n&lt;td&gt;96 GB&lt;/td&gt;\n&lt;td&gt;\u2014&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Default context is 262K tokens. If you hit OOM, reduce it \u2014 but try to keep at least 128K to preserve thinking quality.\nWe are working on the 1M context support.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;Key specs:&lt;/strong&gt; 397B total params, 17B active (MoE with 512 experts, 10 active per token), 262K native context (extensible to 1M+), multimodal (text + image + video), supports 201 languages, built-in thinking mode, all the good stuff from Qwen3.5 (Nothing changed, ~99% accuracy)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/95GuQAlZkg45UATGFrnCWM5l1vr6trkOSnoWqyooK8Y.png?auto=webp&amp;s=4f467ae517ef11a031d5f9bef8e60a55f228b6cd", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/95GuQAlZkg45UATGFrnCWM5l1vr6trkOSnoWqyooK8Y.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=35218951b8946a2840be05a80a60273183a860e5", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/95GuQAlZkg45UATGFrnCWM5l1vr6trkOSnoWqyooK8Y.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d3f14d4f29f7185e61ad8cc531dcd3935dbafab", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/95GuQAlZkg45UATGFrnCWM5l1vr6trkOSnoWqyooK8Y.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=86ae3731dc7145bad6c43c3fe15c0eb0d49d9a0a", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/95GuQAlZkg45UATGFrnCWM5l1vr6trkOSnoWqyooK8Y.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=013ba8f9fd582d2823426d8a98a94ada1064ed38", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/95GuQAlZkg45UATGFrnCWM5l1vr6trkOSnoWqyooK8Y.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a4ff58ed3e9b41142fb8e4b7bdb15deff3bf7c7", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/95GuQAlZkg45UATGFrnCWM5l1vr6trkOSnoWqyooK8Y.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a9f4a2f629a5c06c08a4c3fd1b5d985ae3a10710", "width": 1080, "height": 583}], "variants": {}, "id": "95GuQAlZkg45UATGFrnCWM5l1vr6trkOSnoWqyooK8Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ccac2b", "id": "1r77fz7", "is_robot_indexable": true, "report_reasons": null, "author": "TeekayTK", "discussion_type": null, "num_comments": 16, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/", "subreddit_subscribers": 627928, "created_utc": 1771338463.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}
