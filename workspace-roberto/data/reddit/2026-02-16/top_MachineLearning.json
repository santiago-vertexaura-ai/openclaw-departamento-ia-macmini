{"kind": "Listing", "data": {"after": null, "dist": 8, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I am tired of reading all these clearly LLM generated \u2018I implemented XYZ in python\u2019 and nonsensical long replies on this subreddit. They add absolutely zero value and just creates meaningless noise. Can we block these posts and replies?", "author_fullname": "t2_1itvht4f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we stop these LLM posts and replies? [D]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5gogk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 200, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "cf1711c6-6abd-11ea-9d48-0e44a5e75ec1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 200, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771167589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am tired of reading all these clearly LLM generated \u2018I implemented XYZ in python\u2019 and nonsensical long replies on this subreddit. They add absolutely zero value and just creates meaningless noise. Can we block these posts and replies?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "PhD", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r5gogk", "is_robot_indexable": true, "report_reasons": null, "author": "Playful-Fee-4318", "discussion_type": null, "num_comments": 29, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/MachineLearning/comments/1r5gogk/can_we_stop_these_llm_posts_and_replies_d/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r5gogk/can_we_stop_these_llm_posts_and_replies_d/", "subreddit_subscribers": 3023015, "created_utc": 1771167589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I've tried to use a Transformer decoder architecture to model a sequence of user actions. Unlike an item\\_id paradigm where each interaction is described by the id of the item the user interacted with, I need to express the interaction through a series of attributes.\n\nFor example \"user clicked on a red button on the top left of the screen showing the word Hello\", which today I'm tokenizing as something like \\[BOS\\]\\[action:click\\]\\[what:red\\_button\\]\\[location:top\\_left\\]\\[text:hello\\]. I concatenate a series of interactions together, add a few time gap tokens, and then use standard CE to learn the sequential patterns and predict some key action (like a purchase 7 days in the future). I measure success with a recall@k metric.\n\nI've tried a buch of architectures framed around gpt2, from standard next token prediction, to weighing the down funnel action more, to contrastive heads, but I can hardly move the needle compared to naive baselines (i.e. the user will buy whatever they clicked on the most).\n\nIs there any particular architecture that is a natural fit to the problem I'm describing?", "author_fullname": "t2_4944e5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Advice on sequential recommendations architectures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5u24v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771199556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried to use a Transformer decoder architecture to model a sequence of user actions. Unlike an item_id paradigm where each interaction is described by the id of the item the user interacted with, I need to express the interaction through a series of attributes.&lt;/p&gt;\n\n&lt;p&gt;For example &amp;quot;user clicked on a red button on the top left of the screen showing the word Hello&amp;quot;, which today I&amp;#39;m tokenizing as something like [BOS][action:click][what:red_button][location:top_left][text:hello]. I concatenate a series of interactions together, add a few time gap tokens, and then use standard CE to learn the sequential patterns and predict some key action (like a purchase 7 days in the future). I measure success with a recall@k metric.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried a buch of architectures framed around gpt2, from standard next token prediction, to weighing the down funnel action more, to contrastive heads, but I can hardly move the needle compared to naive baselines (i.e. the user will buy whatever they clicked on the most).&lt;/p&gt;\n\n&lt;p&gt;Is there any particular architecture that is a natural fit to the problem I&amp;#39;m describing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r5u24v", "is_robot_indexable": true, "report_reasons": null, "author": "adjgiulio", "discussion_type": null, "num_comments": 4, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r5u24v/d_advice_on_sequential_recommendations/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r5u24v/d_advice_on_sequential_recommendations/", "subreddit_subscribers": 3023015, "created_utc": 1771199556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "The [paper](https://openreview.net/pdf?id=GhTdNOMfOD) was accepted as a spotlight poster at ICML for 2025.\n\nFor industry, I know that when it comes to time series forecasting, many non faang companies still use ARIMA due to resource cost and efficiency, and they focus on stationary data. I wonder if this model can be a good alternative that can be implemented. Worth noting that TimeBase is benchmarked on long-horizon tasks (96\u2013720 steps), so if your ARIMA usage is for short-term forecasting, the comparison is less direct. What are your thoughts? Their code is public on github, I  provided the link [here](https://github.com/hqh0728/TimeBase)", "author_fullname": "t2_6k7647ey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[R] TimeBase: The Power of Minimalism in Efficient Long-term Time Series\nForecasting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5tzgh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1771199594.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771199363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &lt;a href=\"https://openreview.net/pdf?id=GhTdNOMfOD\"&gt;paper&lt;/a&gt; was accepted as a spotlight poster at ICML for 2025.&lt;/p&gt;\n\n&lt;p&gt;For industry, I know that when it comes to time series forecasting, many non faang companies still use ARIMA due to resource cost and efficiency, and they focus on stationary data. I wonder if this model can be a good alternative that can be implemented. Worth noting that TimeBase is benchmarked on long-horizon tasks (96\u2013720 steps), so if your ARIMA usage is for short-term forecasting, the comparison is less direct. What are your thoughts? Their code is public on github, I  provided the link &lt;a href=\"https://github.com/hqh0728/TimeBase\"&gt;here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r5tzgh", "is_robot_indexable": true, "report_reasons": null, "author": "Whatever_635", "discussion_type": null, "num_comments": 3, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r5tzgh/r_timebase_the_power_of_minimalism_in_efficient/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r5tzgh/r_timebase_the_power_of_minimalism_in_efficient/", "subreddit_subscribers": 3023015, "created_utc": 1771199363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Hi I am preparing for a interview at an AI Lab for LLM inference team with a systems role, not MLE. I have been told I will have an LLM inference related coding round, a design round and an inference optimization related discussion. I have been extensively preparing for these. My Prep for coding is learning to code from scratch the following: SelfAttention, Transformer block, BPE tokenizer, Sampling methods, LV Cache, Bean Search. For other two interviews, I am just studying all the inference design and bottlenecks and old/new work done to eliminate them. I would love to hear if anyone has had similar interview and can share experiences.", "author_fullname": "t2_2c6o0o32", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Interview experience for LLM inference systems position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5vncj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771203855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am preparing for a interview at an AI Lab for LLM inference team with a systems role, not MLE. I have been told I will have an LLM inference related coding round, a design round and an inference optimization related discussion. I have been extensively preparing for these. My Prep for coding is learning to code from scratch the following: SelfAttention, Transformer block, BPE tokenizer, Sampling methods, LV Cache, Bean Search. For other two interviews, I am just studying all the inference design and bottlenecks and old/new work done to eliminate them. I would love to hear if anyone has had similar interview and can share experiences.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r5vncj", "is_robot_indexable": true, "report_reasons": null, "author": "dividebyzero74", "discussion_type": null, "num_comments": 7, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r5vncj/d_interview_experience_for_llm_inference_systems/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r5vncj/d_interview_experience_for_llm_inference_systems/", "subreddit_subscribers": 3023015, "created_utc": 1771203855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Hello everyone!\n\nI am writing here to share a library I am currently developing for research use that filled a niche for me in the Equinox/JAX eco-system:\u00a0[eqx-learn](https://github.com/eqx-learn/eqx-learn).\n\nI am using Equinox as the foundation for my radio-frequency modelling library\u00a0[ParamRF](https://github.com/paramrf/paramrf), and I have absolutely loved the mixed OO/functional style. However, for my research, I require classical ML models (specifically PCA and Gaussian Process Regression), but could not find an Equinox-native library in the ecosystem that was as straight-forward and consistent as scikit-learn.\n\neqx-learn aims to address this, with a JAX-based take on the scikit-learn API. All models in the library are ultimately Equinox Module's, and can be fit using the library's free \"fit\" function. The design is such that models simply \"advertise\" their capabilities by implementing specific methods (e.g. solve(X, y), condition(X, y), loss(), and the \"fit\" function then fits/trains the model accordingly. I believe that this de-coupling of capabilities vs fitting algorithm fits the JAX style better, and also has lots of potential.\n\nAt the moment, eqx-learn addresses all my research needs, but I thought it may be useful to share the library online to advertise that it exists, and mention that I am happy to accept PRs for additional models and fitting algorithms!\n\nAlthough there are no docs, there are short examples in the repo :).\n\nHappy coding!\n\nCheers, Gary", "author_fullname": "t2_6ye0ff77", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P] eqx-learn: Classical machine learning using JAX and Equinox", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r63hz2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Project", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771227943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I am writing here to share a library I am currently developing for research use that filled a niche for me in the Equinox/JAX eco-system:\u00a0&lt;a href=\"https://github.com/eqx-learn/eqx-learn\"&gt;eqx-learn&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I am using Equinox as the foundation for my radio-frequency modelling library\u00a0&lt;a href=\"https://github.com/paramrf/paramrf\"&gt;ParamRF&lt;/a&gt;, and I have absolutely loved the mixed OO/functional style. However, for my research, I require classical ML models (specifically PCA and Gaussian Process Regression), but could not find an Equinox-native library in the ecosystem that was as straight-forward and consistent as scikit-learn.&lt;/p&gt;\n\n&lt;p&gt;eqx-learn aims to address this, with a JAX-based take on the scikit-learn API. All models in the library are ultimately Equinox Module&amp;#39;s, and can be fit using the library&amp;#39;s free &amp;quot;fit&amp;quot; function. The design is such that models simply &amp;quot;advertise&amp;quot; their capabilities by implementing specific methods (e.g. solve(X, y), condition(X, y), loss(), and the &amp;quot;fit&amp;quot; function then fits/trains the model accordingly. I believe that this de-coupling of capabilities vs fitting algorithm fits the JAX style better, and also has lots of potential.&lt;/p&gt;\n\n&lt;p&gt;At the moment, eqx-learn addresses all my research needs, but I thought it may be useful to share the library online to advertise that it exists, and mention that I am happy to accept PRs for additional models and fitting algorithms!&lt;/p&gt;\n\n&lt;p&gt;Although there are no docs, there are short examples in the repo :).&lt;/p&gt;\n\n&lt;p&gt;Happy coding!&lt;/p&gt;\n\n&lt;p&gt;Cheers, Gary&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0EULQ_mx2elBgURQ_UBxKZublxePET6E76mVt6raGgs.png?auto=webp&amp;s=d3f5000b9326658e4e5241424e6fd93ce0bedcf8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0EULQ_mx2elBgURQ_UBxKZublxePET6E76mVt6raGgs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb5b69071eed03723197c922a854dde75d1a0a6b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0EULQ_mx2elBgURQ_UBxKZublxePET6E76mVt6raGgs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8199244c6e5af9e3accfa5390c579ea5fdbb85cf", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0EULQ_mx2elBgURQ_UBxKZublxePET6E76mVt6raGgs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d1f844432aff7f90e7feabb59b1835de8cc057b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0EULQ_mx2elBgURQ_UBxKZublxePET6E76mVt6raGgs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=454d699117a47f087c92e85ebc214ff6fa465b26", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0EULQ_mx2elBgURQ_UBxKZublxePET6E76mVt6raGgs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d3ba0f14d3b0117f919633f4ac04989f51baa670", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0EULQ_mx2elBgURQ_UBxKZublxePET6E76mVt6raGgs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=279238f62b751cc9b45ad6ece381292bdd06cd2b", "width": 1080, "height": 540}], "variants": {}, "id": "0EULQ_mx2elBgURQ_UBxKZublxePET6E76mVt6raGgs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "c6dea51c-19d3-11f0-81a2-deb9d8e21ccb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7d659a", "id": "1r63hz2", "is_robot_indexable": true, "report_reasons": null, "author": "gvcallen", "discussion_type": null, "num_comments": 1, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r63hz2/p_eqxlearn_classical_machine_learning_using_jax/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r63hz2/p_eqxlearn_classical_machine_learning_using_jax/", "subreddit_subscribers": 3023015, "created_utc": 1771227943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "Hi I got 3 official reviews. OA: 2/2.5/2.5 (average OA is 2.33) and Confidence: 4/4/3 (average Confidence is 3.67) \n\nThoughts?", "author_fullname": "t2_15u4ia7tqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] ACL ARR Jan 2026 Reviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r60cnf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771217474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I got 3 official reviews. OA: 2/2.5/2.5 (average OA is 2.33) and Confidence: 4/4/3 (average Confidence is 3.67) &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r60cnf", "is_robot_indexable": true, "report_reasons": null, "author": "srkrrr", "discussion_type": null, "num_comments": 10, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r60cnf/d_acl_arr_jan_2026_reviews/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r60cnf/d_acl_arr_jan_2026_reviews/", "subreddit_subscribers": 3023015, "created_utc": 1771217474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "METR\u2019s Time Horizon benchmark (TH1 / TH1.1) estimates how long a task (in human-expert minutes) a model can complete with **50% reliability**.\n\nhttps://preview.redd.it/sow40w7ccsjg1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=ff50a3774cfdc16bc51beedb869f9affda901c9f\n\nMost people look at p50\\_horizon\\_length.\n\nHowever, the raw TH1.1 YAML also includes working\\_time: **total wall-clock seconds the agent spent across the full suite** (including failed attempts). This is *not* FLOPs or dollars, but it\u2019s still a useful \u201chow much runtime did the eval consume?\u201d signal.\n\nLinks:\n\n* Methodology / TH1 baseline: [https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/)\n* TH1.1 update: [https://metr.org/blog/2026-1-29-time-horizon-1-1/](https://metr.org/blog/2026-1-29-time-horizon-1-1/)\n* Raw YAML: [https://metr.org/assets/benchmark\\_results\\_1\\_1.yaml](https://metr.org/assets/benchmark_results_1_1.yaml)\n* Analysis repo: [https://github.com/METR/eval-analysis-public](https://github.com/METR/eval-analysis-public)\n\n# What jumped out\n\nAt the top end:\n\n* **GPT-5.2:** \\~142.4 hours working\\_time, p50 horizon **394 min**\n* **Claude Opus 4.5:** \\~5.5 hours working\\_time, p50 horizon **320 min**\n\nThat\u2019s roughly **26\u00d7** more total runtime for about **23%** higher horizon.\n\nIf you normalize *horizon per runtime-hour* (very rough efficiency proxy):\n\n* Claude Opus 4.5: **\\~58 min horizon / runtime-hour**\n* GPT-5.2: **\\~2.8 min horizon / runtime-hour**\n\n(checkout the raw YAML for full results)\n\n# Big confounder (important)\n\nDifferent models use different scaffolds in the YAML (e.g. OpenAI entries reference triframe\\_\\* scaffolding, others reference metr\\_agents/react). That can change tool-calling style, retries, and how \u201cexpensive\u201d the eval is in wall-clock time. So I\u2019m treating working\\_time as a **signal**, not a clean apples-to-apples efficiency metric.\n\n# Questions for the sub\n\n1. Should METR publish a **secondary leaderboard** that\u2019s explicit about runtime/attempt budget (or normalize by it)?\n2. How much of this gap do you think is **scaffold behavior** vs model behavior?\n3. Is there a better \u201cefficiency\u201d denominator than working\\_time that METR could realistically publish (token counts, tool-call counts, etc.)?METR\u2019s Time Horizon benchmark (TH1 / TH1.1) estimates how long a task (in human-expert minutes) a model can complete with 50% reliability.Most people look at p50\\_horizon\\_length.However, the raw TH1.1 YAML also includes working\\_time: total wall-clock seconds the agent spent across the full suite (including failed attempts). This is not FLOPs or dollars, but it\u2019s still a useful \u201chow much runtime did the eval consume?\u201d signal.Links:Methodology / TH1 baseline: [https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) TH1.1 update: [https://metr.org/blog/2026-1-29-time-horizon-1-1/](https://metr.org/blog/2026-1-29-time-horizon-1-1/) Raw YAML: [https://metr.org/assets/benchmark\\_results\\_1\\_1.yaml](https://metr.org/assets/benchmark_results_1_1.yaml) Analysis repo: [https://github.com/METR/eval-analysis-publicWhat](https://github.com/METR/eval-analysis-publicWhat) jumped outAt the top end:GPT-5.2: \\~142.4 hours working\\_time, p50 horizon 394 min Claude Opus 4.5: \\~5.5 hours working\\_time, p50 horizon 320 minThat\u2019s roughly 26\u00d7 more total runtime for about 23% higher horizon.If you normalize horizon per runtime-hour (very rough efficiency proxy):Claude Opus 4.5: \\~58 min horizon / runtime-hour GPT-5.2: \\~2.8 min horizon / runtime-hour(checkout the raw YAML for full results)Big confounder (important)Different models use different scaffolds in the YAML (e.g. OpenAI entries reference triframe\\_\\* scaffolding, others reference metr\\_agents/react). That can change tool-calling style, retries, and how \u201cexpensive\u201d the eval is in wall-clock time. So I\u2019m treating working\\_time as a signal, not a clean apples-to-apples efficiency metric.Questions for the subShould METR publish a secondary leaderboard that\u2019s explicit about runtime/attempt budget (or normalize by it)? How much of this gap do you think is scaffold behavior vs model behavior? Is there a better \u201cefficiency\u201d denominator than working\\_time that METR could realistically publish (token counts, tool-call counts, etc.)?\n\nBtw I'm starting a new home for discussions of how AI models compare across several domains and evals, if interested consider joining us at r/CompetitiveAI ", "author_fullname": "t2_1g0v0j0z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] METR TH1.1: \u201cworking_time\u201d is wildly different across models. Quick breakdown + questions.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "media_metadata": {"sow40w7ccsjg1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/sow40w7ccsjg1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3756ef6cc014f4f38ba2b34e024582accc8ce63"}, {"y": 101, "x": 216, "u": "https://preview.redd.it/sow40w7ccsjg1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6f95ac3fe2db296e40c47bb7f804bff47ce72a8"}, {"y": 150, "x": 320, "u": "https://preview.redd.it/sow40w7ccsjg1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1b0e8a9f0665ff3016e87b170c4aa95d22368ed7"}, {"y": 301, "x": 640, "u": "https://preview.redd.it/sow40w7ccsjg1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=51358bb6995c72f576cfd9e5420a6f6999467e68"}, {"y": 452, "x": 960, "u": "https://preview.redd.it/sow40w7ccsjg1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=847729728e88951e30c6c03705c6b7c356fe8a86"}, {"y": 508, "x": 1080, "u": "https://preview.redd.it/sow40w7ccsjg1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cf329c4981fc8458e3a942de43bc96deec9757e6"}], "s": {"y": 565, "x": 1200, "u": "https://preview.redd.it/sow40w7ccsjg1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=ff50a3774cfdc16bc51beedb869f9affda901c9f"}, "id": "sow40w7ccsjg1"}}, "name": "t3_1r60e9a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/eqnfhmFGvAY7Bo6NcCcuWEWCtcy0Fim3Xnn_DZ17Io0.png?width=140&amp;height=83&amp;auto=webp&amp;s=284c919bf210b1daf178951904f679369acc2b26", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771217611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;METR\u2019s Time Horizon benchmark (TH1 / TH1.1) estimates how long a task (in human-expert minutes) a model can complete with &lt;strong&gt;50% reliability&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/sow40w7ccsjg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff50a3774cfdc16bc51beedb869f9affda901c9f\"&gt;https://preview.redd.it/sow40w7ccsjg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff50a3774cfdc16bc51beedb869f9affda901c9f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Most people look at p50_horizon_length.&lt;/p&gt;\n\n&lt;p&gt;However, the raw TH1.1 YAML also includes working_time: &lt;strong&gt;total wall-clock seconds the agent spent across the full suite&lt;/strong&gt; (including failed attempts). This is &lt;em&gt;not&lt;/em&gt; FLOPs or dollars, but it\u2019s still a useful \u201chow much runtime did the eval consume?\u201d signal.&lt;/p&gt;\n\n&lt;p&gt;Links:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Methodology / TH1 baseline: &lt;a href=\"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/\"&gt;https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;TH1.1 update: &lt;a href=\"https://metr.org/blog/2026-1-29-time-horizon-1-1/\"&gt;https://metr.org/blog/2026-1-29-time-horizon-1-1/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Raw YAML: &lt;a href=\"https://metr.org/assets/benchmark_results_1_1.yaml\"&gt;https://metr.org/assets/benchmark_results_1_1.yaml&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Analysis repo: &lt;a href=\"https://github.com/METR/eval-analysis-public\"&gt;https://github.com/METR/eval-analysis-public&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What jumped out&lt;/h1&gt;\n\n&lt;p&gt;At the top end:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GPT-5.2:&lt;/strong&gt; ~142.4 hours working_time, p50 horizon &lt;strong&gt;394 min&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Claude Opus 4.5:&lt;/strong&gt; ~5.5 hours working_time, p50 horizon &lt;strong&gt;320 min&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That\u2019s roughly &lt;strong&gt;26\u00d7&lt;/strong&gt; more total runtime for about &lt;strong&gt;23%&lt;/strong&gt; higher horizon.&lt;/p&gt;\n\n&lt;p&gt;If you normalize &lt;em&gt;horizon per runtime-hour&lt;/em&gt; (very rough efficiency proxy):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Claude Opus 4.5: &lt;strong&gt;~58 min horizon / runtime-hour&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;GPT-5.2: &lt;strong&gt;~2.8 min horizon / runtime-hour&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;(checkout the raw YAML for full results)&lt;/p&gt;\n\n&lt;h1&gt;Big confounder (important)&lt;/h1&gt;\n\n&lt;p&gt;Different models use different scaffolds in the YAML (e.g. OpenAI entries reference triframe_* scaffolding, others reference metr_agents/react). That can change tool-calling style, retries, and how \u201cexpensive\u201d the eval is in wall-clock time. So I\u2019m treating working_time as a &lt;strong&gt;signal&lt;/strong&gt;, not a clean apples-to-apples efficiency metric.&lt;/p&gt;\n\n&lt;h1&gt;Questions for the sub&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should METR publish a &lt;strong&gt;secondary leaderboard&lt;/strong&gt; that\u2019s explicit about runtime/attempt budget (or normalize by it)?&lt;/li&gt;\n&lt;li&gt;How much of this gap do you think is &lt;strong&gt;scaffold behavior&lt;/strong&gt; vs model behavior?&lt;/li&gt;\n&lt;li&gt;Is there a better \u201cefficiency\u201d denominator than working_time that METR could realistically publish (token counts, tool-call counts, etc.)?METR\u2019s Time Horizon benchmark (TH1 / TH1.1) estimates how long a task (in human-expert minutes) a model can complete with 50% reliability.Most people look at p50_horizon_length.However, the raw TH1.1 YAML also includes working_time: total wall-clock seconds the agent spent across the full suite (including failed attempts). This is not FLOPs or dollars, but it\u2019s still a useful \u201chow much runtime did the eval consume?\u201d signal.Links:Methodology / TH1 baseline: &lt;a href=\"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/\"&gt;https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/&lt;/a&gt; TH1.1 update: &lt;a href=\"https://metr.org/blog/2026-1-29-time-horizon-1-1/\"&gt;https://metr.org/blog/2026-1-29-time-horizon-1-1/&lt;/a&gt; Raw YAML: &lt;a href=\"https://metr.org/assets/benchmark_results_1_1.yaml\"&gt;https://metr.org/assets/benchmark_results_1_1.yaml&lt;/a&gt; Analysis repo: &lt;a href=\"https://github.com/METR/eval-analysis-publicWhat\"&gt;https://github.com/METR/eval-analysis-publicWhat&lt;/a&gt; jumped outAt the top end:GPT-5.2: ~142.4 hours working_time, p50 horizon 394 min Claude Opus 4.5: ~5.5 hours working_time, p50 horizon 320 minThat\u2019s roughly 26\u00d7 more total runtime for about 23% higher horizon.If you normalize horizon per runtime-hour (very rough efficiency proxy):Claude Opus 4.5: ~58 min horizon / runtime-hour GPT-5.2: ~2.8 min horizon / runtime-hour(checkout the raw YAML for full results)Big confounder (important)Different models use different scaffolds in the YAML (e.g. OpenAI entries reference triframe_* scaffolding, others reference metr_agents/react). That can change tool-calling style, retries, and how \u201cexpensive\u201d the eval is in wall-clock time. So I\u2019m treating working_time as a signal, not a clean apples-to-apples efficiency metric.Questions for the subShould METR publish a secondary leaderboard that\u2019s explicit about runtime/attempt budget (or normalize by it)? How much of this gap do you think is scaffold behavior vs model behavior? Is there a better \u201cefficiency\u201d denominator than working_time that METR could realistically publish (token counts, tool-call counts, etc.)?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Btw I&amp;#39;m starting a new home for discussions of how AI models compare across several domains and evals, if interested consider joining us at &lt;a href=\"/r/CompetitiveAI\"&gt;r/CompetitiveAI&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "15995904-19d4-11f0-b8c9-0eed6ea89bc1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#26c4d9", "id": "1r60e9a", "is_robot_indexable": true, "report_reasons": null, "author": "snakemas", "discussion_type": null, "num_comments": 1, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r60e9a/d_metr_th11_working_time_is_wildly_different/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r60e9a/d_metr_th11_working_time_is_wildly_different/", "subreddit_subscribers": 3023015, "created_utc": 1771217611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "It decided to blow out my right headphone to make me show fear\n\nSome Background:\n\nI\u2019m working on integrating computer vision and facial tracking into VCV Rack 2 with the goal of, for now, having emotions converted to CV output and granting control over synths. I\u2019ve been adding a lot of features and really trying to innovate with animated panels and whatnot but I got the grand idea to use Machine Learning to have another thing with its own goals of changing your emotions with sound. Did NOT calibrate properly.", "author_fullname": "t2_1z4fhotkwa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P]ut a Neural Network in VCV Rack 2 and told it to make sounds that influence my emotion tracking module\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5ogo5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Project", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771185804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It decided to blow out my right headphone to make me show fear&lt;/p&gt;\n\n&lt;p&gt;Some Background:&lt;/p&gt;\n\n&lt;p&gt;I\u2019m working on integrating computer vision and facial tracking into VCV Rack 2 with the goal of, for now, having emotions converted to CV output and granting control over synths. I\u2019ve been adding a lot of features and really trying to innovate with animated panels and whatnot but I got the grand idea to use Machine Learning to have another thing with its own goals of changing your emotions with sound. Did NOT calibrate properly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "c6dea51c-19d3-11f0-81a2-deb9d8e21ccb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7d659a", "id": "1r5ogo5", "is_robot_indexable": true, "report_reasons": null, "author": "MillieBoeBillie", "discussion_type": null, "num_comments": 4, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/1r5ogo5/put_a_neural_network_in_vcv_rack_2_and_told_it_to/", "stickied": false, "url": "https://www.reddit.com/r/MachineLearning/comments/1r5ogo5/put_a_neural_network_in_vcv_rack_2_and_told_it_to/", "subreddit_subscribers": 3023015, "created_utc": 1771185804.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}
