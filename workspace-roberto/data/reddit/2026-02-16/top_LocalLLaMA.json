{"kind": "Listing", "data": {"after": "t3_1r5lra1", "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "I am highly suspicious that openclaw's virality is organic. I don't know of anyone (online or IRL) that is actually using it and I am deep in the AI ecosystem (both online and IRL). If this sort of thing is up anyone's alley, its the members of localllama - so are you using it? \n\nWith the announcement that OpenAI bought OpenClaw, conspiracy theory is that it was manufactured social media marketing (on twitter) to hype it up before acquisition. Theres no way this graph is real: https://www.star-history.com/#openclaw/openclaw&amp;Comfy-Org/ComfyUI&amp;type=date&amp;legend=top-left", "author_fullname": "t2_xucqa0ilr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone actually using Openclaw?", "link_flair_richtext": [{"e": "text", "t": "Question | Help"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5v1jb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": "", "subreddit_type": "public", "ups": 450, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question | Help", "can_mod_post": false, "score": 450, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771202168.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am highly suspicious that openclaw&amp;#39;s virality is organic. I don&amp;#39;t know of anyone (online or IRL) that is actually using it and I am deep in the AI ecosystem (both online and IRL). If this sort of thing is up anyone&amp;#39;s alley, its the members of localllama - so are you using it? &lt;/p&gt;\n\n&lt;p&gt;With the announcement that OpenAI bought OpenClaw, conspiracy theory is that it was manufactured social media marketing (on twitter) to hype it up before acquisition. Theres no way this graph is real: &lt;a href=\"https://www.star-history.com/#openclaw/openclaw&amp;amp;Comfy-Org/ComfyUI&amp;amp;type=date&amp;amp;legend=top-left\"&gt;https://www.star-history.com/#openclaw/openclaw&amp;amp;Comfy-Org/ComfyUI&amp;amp;type=date&amp;amp;legend=top-left&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h_DRDAnUOUqxtC_rCvf20sP_oO8cssauQSYlzUdEZR8.jpeg?auto=webp&amp;s=ccba87de64afcfb333b39a35be360100409fd798", "width": 1128, "height": 524}, "resolutions": [{"url": "https://external-preview.redd.it/h_DRDAnUOUqxtC_rCvf20sP_oO8cssauQSYlzUdEZR8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b17f2d2ae743ad18b77ec8ea6379865b5900259", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/h_DRDAnUOUqxtC_rCvf20sP_oO8cssauQSYlzUdEZR8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf0d281a66b8ed73bf7e2486effa9032d5a330f4", "width": 216, "height": 100}, {"url": "https://external-preview.redd.it/h_DRDAnUOUqxtC_rCvf20sP_oO8cssauQSYlzUdEZR8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c83f5972305aeb5fdce2644aaaa86d1fe3751124", "width": 320, "height": 148}, {"url": "https://external-preview.redd.it/h_DRDAnUOUqxtC_rCvf20sP_oO8cssauQSYlzUdEZR8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca060f0411d78db646e0884798786a61ddc815ba", "width": 640, "height": 297}, {"url": "https://external-preview.redd.it/h_DRDAnUOUqxtC_rCvf20sP_oO8cssauQSYlzUdEZR8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d61564b9f5d4f86cc42578dba9b224eb70da7f24", "width": 960, "height": 445}, {"url": "https://external-preview.redd.it/h_DRDAnUOUqxtC_rCvf20sP_oO8cssauQSYlzUdEZR8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c5ca768eed724b85743307f9ae53bded9860055d", "width": 1080, "height": 501}], "variants": {}, "id": "h_DRDAnUOUqxtC_rCvf20sP_oO8cssauQSYlzUdEZR8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#5a74cc", "id": "1r5v1jb", "is_robot_indexable": true, "report_reasons": null, "author": "rm-rf-rm", "discussion_type": null, "num_comments": 380, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/", "subreddit_subscribers": 626208, "created_utc": 1771202168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "MiniMax-2.5 is a new open LLM achieving SOTA in coding, agentic tool use and search and office work.\n\nThe 230B parameters (10B active) model has a **200K context** window and unquantized bf16 requires **457GB**.\n\nUnsloth Dynamic **3-bit** GGUF reduces size to **101GB** **(-62%).**\n\n**Official Guide -** [**https://unsloth.ai/docs/models/minimax-2.5**](https://unsloth.ai/docs/models/minimax-2.5)\n\n**GGUF Models -** [**https://huggingface.co/unsloth/MiniMax-M2.5-GGUF**](https://huggingface.co/unsloth/MiniMax-M2.5-GGUF)\n\n**Top LLM, RAG and AI Agents updates of this week** \\- [https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a](https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a)", "author_fullname": "t2_20kicrloo6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "You can run MiniMax-2.5 locally", "link_flair_richtext": [{"e": "text", "t": "Resources"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5h1gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": "", "ups": 423, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resources", "can_mod_post": false, "score": 423, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/hd369oaucojg1.jpeg?width=140&amp;height=140&amp;crop=1:1,smart&amp;auto=webp&amp;s=559da0cd0eb8c4af3212c37ee7f66fd40dbf39c2", "edited": 1771215476.0, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771168491.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MiniMax-2.5 is a new open LLM achieving SOTA in coding, agentic tool use and search and office work.&lt;/p&gt;\n\n&lt;p&gt;The 230B parameters (10B active) model has a &lt;strong&gt;200K context&lt;/strong&gt; window and unquantized bf16 requires &lt;strong&gt;457GB&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Unsloth Dynamic &lt;strong&gt;3-bit&lt;/strong&gt; GGUF reduces size to &lt;strong&gt;101GB&lt;/strong&gt; &lt;strong&gt;(-62%).&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Official Guide -&lt;/strong&gt; &lt;a href=\"https://unsloth.ai/docs/models/minimax-2.5\"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/minimax-2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GGUF Models -&lt;/strong&gt; &lt;a href=\"https://huggingface.co/unsloth/MiniMax-M2.5-GGUF\"&gt;&lt;strong&gt;https://huggingface.co/unsloth/MiniMax-M2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Top LLM, RAG and AI Agents updates of this week&lt;/strong&gt; - &lt;a href=\"https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a\"&gt;https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/hd369oaucojg1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/hd369oaucojg1.jpeg?auto=webp&amp;s=05dfaed258553d0170cdcda32f0982c596a9ef0f", "width": 800, "height": 896}, "resolutions": [{"url": "https://preview.redd.it/hd369oaucojg1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1cd5c6273a2a0ed7b57f61c572e677da0a2eebb6", "width": 108, "height": 120}, {"url": "https://preview.redd.it/hd369oaucojg1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a6f05ea5b7f5a0cb1d3b82aea9eaf42f6256e7f", "width": 216, "height": 241}, {"url": "https://preview.redd.it/hd369oaucojg1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8760842be09d75565bc7f0c42625410cf4dbcb35", "width": 320, "height": 358}, {"url": "https://preview.redd.it/hd369oaucojg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=baf9267391b3836cb000418670d350915c3a8405", "width": 640, "height": 716}], "variants": {}, "id": "hd369oaucojg1"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ccac2b", "id": "1r5h1gj", "is_robot_indexable": true, "report_reasons": null, "author": "Dear-Success-1441", "discussion_type": null, "num_comments": 160, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/", "stickied": false, "url": "https://i.redd.it/hd369oaucojg1.jpeg", "subreddit_subscribers": 626208, "created_utc": 1771168491.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Sources reveal that Alibaba will open-source its next-generation large model, Qwen3.5, tonight on Lunar New Year's Eve. The model reportedly features a comprehensive innovation in its architecture.\n\nhttps://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;format=pjpg&amp;auto=webp&amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06\n\n[https://x.com/Sino\\_Market/status/2023218866370068561?s=20](https://x.com/Sino_Market/status/2023218866370068561?s=20)", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Qwen 3.5 will be released today", "link_flair_richtext": [{"e": "text", "t": "News"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"n8tuw9gmfsjg1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=16ff0ad00683c1aeeb79f4fc300c4881e2a47e5e"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=331ae8133a3d7e82af0acd2011e95f7bfcb5d573"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb0b3e76ee89b2201b3d74bdcd302ba745e870d2"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8922a3c4a32833a12b85f0b6ae9fa115ed0b9f9f"}], "s": {"y": 383, "x": 680, "u": "https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;format=pjpg&amp;auto=webp&amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06"}, "id": "n8tuw9gmfsjg1"}}, "name": "t3_1r60ety", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": "", "subreddit_type": "public", "ups": 339, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_szz4yy2pc", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 339, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=140&amp;height=78&amp;auto=webp&amp;s=ff46b508a3b564db9ac8039bb61d1b0f08588ef3", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771217660.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sources reveal that Alibaba will open-source its next-generation large model, Qwen3.5, tonight on Lunar New Year&amp;#39;s Eve. The model reportedly features a comprehensive innovation in its architecture.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06\"&gt;https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://x.com/Sino_Market/status/2023218866370068561?s=20\"&gt;https://x.com/Sino_Market/status/2023218866370068561?s=20&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#cc3600", "id": "1r60ety", "is_robot_indexable": true, "report_reasons": null, "author": "External_Mood4719", "discussion_type": null, "num_comments": 95, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/", "subreddit_subscribers": 626208, "created_utc": 1771217660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "[https://huggingface.co/Qwen/Qwen3.5-397B-A17B](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)", "author_fullname": "t2_700vdsbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Qwen3.5-397B-A17B is out!!", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r656d7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "#bbbdbf", "subreddit_type": "public", "ups": 333, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "03eba0e8-72f2-11ee-96eb-9a14648159ce", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 333, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"e": "text", "t": "koboldcpp"}], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771234143.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3.5-397B-A17B\"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sxCtTuIrZpTpAOWoo9pt0eNH_oV-_xUiqhE8DoFPkFM.png?auto=webp&amp;s=23a2866ccd730b9643bc6607c0920a446cf24399", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/sxCtTuIrZpTpAOWoo9pt0eNH_oV-_xUiqhE8DoFPkFM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7318ec3ce4509fbace98fa419ca07a197bbf6b12", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/sxCtTuIrZpTpAOWoo9pt0eNH_oV-_xUiqhE8DoFPkFM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=845c40f90d04300d26f682352d92f5119dce277a", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/sxCtTuIrZpTpAOWoo9pt0eNH_oV-_xUiqhE8DoFPkFM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7e7dd4c3ab2924175f5dc3b9816b8c268f639c5", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/sxCtTuIrZpTpAOWoo9pt0eNH_oV-_xUiqhE8DoFPkFM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92b4cb0c011ee0ca8ee5cbb20a760c3a1f372788", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/sxCtTuIrZpTpAOWoo9pt0eNH_oV-_xUiqhE8DoFPkFM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d0618a3224a2591da1e041a5c1cd7a3d816cf77", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/sxCtTuIrZpTpAOWoo9pt0eNH_oV-_xUiqhE8DoFPkFM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb835272d9ec8b6372f3aad7de3527217c39649e", "width": 1080, "height": 583}], "variants": {}, "id": "sxCtTuIrZpTpAOWoo9pt0eNH_oV-_xUiqhE8DoFPkFM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": "koboldcpp", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r656d7", "is_robot_indexable": true, "report_reasons": null, "author": "lolxdmainkaisemaanlu", "discussion_type": null, "num_comments": 43, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/", "subreddit_subscribers": 626208, "created_utc": 1771234143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "Qwen releases Qwen3.5\ud83d\udc9c! Run 3-bit on a 192GB RAM Mac, or 4-bit (MXFP4) on an M3 Ultra with 256GB RAM (or less). Qwen releases the first open model of their Qwen3.5 family. [https://huggingface.co/Qwen/Qwen3.5-397B-A17B](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)\n\nIt performs on par with Gemini 3 Pro, Claude Opus 4.5, and GPT-5.2.\n\nGuide to run them: [https://unsloth.ai/docs/models/qwen3.5](https://unsloth.ai/docs/models/qwen3.5)\n\nUnsloth dynamic GGUFs at: [https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF](https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF)\n\nExcited for this week! \ud83d\ude42", "author_fullname": "t2_5wukhd4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Qwen3.5-397B-A17B Unsloth GGUFs", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1r6599e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": "", "ups": 165, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 165, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://preview.redd.it/zgfpbga5ttjg1.png?width=140&amp;height=140&amp;crop=1:1,smart&amp;auto=webp&amp;s=7bb89743b400d68d16c8d2cdb5da44055ef86dfa", "edited": 1771238828.0, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771234450.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen releases Qwen3.5\ud83d\udc9c! Run 3-bit on a 192GB RAM Mac, or 4-bit (MXFP4) on an M3 Ultra with 256GB RAM (or less). Qwen releases the first open model of their Qwen3.5 family. &lt;a href=\"https://huggingface.co/Qwen/Qwen3.5-397B-A17B\"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It performs on par with Gemini 3 Pro, Claude Opus 4.5, and GPT-5.2.&lt;/p&gt;\n\n&lt;p&gt;Guide to run them: &lt;a href=\"https://unsloth.ai/docs/models/qwen3.5\"&gt;https://unsloth.ai/docs/models/qwen3.5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Unsloth dynamic GGUFs at: &lt;a href=\"https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Excited for this week! \ud83d\ude42&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zgfpbga5ttjg1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zgfpbga5ttjg1.png?auto=webp&amp;s=47e0bf52302e80bbbbe8d673322ee6395725c7af", "width": 4000, "height": 4500}, "resolutions": [{"url": "https://preview.redd.it/zgfpbga5ttjg1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=deea17efaf7688d2e13a0367944b8d1578e430d5", "width": 108, "height": 121}, {"url": "https://preview.redd.it/zgfpbga5ttjg1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7cf9d4bae098cb02594994ed8afafde83764349a", "width": 216, "height": 243}, {"url": "https://preview.redd.it/zgfpbga5ttjg1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5157de5dd5916c6e265c47d6e10bf80d5bf3719", "width": 320, "height": 360}, {"url": "https://preview.redd.it/zgfpbga5ttjg1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b525bb85c217819dae77ecab42757b843211d14", "width": 640, "height": 720}, {"url": "https://preview.redd.it/zgfpbga5ttjg1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=59cb5fb4ddde01deac4a9403d2706e52817b8238", "width": 960, "height": 1080}, {"url": "https://preview.redd.it/zgfpbga5ttjg1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=85cb5de013055905dfb4e64ba4840acfc3861c05", "width": 1080, "height": 1215}], "variants": {}, "id": "zgfpbga5ttjg1"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r6599e", "is_robot_indexable": true, "report_reasons": null, "author": "danielhanchen", "discussion_type": null, "num_comments": 55, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/", "stickied": false, "url": "https://i.redd.it/zgfpbga5ttjg1.png", "subreddit_subscribers": 626208, "created_utc": 1771234450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "[https://github.com/karpathy/nanochat/discussions/481](https://github.com/karpathy/nanochat/discussions/481)\n\nQuote: ..., each year the cost to train GPT-2 is falling to approximately 40% of the previous year. (I think this is an underestimate and that further improvements are still quite possible). The gains come from everywhere: better hardware (H100 vs TPU v3), better software (Flash Attention 3, torch.compile), better algorithms (Muon optimizer, architectural improvements), and better data (FineWeb-edu).\n\n# What Worked\n\n1. **Flash Attention 3** \u2014 \\~9% tok/sec improvement. Native tensor layout, single API for training and inference.\n2. **Sliding window attention** \u2014 `SSSL` pattern. Compute savings without quality loss.\n3. **Muon optimizer overhaul** \u2014 Polar Express, NorMuon variance reduction, cautious weight decay with linear schedule to zero. The cautious WD was a clear win. I tried to delete Muon and couldn't.\n4. **Per-layer residual scalars** \u2014 `x = \u03bb_resid * x + \u03bb_x0 * x0`. Consistent improvement across all model sizes (0.003-0.01 bpb).\n5. **Value Embeddings at alternating layers** \u2014 Models love the value embeddings capacity. Any attempt to reduce it (low-rank, sharing, projections) hurt. We tried U-shaped placement, every layer, alternating\u2014alternating won.\n6. **BOS-aligned dataloader** \u2014 Every row starts with BOS. Made midtraining unnecessary (deleted it). BestFit-Crop packing reduces waste vs naive cropping.\n7. **Hyperparameter sweep at scale** \u2014 320 experiments to find that `x0_beta1=0.96` is optimal at d20. Key lesson: small-scale tuning doesn't transfer. Validate at target scale.\n8. **Scaling law discovery** \u2014 We empirically measured the optimal tokens:params ratio to be \\~10. It's important to do the actual experiment on your own network.\n\n# What Didn't Work\n\n1. **Multi-token prediction (MTP)** \u2014 +13GB memory, no improvement\n2. **Varlen attention** \u2014 BOS-aligned dataloader already handles this to some extent. Attending across BOS document boundaries does not seem to make things much worse.\n3. **FP8 for lm\\_head** \u2014 Works, but +2GB memory (!), only 1% speedup, todo to look into more.\n4. **Half-truncated RoPE** \u2014 No improvement\n5. **Asymmetric softcap** \u2014 Slightly worse\n6. **Skip connections / backout** \u2014 No improvement, +2GB memory\n7. **Smear gate, attention gates** \u2014 Negligible improvement, not worth complexity\n8. **Batch size schedule** \u2014 Deemed a little too complex\n9. **Bigram embeddings (Engram-lite)** \u2014 Works, but not by too much, and it bloats complexity and parameter count by a lot, so it was skipped in the end.\n10. **Hyperball/MuonH** \u2014 Intriguing idea, didn't work out of the box", "author_fullname": "t2_m40tjcn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deflation: Cost to train A.I. models drops 40% per year - Karpathy", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5uhfu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": "", "subreddit_type": "public", "ups": 125, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 125, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"a": ":Discord:", "e": "emoji", "u": "https://emoji.redditmedia.com/08m5x9chttjf1_t5_81eyvm/Discord"}], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771200673.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/karpathy/nanochat/discussions/481\"&gt;https://github.com/karpathy/nanochat/discussions/481&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Quote: ..., each year the cost to train GPT-2 is falling to approximately 40% of the previous year. (I think this is an underestimate and that further improvements are still quite possible). The gains come from everywhere: better hardware (H100 vs TPU v3), better software (Flash Attention 3, torch.compile), better algorithms (Muon optimizer, architectural improvements), and better data (FineWeb-edu).&lt;/p&gt;\n\n&lt;h1&gt;What Worked&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Flash Attention 3&lt;/strong&gt; \u2014 ~9% tok/sec improvement. Native tensor layout, single API for training and inference.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Sliding window attention&lt;/strong&gt; \u2014 &lt;code&gt;SSSL&lt;/code&gt; pattern. Compute savings without quality loss.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Muon optimizer overhaul&lt;/strong&gt; \u2014 Polar Express, NorMuon variance reduction, cautious weight decay with linear schedule to zero. The cautious WD was a clear win. I tried to delete Muon and couldn&amp;#39;t.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Per-layer residual scalars&lt;/strong&gt; \u2014 &lt;code&gt;x = \u03bb_resid * x + \u03bb_x0 * x0&lt;/code&gt;. Consistent improvement across all model sizes (0.003-0.01 bpb).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Value Embeddings at alternating layers&lt;/strong&gt; \u2014 Models love the value embeddings capacity. Any attempt to reduce it (low-rank, sharing, projections) hurt. We tried U-shaped placement, every layer, alternating\u2014alternating won.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;BOS-aligned dataloader&lt;/strong&gt; \u2014 Every row starts with BOS. Made midtraining unnecessary (deleted it). BestFit-Crop packing reduces waste vs naive cropping.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hyperparameter sweep at scale&lt;/strong&gt; \u2014 320 experiments to find that &lt;code&gt;x0_beta1=0.96&lt;/code&gt; is optimal at d20. Key lesson: small-scale tuning doesn&amp;#39;t transfer. Validate at target scale.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scaling law discovery&lt;/strong&gt; \u2014 We empirically measured the optimal tokens:params ratio to be ~10. It&amp;#39;s important to do the actual experiment on your own network.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;What Didn&amp;#39;t Work&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Multi-token prediction (MTP)&lt;/strong&gt; \u2014 +13GB memory, no improvement&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Varlen attention&lt;/strong&gt; \u2014 BOS-aligned dataloader already handles this to some extent. Attending across BOS document boundaries does not seem to make things much worse.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;FP8 for lm_head&lt;/strong&gt; \u2014 Works, but +2GB memory (!), only 1% speedup, todo to look into more.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Half-truncated RoPE&lt;/strong&gt; \u2014 No improvement&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Asymmetric softcap&lt;/strong&gt; \u2014 Slightly worse&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Skip connections / backout&lt;/strong&gt; \u2014 No improvement, +2GB memory&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Smear gate, attention gates&lt;/strong&gt; \u2014 Negligible improvement, not worth complexity&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Batch size schedule&lt;/strong&gt; \u2014 Deemed a little too complex&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Bigram embeddings (Engram-lite)&lt;/strong&gt; \u2014 Works, but not by too much, and it bloats complexity and parameter count by a lot, so it was skipped in the end.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hyperball/MuonH&lt;/strong&gt; \u2014 Intriguing idea, didn&amp;#39;t work out of the box&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sVRwqqgRZiG4XDKBTcFlMdrgCaMjVfm6rAll2ozPwqU.png?auto=webp&amp;s=80012d909c5036de78420908c0346fcab35081a8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/sVRwqqgRZiG4XDKBTcFlMdrgCaMjVfm6rAll2ozPwqU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ec6d2d7eb0639c5a6080cf9d0784239da216ad4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/sVRwqqgRZiG4XDKBTcFlMdrgCaMjVfm6rAll2ozPwqU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=851d0626317ba0e84997b9998ce5ce1ee3c57e1a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/sVRwqqgRZiG4XDKBTcFlMdrgCaMjVfm6rAll2ozPwqU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f63d3f64ef855cbe1471e3be04c877b97cd1b5dc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/sVRwqqgRZiG4XDKBTcFlMdrgCaMjVfm6rAll2ozPwqU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa7057787aab6ef079a747c489e17c0ea0a43245", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/sVRwqqgRZiG4XDKBTcFlMdrgCaMjVfm6rAll2ozPwqU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f51b1f9f7dca8d79bfd6c750e1d73bc3a65371b7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/sVRwqqgRZiG4XDKBTcFlMdrgCaMjVfm6rAll2ozPwqU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9ce4d92958a5a7af1910b572381f807ddd86e96", "width": 1080, "height": 540}], "variants": {}, "id": "sVRwqqgRZiG4XDKBTcFlMdrgCaMjVfm6rAll2ozPwqU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Discord:", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r5uhfu", "is_robot_indexable": true, "report_reasons": null, "author": "Terminator857", "discussion_type": null, "num_comments": 12, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/", "subreddit_subscribers": 626208, "created_utc": 1771200673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "I am running large llms on my\u00a0**8Gb**\u00a0**laptop 3070ti**. I have optimized:\u00a0[**LTX-2**](https://github.com/nalexand/LTX-2-OPTIMIZED)**,** [**Wan2.2**](https://github.com/nalexand/Wan2.2)**,** [**HeartMula**](https://github.com/nalexand/HeartMula-OPTIMIZED-8GB)**,** [**ACE-STEP 1.5**](https://github.com/nalexand/ACE-Step-1.5-OPTIMIZED).\n\nAnd now i abble to run 80b parameters model\u00a0**Qwen3-Coder-Next !!!**\n\n**Instruction here:**\u00a0[https://github.com/nalexand/Qwen3-Coder-OPTIMIZED](https://github.com/nalexand/Qwen3-Coder-OPTIMIZED)\n\nIt is FP8 quant 80Gb in size, it is impossible to fit it on 8Gb VRAM + 32Gb RAM.\n\nSo first i tried offloading to disk with device=\"auto\" using accelerate and i got\u00a0**1 token per 255 second**\u00a0:(.\n\nThan i found that most of large tensors is mlp experts and all other fit in 4.6Gb VRAM so i build custom lazy loading for experts with 2 layers caching VRAM + pinned RAM and got up to 85% cache hit rate and speed up to 1.2t/s it\\`s\u00a0**300x speedup.**\n\n**I wonder what speed will be on 4090 or 5090 desktop..**\n\n    self.max_gpu_cache = 18  # \n    TODO: calculate based on free ram and context window size\n    self.max_ram_cache = 100 # \n    TODO: calculate based on available pinable memory or use unpinned (slow)\n\nTune this two parameters for your RAM/VRAM (each 18 it is about 3GB). For 5090 max\\_gpu\\_cache = 120 and it is &gt;85% cache hit rate. Who can check speed?\n\nBest for loading speed: PCE 5.0 Raid 0 up to 30Gb/s NVME SSD.\n\nAvailable pinable ram (usualy 1/2 RAM) with DMA - much faster than RAM.\n\nHope 5090 will give &gt; 20 t/s..", "author_fullname": "t2_1ub34sfrux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to run Qwen3-Coder-Next 80b parameters model on 8Gb VRAM", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5m4vl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 112, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 112, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1771181127.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1771180394.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running large llms on my\u00a0&lt;strong&gt;8Gb&lt;/strong&gt;\u00a0&lt;strong&gt;laptop 3070ti&lt;/strong&gt;. I have optimized:\u00a0&lt;a href=\"https://github.com/nalexand/LTX-2-OPTIMIZED\"&gt;&lt;strong&gt;LTX-2&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href=\"https://github.com/nalexand/Wan2.2\"&gt;&lt;strong&gt;Wan2.2&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href=\"https://github.com/nalexand/HeartMula-OPTIMIZED-8GB\"&gt;&lt;strong&gt;HeartMula&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href=\"https://github.com/nalexand/ACE-Step-1.5-OPTIMIZED\"&gt;&lt;strong&gt;ACE-STEP 1.5&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;And now i abble to run 80b parameters model\u00a0&lt;strong&gt;Qwen3-Coder-Next !!!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Instruction here:&lt;/strong&gt;\u00a0&lt;a href=\"https://github.com/nalexand/Qwen3-Coder-OPTIMIZED\"&gt;https://github.com/nalexand/Qwen3-Coder-OPTIMIZED&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It is FP8 quant 80Gb in size, it is impossible to fit it on 8Gb VRAM + 32Gb RAM.&lt;/p&gt;\n\n&lt;p&gt;So first i tried offloading to disk with device=&amp;quot;auto&amp;quot; using accelerate and i got\u00a0&lt;strong&gt;1 token per 255 second&lt;/strong&gt;\u00a0:(.&lt;/p&gt;\n\n&lt;p&gt;Than i found that most of large tensors is mlp experts and all other fit in 4.6Gb VRAM so i build custom lazy loading for experts with 2 layers caching VRAM + pinned RAM and got up to 85% cache hit rate and speed up to 1.2t/s it`s\u00a0&lt;strong&gt;300x speedup.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I wonder what speed will be on 4090 or 5090 desktop..&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;self.max_gpu_cache = 18  # \nTODO: calculate based on free ram and context window size\nself.max_ram_cache = 100 # \nTODO: calculate based on available pinable memory or use unpinned (slow)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Tune this two parameters for your RAM/VRAM (each 18 it is about 3GB). For 5090 max_gpu_cache = 120 and it is &amp;gt;85% cache hit rate. Who can check speed?&lt;/p&gt;\n\n&lt;p&gt;Best for loading speed: PCE 5.0 Raid 0 up to 30Gb/s NVME SSD.&lt;/p&gt;\n\n&lt;p&gt;Available pinable ram (usualy 1/2 RAM) with DMA - much faster than RAM.&lt;/p&gt;\n\n&lt;p&gt;Hope 5090 will give &amp;gt; 20 t/s..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jMcTfmic6LjNDHtq6vKDJqu0hHsapIb-6732z2luvTY.png?auto=webp&amp;s=13f9fb4daaaa24b83b8a2ba40caa0ae72c7c7ac5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/jMcTfmic6LjNDHtq6vKDJqu0hHsapIb-6732z2luvTY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b56ce62020979e014cd7c5e94813fcb1d8a76545", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/jMcTfmic6LjNDHtq6vKDJqu0hHsapIb-6732z2luvTY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2fb5d84e2d11ce1227bdd784a4ec9a673a44c623", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/jMcTfmic6LjNDHtq6vKDJqu0hHsapIb-6732z2luvTY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7f59111671d9d296ce762407f6f3b650edfe73c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/jMcTfmic6LjNDHtq6vKDJqu0hHsapIb-6732z2luvTY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=83b2a8ab6dafa5c1ed91dc381c8f1b25dec75bfd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/jMcTfmic6LjNDHtq6vKDJqu0hHsapIb-6732z2luvTY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa537aeca74d843948181038334dfdc930e203bc", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/jMcTfmic6LjNDHtq6vKDJqu0hHsapIb-6732z2luvTY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=964debedf87f756e9cf33ee5b20b1661ad58d049", "width": 1080, "height": 540}], "variants": {}, "id": "jMcTfmic6LjNDHtq6vKDJqu0hHsapIb-6732z2luvTY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r5m4vl", "is_robot_indexable": true, "report_reasons": null, "author": "AccomplishedLeg527", "discussion_type": null, "num_comments": 45, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/", "subreddit_subscribers": 626208, "created_utc": 1771180394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "another 1T model :)\n\n  \nfrom **inclusionAI**:\n\n  \nLing-2.5-1T, Inclusive Intelligence, Instant Impact.\n\nToday, we launch Ling-2.5-1T and make it open source.\n\nThinking models raise the ceiling of intelligence, while instant models expand its reach by balancing efficiency and performance\u2014making AGI not only more powerful, but also more accessible. As the latest flagship instant model in the Ling family, Ling-2.5-1T delivers comprehensive upgrades across model architecture, token efficiency, and preference alignment, designed to bring universally accessible AI to a new level of quality.\n\n* Ling-2.5-1T features 1T total parameters (with 63B active parameters). Its pre-training corpus has expanded from 20T to 29T tokens compared to the previous generation. Leveraging an efficient hybrid linear attention architecture and refined data strategy, the model delivers exceptionally high throughput while processing context lengths of up to 1M tokens.\n* By introducing a composite reward mechanism combining \"Correctness\" and \"Process Redundancy\", Ling-2.5-1T further pushes the frontier of efficiency-performance balance in instant models. At comparable token efficiency levels, Ling-2.5-1T\u2019s reasoning capabilities significantly outperform its predecessor, approaching the level of frontier \"thinking models\" that typically consume \\~4x the output tokens.\n* Through refined alignment strategies\u2014such as bidirectional RL feedback and Agent-based instruction constraint verification\u2014Ling-2.5-1T achieves substantial improvements over the previous generation in preference alignment tasks, including creative writing and instruction following.\n* Trained with Agentic RL in large-scale high-fidelity interactive environments, Ling-2.5-1T is compatible with mainstream agent platforms such as Claude Code, OpenCode, and OpenClaw. It achieves leading open-source performance on the general tool-calling benchmark, BFCL-V4.\n\n", "author_fullname": "t2_vqgbql9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "inclusionAI/Ling-2.5-1T \u00b7 Hugging Face", "link_flair_richtext": [{"e": "text", "t": "New Model"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5qfb8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "#bbbdbf", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "New Model", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=140&amp;height=75&amp;auto=webp&amp;s=bf506bd496b059cde4d53043be8c7a39c91d296d", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"e": "text", "t": "llama.cpp"}], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771190454.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "huggingface.co", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;another 1T model :)&lt;/p&gt;\n\n&lt;p&gt;from &lt;strong&gt;inclusionAI&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;Ling-2.5-1T, Inclusive Intelligence, Instant Impact.&lt;/p&gt;\n\n&lt;p&gt;Today, we launch Ling-2.5-1T and make it open source.&lt;/p&gt;\n\n&lt;p&gt;Thinking models raise the ceiling of intelligence, while instant models expand its reach by balancing efficiency and performance\u2014making AGI not only more powerful, but also more accessible. As the latest flagship instant model in the Ling family, Ling-2.5-1T delivers comprehensive upgrades across model architecture, token efficiency, and preference alignment, designed to bring universally accessible AI to a new level of quality.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ling-2.5-1T features 1T total parameters (with 63B active parameters). Its pre-training corpus has expanded from 20T to 29T tokens compared to the previous generation. Leveraging an efficient hybrid linear attention architecture and refined data strategy, the model delivers exceptionally high throughput while processing context lengths of up to 1M tokens.&lt;/li&gt;\n&lt;li&gt;By introducing a composite reward mechanism combining &amp;quot;Correctness&amp;quot; and &amp;quot;Process Redundancy&amp;quot;, Ling-2.5-1T further pushes the frontier of efficiency-performance balance in instant models. At comparable token efficiency levels, Ling-2.5-1T\u2019s reasoning capabilities significantly outperform its predecessor, approaching the level of frontier &amp;quot;thinking models&amp;quot; that typically consume ~4x the output tokens.&lt;/li&gt;\n&lt;li&gt;Through refined alignment strategies\u2014such as bidirectional RL feedback and Agent-based instruction constraint verification\u2014Ling-2.5-1T achieves substantial improvements over the previous generation in preference alignment tasks, including creative writing and instruction following.&lt;/li&gt;\n&lt;li&gt;Trained with Agentic RL in large-scale high-fidelity interactive environments, Ling-2.5-1T is compatible with mainstream agent platforms such as Claude Code, OpenCode, and OpenClaw. It achieves leading open-source performance on the general tool-calling benchmark, BFCL-V4.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://huggingface.co/inclusionAI/Ling-2.5-1T", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?auto=webp&amp;s=f06557d68afb3a6bf3afc0dc5fadd2787cff2815", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=27d61638b2c124411fb058285d99a636500d0d41", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=14bf030863370956e01418efa6c482871594e3d4", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a52d371b346ad6a594be5f270dab774c845383c", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c049d1c20ccf2dbac44fc910e04d8dc862b0d7b1", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=827828cbadf52af40f8baaa45c0496f430668484", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0e96d07947b72a696a1263da77c69bdc175e67c", "width": 1080, "height": 583}], "variants": {}, "id": "nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "llama.cpp", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1r5qfb8", "is_robot_indexable": true, "report_reasons": null, "author": "jacek2023", "discussion_type": null, "num_comments": 19, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/", "stickied": false, "url": "https://huggingface.co/inclusionAI/Ling-2.5-1T", "subreddit_subscribers": 626208, "created_utc": 1771190454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "I hate hate hate how every time a new model comes out its about how its better at coding. What happened to the heyday of llama 2 finetunes that were all about creative writing and other use cases.\n\nIs it all the vibe coders that are going crazy over the models coding abilities??\n\nLike what about other conversational use cases? I am not even talking about gooning (again opus is best for that too), but long form writing, understanding context at more than a surface level. I think there is a pretty big market for this but it seems like all the models created these days are for fucking coding. Ugh.", "author_fullname": "t2_b5vf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is everything about code now?", "link_flair_richtext": [{"e": "text", "t": "Discussion"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1r63fhu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 81, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 81, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1771227684.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.LocalLLaMA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hate hate hate how every time a new model comes out its about how its better at coding. What happened to the heyday of llama 2 finetunes that were all about creative writing and other use cases.&lt;/p&gt;\n\n&lt;p&gt;Is it all the vibe coders that are going crazy over the models coding abilities??&lt;/p&gt;\n\n&lt;p&gt;Like what about other conversational use cases? I am not even talking about gooning (again opus is best for that too), but long form writing, understanding context at more than a surface level. I think there is a pretty big market for this but it seems like all the models created these days are for fucking coding. Ugh.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#646d73", "id": "1r63fhu", "is_robot_indexable": true, "report_reasons": null, "author": "falconandeagle", "discussion_type": null, "num_comments": 99, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/", "stickied": false, "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/", "subreddit_subscribers": 626208, "created_utc": 1771227684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "LocalLLaMA", "selftext": "I optimized learnable input embeddings for a frozen GPT-2 XL model so that its attention maps display the frames of the Bad Apple music video. The model never saw an image in its life, The optimizer just found the right inputs.  \n  \nThis is a silly little project but I found it interesting, here are some details about how I made that work:  \n\\- freeze the entire model, only optimize a raw 256x1600 embedding tensor per frame  \n\\- target a single attention head (head 0, layer 0), only compute Q and K projections  \n\\- use MSE loss in logit space (pre-softmax) instead of on the attention weights, gives \\~250x stronger gradients  \n\\- multi-start optimization: 3 random seeds, keep the best, refine  \n\\- post-processing: per-row z-score normalization + gaussian blur + magma colormap  \n  \n3286 frames, \\~12 minutes on an RTX 5070 Ti, 4.5 GB VRAM.  \n  \nBlog post (full writeup with math): [https://brayevalerien.com/blog/bad-apple-but-its-gpt2/](https://brayevalerien.com/blog/bad-apple-but-its-gpt2/)  \nCode: [https://github.com/brayevalerien/bad-apple-but-its-gpt2](https://github.com/brayevalerien/bad-apple-but-its-gpt2)  \nYouTube: [https://www.youtube.com/watch?v=UU14rQO6VzU](https://www.youtube.com/watch?v=UU14rQO6VzU)", "author_fullname": "t2_18dkyyzx2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bad Apple but it's GPT-2 XL Attention Maps", "link_flair_richtext": [{"e": "text", "t": "Funny"}], "subreddit_name_prefixed": "r/LocalLLaMA", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1r5lra1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 68, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/UU14rQO6VzU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Bad Apple but it&amp;#39;s GPT-2 XL Attention Maps\"&gt;&lt;/iframe&gt;", "width": 267, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Bad Apple but it's GPT-2 XL Attention Maps", "type": "video", "thumbnail_width": 480, "height": 200, "width": 267, "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/UU14rQO6VzU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Bad Apple but it&amp;#39;s GPT-2 XL Attention Maps\"&gt;&lt;/iframe&gt;", "author_name": "The Latent Explorer", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/UU14rQO6VzU/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@thelatentexplorer"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/UU14rQO6VzU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Bad Apple but it&amp;#39;s GPT-2 XL Attention Maps\"&gt;&lt;/iframe&gt;", "width": 267, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1r5lra1", "height": 200}, "link_flair_text": "Funny", "can_mod_post": false, "score": 68, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/bQ8_O8mHCtpCo5Q-asAduYJCGmACnuapiWfZUdt-AYQ.jpeg?width=140&amp;height=105&amp;auto=webp&amp;s=e363d823e1e308804d49d0cec5d14fd0fb80dfaf", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1771179542.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I optimized learnable input embeddings for a frozen GPT-2 XL model so that its attention maps display the frames of the Bad Apple music video. The model never saw an image in its life, The optimizer just found the right inputs.  &lt;/p&gt;\n\n&lt;p&gt;This is a silly little project but I found it interesting, here are some details about how I made that work:&lt;br/&gt;\n- freeze the entire model, only optimize a raw 256x1600 embedding tensor per frame&lt;br/&gt;\n- target a single attention head (head 0, layer 0), only compute Q and K projections&lt;br/&gt;\n- use MSE loss in logit space (pre-softmax) instead of on the attention weights, gives ~250x stronger gradients&lt;br/&gt;\n- multi-start optimization: 3 random seeds, keep the best, refine&lt;br/&gt;\n- post-processing: per-row z-score normalization + gaussian blur + magma colormap  &lt;/p&gt;\n\n&lt;p&gt;3286 frames, ~12 minutes on an RTX 5070 Ti, 4.5 GB VRAM.  &lt;/p&gt;\n\n&lt;p&gt;Blog post (full writeup with math): &lt;a href=\"https://brayevalerien.com/blog/bad-apple-but-its-gpt2/\"&gt;https://brayevalerien.com/blog/bad-apple-but-its-gpt2/&lt;/a&gt;&lt;br/&gt;\nCode: &lt;a href=\"https://github.com/brayevalerien/bad-apple-but-its-gpt2\"&gt;https://github.com/brayevalerien/bad-apple-but-its-gpt2&lt;/a&gt;&lt;br/&gt;\nYouTube: &lt;a href=\"https://www.youtube.com/watch?v=UU14rQO6VzU\"&gt;https://www.youtube.com/watch?v=UU14rQO6VzU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=UU14rQO6VzU", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bQ8_O8mHCtpCo5Q-asAduYJCGmACnuapiWfZUdt-AYQ.jpeg?auto=webp&amp;s=dfa296685dd91eb63cc051fb754faadd302f995d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/bQ8_O8mHCtpCo5Q-asAduYJCGmACnuapiWfZUdt-AYQ.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=983b25c69c8ffbe9c1bb5abc38edbcaa9b91ebf8", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/bQ8_O8mHCtpCo5Q-asAduYJCGmACnuapiWfZUdt-AYQ.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff9ae50cb12b2308b839f88229e2ecb2931cd01d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/bQ8_O8mHCtpCo5Q-asAduYJCGmACnuapiWfZUdt-AYQ.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b50e9ecd47f69e52dab8879c365cdc6251af431c", "width": 320, "height": 240}], "variants": {}, "id": "bQ8_O8mHCtpCo5Q-asAduYJCGmACnuapiWfZUdt-AYQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_81eyvm", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0dd3bb", "id": "1r5lra1", "is_robot_indexable": true, "report_reasons": null, "author": "TheLatentExplorer", "discussion_type": null, "num_comments": 5, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/LocalLLaMA/comments/1r5lra1/bad_apple_but_its_gpt2_xl_attention_maps/", "stickied": false, "url": "https://www.youtube.com/watch?v=UU14rQO6VzU", "subreddit_subscribers": 626208, "created_utc": 1771179542.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Bad Apple but it's GPT-2 XL Attention Maps", "type": "video", "thumbnail_width": 480, "height": 200, "width": 267, "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/UU14rQO6VzU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Bad Apple but it&amp;#39;s GPT-2 XL Attention Maps\"&gt;&lt;/iframe&gt;", "author_name": "The Latent Explorer", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/UU14rQO6VzU/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@thelatentexplorer"}}, "is_video": false}}], "before": null}}
