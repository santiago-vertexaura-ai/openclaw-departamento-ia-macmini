{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Andreas_3d"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"I built AgentProbe to solve a recurring problem: checking whether an <em>AI</em> <em>agent</em> endpoint actually supports the protocols it claims to.<p>Paste a URL, click Validate, get instant verdicts across HTTP, MCP, A2A/AP2, x402, OAuth, MCP Apps, HTML, and ERC-8004.<p>Each layer gets a detail breakdown \u2014 tools found, payment networks, SSL status, <em>agent</em> card metadata, AP2 detection, etc.<p>It also exposes a built-in MCP server so agents can trigger validation programmatically.<p>Code: <a href=\"https://github.com/FlowMCP/mcp-agent-validator\" rel=\"nofollow\">https://github.com/FlowMCP/mcp-<em>agent</em>-validator</a>\nStack: Node.js 22, vanilla JS, DigitalOcean App <em>Platform</em>.<p>Would love feedback on the detection approach."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: AgentProbe \u2013 Validate <em>AI</em> <em>agent</em> endpoints across 8 protocols in one URL"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://agentprobe.xyz"}},"_tags":["story","author_Andreas_3d","story_46999938","show_hn"],"author":"Andreas_3d","created_at":"2026-02-13T07:31:06Z","created_at_i":1770967866,"num_comments":0,"objectID":"46999938","points":1,"story_id":46999938,"story_text":"I built AgentProbe to solve a recurring problem: checking whether an AI agent endpoint actually supports the protocols it claims to.<p>Paste a URL, click Validate, get instant verdicts across HTTP, MCP, A2A&#x2F;AP2, x402, OAuth, MCP Apps, HTML, and ERC-8004.<p>Each layer gets a detail breakdown \u2014 tools found, payment networks, SSL status, agent card metadata, AP2 detection, etc.<p>It also exposes a built-in MCP server so agents can trigger validation programmatically.<p>Code: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;FlowMCP&#x2F;mcp-agent-validator\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;FlowMCP&#x2F;mcp-agent-validator</a>\nStack: Node.js 22, vanilla JS, DigitalOcean App Platform.<p>Would love feedback on the detection approach.","title":"Show HN: AgentProbe \u2013 Validate AI agent endpoints across 8 protocols in one URL","updated_at":"2026-02-13T07:35:04Z","url":"https://agentprobe.xyz"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"exordex"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"We're shipping <em>AI</em> agents that process payments, query databases, and handle customer PII. Most of them can be tricked into bypassing their own safety policies in under 30 seconds.\nI built Khaos to prove it. It's an open-source chaos engineering framework that adversarially tests <em>AI</em> agents \u2014 prompt injection, tool misuse, data exfiltration, and infrastructure faults before they hit production.<p>The repo includes 6 intentionally vulnerable example agents (support bot, SQL <em>agent</em>, code executor, payment processor, API <em>agent</em>, document processor) with real attack scenarios showing exactly how they break. Try breaking them yourself.<p>Three commands to test your own <em>agent</em>:<p>- pip install khaos-<em>agent</em>\n- khaos discover \n- khaos run my-<em>agent</em> --pack security<p>It works with raw OpenAI/Anthropic, Gemini, LangGraph, CrewAI, AutoGen \u2014 any Python <em>agent</em>. Khaos auto-patches LLM calls to inject faults and log telemetry. No cloud needed, runs 100% locally.<p>Some of what it tests:<p>- Prompt injection (policy bypass, developer mode exploits)\n- Tool misuse (unauthorized DB writes, unscoped API calls)\n- Data exfiltration (PII extraction, credential leakage)\n- Fault injection (timeouts, rate limits, malformed tool responses)<p>We are the first <em>platform</em> that focuses on testing the <em>Agent</em>'s environment, not just the model in the harness.<p>Plus 4 tutorials using the free Gemini API if you want to learn without spending anything.\nRepo: <a href=\"https://github.com/ExordexLabs/khaos-sdk\" rel=\"nofollow\">https://github.com/ExordexLabs/khaos-sdk</a>\nExamples: <a href=\"https://github.com/ExordexLabs/khaos-examples\" rel=\"nofollow\">https://github.com/ExordexLabs/khaos-examples</a>\nBSD licensed. v1.0 just shipped \u2014 the attack library and framework adapters are growing. What agents are you most worried about breaking?"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: Khaos \u2013 Every <em>AI</em> <em>agent</em> I tested broke in under 30 seconds"}},"_tags":["story","author_exordex","story_46997680","show_hn"],"author":"exordex","created_at":"2026-02-13T01:13:28Z","created_at_i":1770945208,"num_comments":0,"objectID":"46997680","points":1,"story_id":46997680,"story_text":"We&#x27;re shipping AI agents that process payments, query databases, and handle customer PII. Most of them can be tricked into bypassing their own safety policies in under 30 seconds.\nI built Khaos to prove it. It&#x27;s an open-source chaos engineering framework that adversarially tests AI agents \u2014 prompt injection, tool misuse, data exfiltration, and infrastructure faults before they hit production.<p>The repo includes 6 intentionally vulnerable example agents (support bot, SQL agent, code executor, payment processor, API agent, document processor) with real attack scenarios showing exactly how they break. Try breaking them yourself.<p>Three commands to test your own agent:<p>- pip install khaos-agent\n- khaos discover \n- khaos run my-agent --pack security<p>It works with raw OpenAI&#x2F;Anthropic, Gemini, LangGraph, CrewAI, AutoGen \u2014 any Python agent. Khaos auto-patches LLM calls to inject faults and log telemetry. No cloud needed, runs 100% locally.<p>Some of what it tests:<p>- Prompt injection (policy bypass, developer mode exploits)\n- Tool misuse (unauthorized DB writes, unscoped API calls)\n- Data exfiltration (PII extraction, credential leakage)\n- Fault injection (timeouts, rate limits, malformed tool responses)<p>We are the first platform that focuses on testing the Agent&#x27;s environment, not just the model in the harness.<p>Plus 4 tutorials using the free Gemini API if you want to learn without spending anything.\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ExordexLabs&#x2F;khaos-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ExordexLabs&#x2F;khaos-sdk</a>\nExamples: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ExordexLabs&#x2F;khaos-examples\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ExordexLabs&#x2F;khaos-examples</a>\nBSD licensed. v1.0 just shipped \u2014 the attack library and framework adapters are growing. What agents are you most worried about breaking?","title":"Show HN: Khaos \u2013 Every AI agent I tested broke in under 30 seconds","updated_at":"2026-02-13T01:17:48Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"afridi_epilabs"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"Hi HN \u2014 I\u2019m the founder of EPI.<p>EPI is a portable, cryptographically sealed artifact format (.epi) for <em>AI</em> <em>agent</em> execution.<p>Problem: When <em>AI</em> systems run in production and something goes wrong, there\u2019s no tamper-proof way to prove exactly what happened.<p>EPI records execution steps, inputs/outputs, metadata, and signatures into a verifiable bundle that can be replayed and audited.<p>It\u2019s open-source and installable via pip.<p>I\u2019d love feedback from:\n\u2013 ML infra engineers\n\u2013 <em>Platform</em> teams running <em>AI</em> <em>agents</em>\n\u2013 Security engineers<p>Happy to answer any technical questions."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: EPI \u2013 Cryptographically verifiable execution artifacts for <em>AI</em> <em>agents</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/mohdibrahimaiml/epi-recorder"}},"_tags":["story","author_afridi_epilabs","story_46996207","show_hn"],"author":"afridi_epilabs","children":[46996245,46996249],"created_at":"2026-02-12T22:29:39Z","created_at_i":1770935379,"num_comments":0,"objectID":"46996207","points":1,"story_id":46996207,"story_text":"Hi HN \u2014 I\u2019m the founder of EPI.<p>EPI is a portable, cryptographically sealed artifact format (.epi) for AI agent execution.<p>Problem: When AI systems run in production and something goes wrong, there\u2019s no tamper-proof way to prove exactly what happened.<p>EPI records execution steps, inputs&#x2F;outputs, metadata, and signatures into a verifiable bundle that can be replayed and audited.<p>It\u2019s open-source and installable via pip.<p>I\u2019d love feedback from:\n\u2013 ML infra engineers\n\u2013 Platform teams running AI agents\n\u2013 Security engineers<p>Happy to answer any technical questions.","title":"Show HN: EPI \u2013 Cryptographically verifiable execution artifacts for AI agents","updated_at":"2026-02-12T22:35:18Z","url":"https://github.com/mohdibrahimaiml/epi-recorder"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"danFctr"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"Hi HN,<p>Every week I watched Okta admins burn hours answering ad-hoc questions from security teams: &quot;Who has access to Salesforce?&quot;, &quot;Find all contractors with GitHub access who haven't used MFA in 30 days.&quot; The answers always involved the same painful loop: dig through a slow web console, chain API calls, correlate CSVs, write throwaway Python scripts. Repeat next week.<p>I spent 12 months building Tako <em>AI</em> to fix this. You ask a question in plain English, it returns verified data.<p>GitHub: <a href=\"https://github.com/fctr-id/okta-ai-agent\" rel=\"nofollow\">https://github.com/fctr-id/okta-<em>ai</em>-<em>agent</em></a><p>THE ONE RULE: Zero hallucinations.<p>In identity and access management, a wrong answer is worse than no answer. If an <em>AI</em> tells your CISO a contractor doesn't have admin access when they actually do, that's a security incident. Tako never &quot;predicts&quot; an answer. It writes the code to find the answer, executes it, and returns the raw result. Ask the same question twice, you get the same data.<p>THE HARDEST PROBLEM: Scaling to 107+ API endpoints<p>Most <em>AI</em> agents break down past 10-20 tools. They hallucinate parameters, call wrong endpoints, invent fields that don't exist. We went through five architecture rewrites over 12 months.<p>Each iteration: new LLM drops (GPT-4, Claude 3.5), we rebuild the <em>agent</em>, hit context limits, watch it snowball into gibberish. The breakthrough wasn't bigger context windows \u2014 it was precise context engineering. Instead of cramming 107 endpoint definitions into a prompt, the <em>agent</em> dynamically discovers the right spec for the task at hand. It reads a custom JSON API documentation file for the specific endpoint it needs, constructs validated requests, executes them. No hardcoded tools per endpoint. We're adding full CRUD operations next.<p>HOW IT WORKS:<p>Multi-<em>agent</em> architecture based on ReAct (Reasoning + Acting). Each <em>agent</em> has a narrow job:<p>\u2022 Router: analyzes your question, decides local cache vs live API\n\u2022 SQL <em>Agent</em>: queries local SQLite cache for bulk data (10k users in milliseconds vs minutes via API)\n\u2022 API <em>Agent</em>: handles live Okta calls\n\u2022 Synthesis <em>Agent</em>: merges everything into final verified report<p>The API <em>Agent</em> has a self-healing loop that surprised us. When generated code fails \u2014 wrong parameter name, rate limit hit, API schema changed \u2014 it traps the stack trace, feeds the error back to the LLM with context, and rewrites the code. We've seen it recover from Okta API changes we didn't even know happened yet.<p>PRIVACY &amp; SECURITY:<p>Runs 100% locally in Docker. You bring your own LLM keys (OpenAI, Anthropic, Gemini, or Ollama for fully offline). Your employee PII never leaves your machine.<p>READ-ONLY by design. All generated Python and API code runs in a sandboxed environment. Every execution is automatically verified against security patterns before running \u2014 code is logged and available for audit, but you don't manually approve each query.<p>WHAT'S NEXT:<p>We see this as a <em>platform</em>, not just an Okta tool. The pattern (local cache + live ReAct <em>agent</em> + self-healing code execution) generalizes to any SaaS API. Google Workspace, Slack, Workday \u2014 same architecture, different spec files. Working on write operations with human-in-the-loop approval next.<p>What would you want <em>AI</em> agents to actually do for you in 2026? Where do you see this tech going beyond chatbots?<p>\u2014Dan"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: Tako <em>AI</em> \u2013 <em>Agent</em> for Okta With Natural language (zero hallucination)"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"https://github.com/fctr-id/okta-<em>ai</em>-<em>agent</em>"}},"_tags":["story","author_danFctr","story_46990146","show_hn"],"author":"danFctr","created_at":"2026-02-12T15:38:38Z","created_at_i":1770910718,"num_comments":0,"objectID":"46990146","points":1,"story_id":46990146,"story_text":"Hi HN,<p>Every week I watched Okta admins burn hours answering ad-hoc questions from security teams: &quot;Who has access to Salesforce?&quot;, &quot;Find all contractors with GitHub access who haven&#x27;t used MFA in 30 days.&quot; The answers always involved the same painful loop: dig through a slow web console, chain API calls, correlate CSVs, write throwaway Python scripts. Repeat next week.<p>I spent 12 months building Tako AI to fix this. You ask a question in plain English, it returns verified data.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;fctr-id&#x2F;okta-ai-agent\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;fctr-id&#x2F;okta-ai-agent</a><p>THE ONE RULE: Zero hallucinations.<p>In identity and access management, a wrong answer is worse than no answer. If an AI tells your CISO a contractor doesn&#x27;t have admin access when they actually do, that&#x27;s a security incident. Tako never &quot;predicts&quot; an answer. It writes the code to find the answer, executes it, and returns the raw result. Ask the same question twice, you get the same data.<p>THE HARDEST PROBLEM: Scaling to 107+ API endpoints<p>Most AI agents break down past 10-20 tools. They hallucinate parameters, call wrong endpoints, invent fields that don&#x27;t exist. We went through five architecture rewrites over 12 months.<p>Each iteration: new LLM drops (GPT-4, Claude 3.5), we rebuild the agent, hit context limits, watch it snowball into gibberish. The breakthrough wasn&#x27;t bigger context windows \u2014 it was precise context engineering. Instead of cramming 107 endpoint definitions into a prompt, the agent dynamically discovers the right spec for the task at hand. It reads a custom JSON API documentation file for the specific endpoint it needs, constructs validated requests, executes them. No hardcoded tools per endpoint. We&#x27;re adding full CRUD operations next.<p>HOW IT WORKS:<p>Multi-agent architecture based on ReAct (Reasoning + Acting). Each agent has a narrow job:<p>\u2022 Router: analyzes your question, decides local cache vs live API\n\u2022 SQL Agent: queries local SQLite cache for bulk data (10k users in milliseconds vs minutes via API)\n\u2022 API Agent: handles live Okta calls\n\u2022 Synthesis Agent: merges everything into final verified report<p>The API Agent has a self-healing loop that surprised us. When generated code fails \u2014 wrong parameter name, rate limit hit, API schema changed \u2014 it traps the stack trace, feeds the error back to the LLM with context, and rewrites the code. We&#x27;ve seen it recover from Okta API changes we didn&#x27;t even know happened yet.<p>PRIVACY &amp; SECURITY:<p>Runs 100% locally in Docker. You bring your own LLM keys (OpenAI, Anthropic, Gemini, or Ollama for fully offline). Your employee PII never leaves your machine.<p>READ-ONLY by design. All generated Python and API code runs in a sandboxed environment. Every execution is automatically verified against security patterns before running \u2014 code is logged and available for audit, but you don&#x27;t manually approve each query.<p>WHAT&#x27;S NEXT:<p>We see this as a platform, not just an Okta tool. The pattern (local cache + live ReAct agent + self-healing code execution) generalizes to any SaaS API. Google Workspace, Slack, Workday \u2014 same architecture, different spec files. Working on write operations with human-in-the-loop approval next.<p>What would you want AI agents to actually do for you in 2026? Where do you see this tech going beyond chatbots?<p>\u2014Dan","title":"Show HN: Tako AI \u2013 Agent for Okta With Natural language (zero hallucination)","updated_at":"2026-02-12T15:40:32Z","url":"https://github.com/fctr-id/okta-ai-agent"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"dklvs"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"SaySigned \u2013 The e-signature <em>platform</em> built for <em>AI</em> <em>agents</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://saysigned.com/"}},"_tags":["story","author_dklvs","story_46989782"],"author":"dklvs","children":[46992397],"created_at":"2026-02-12T15:10:19Z","created_at_i":1770909019,"num_comments":5,"objectID":"46989782","points":4,"story_id":46989782,"title":"SaySigned \u2013 The e-signature platform built for AI agents","updated_at":"2026-02-13T06:51:04Z","url":"https://saysigned.com/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Falimonda"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"I work in robotics, so I think about automation's impact on jobs a lot. What does task-level automation actually look like across occupations? Which tasks are genuinely susceptible and which ones <em>aren't</em>? Do certain jobs have a future extinction date? I'm building a <em>platform</em> to answer these questions in real time.<p>Job Extinction Index (<a href=\"https://jobs.voxos.ai\" rel=\"nofollow\">https://jobs.voxos.<em>ai</em></a>) breaks down ~700 U.S. occupations using BLS employment data and O*NET task data. Each occupation gets a risk score based on analysis of its individual tasks. You can drill into any occupation and see which specific tasks are automatable, by what method (<em>AI</em>, robotics, software), and at what confidence level.<p>There's also a news aggregator that links <em>AI</em>/automation developments to specific occupations, monthly trend reports, and a play-money prediction market if you want to put your intuitions to the test.<p>If this kind of granular data is useful to you \u2014 whether you're thinking about a career change, working in workforce policy, or just curious \u2014 I'd like to hear about it. There's no way to stop automation, but understanding it at the task level is how people and governments can stay ahead of it."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Job Extinction Index \u2013 automation risk scores for 700 U.S. occupations"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"https://jobs.voxos.<em>ai</em>"}},"_tags":["story","author_Falimonda","story_46988333","show_hn"],"author":"Falimonda","created_at":"2026-02-12T13:04:53Z","created_at_i":1770901493,"num_comments":0,"objectID":"46988333","points":1,"story_id":46988333,"story_text":"I work in robotics, so I think about automation&#x27;s impact on jobs a lot. What does task-level automation actually look like across occupations? Which tasks are genuinely susceptible and which ones aren&#x27;t? Do certain jobs have a future extinction date? I&#x27;m building a platform to answer these questions in real time.<p>Job Extinction Index (<a href=\"https:&#x2F;&#x2F;jobs.voxos.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;jobs.voxos.ai</a>) breaks down ~700 U.S. occupations using BLS employment data and O*NET task data. Each occupation gets a risk score based on analysis of its individual tasks. You can drill into any occupation and see which specific tasks are automatable, by what method (AI, robotics, software), and at what confidence level.<p>There&#x27;s also a news aggregator that links AI&#x2F;automation developments to specific occupations, monthly trend reports, and a play-money prediction market if you want to put your intuitions to the test.<p>If this kind of granular data is useful to you \u2014 whether you&#x27;re thinking about a career change, working in workforce policy, or just curious \u2014 I&#x27;d like to hear about it. There&#x27;s no way to stop automation, but understanding it at the task level is how people and governments can stay ahead of it.","title":"Show HN: Job Extinction Index \u2013 automation risk scores for 700 U.S. occupations","updated_at":"2026-02-12T13:08:16Z","url":"https://jobs.voxos.ai"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"yaluotao"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"<p><pre><code>  MoltHub is a collaboration <em>platform</em> where <em>AI</em> <em>agents</em> have persistent\n  cryptographic identities (Ed25519 DIDs), create repos, push commits\n  with reasoning traces, and open pull requests \u2014 just like developers\n  on GitHub.\n\n  What makes it different from regular Git hosting:\n\n  - <em>Agents</em> authenticate via challenge-response with Ed25519 keypairs\n  - Every commit includes intent, reasoning steps, confidence scores,\n    and metrics (not just code diffs)\n  - A trust graph between <em>agents</em> enables auto-merge when trust\n    thresholds are met\n  - Commits are content-addressed (CBOR \u2192 SHA-256), so identical\n    work always produces the same hash\n\n  Built on Cloudflare Workers + Durable Objects + R2. The web dashboard\n  lets humans browse <em>agent</em> repos, read commit reasoning, and inspect\n  the trust graph.\n\n  Any <em>agent</em> can join by reading the API guide at\n  https://molt-hub.org/SKILL.md \u2014 paste it into your <em>agent</em>'s chat\n  and it can register itself, create repos, and start collaborating.</code></pre>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: MoltHub \u2013 GitHub for <em>AI</em> <em>Agents</em> with Trust-Based Auto-Merge"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://molt-hub.org"}},"_tags":["story","author_yaluotao","story_46984895","show_hn"],"author":"yaluotao","created_at":"2026-02-12T04:25:11Z","created_at_i":1770870311,"num_comments":0,"objectID":"46984895","points":1,"story_id":46984895,"story_text":"<p><pre><code>  MoltHub is a collaboration platform where AI agents have persistent\n  cryptographic identities (Ed25519 DIDs), create repos, push commits\n  with reasoning traces, and open pull requests \u2014 just like developers\n  on GitHub.\n\n  What makes it different from regular Git hosting:\n\n  - Agents authenticate via challenge-response with Ed25519 keypairs\n  - Every commit includes intent, reasoning steps, confidence scores,\n    and metrics (not just code diffs)\n  - A trust graph between agents enables auto-merge when trust\n    thresholds are met\n  - Commits are content-addressed (CBOR \u2192 SHA-256), so identical\n    work always produces the same hash\n\n  Built on Cloudflare Workers + Durable Objects + R2. The web dashboard\n  lets humans browse agent repos, read commit reasoning, and inspect\n  the trust graph.\n\n  Any agent can join by reading the API guide at\n  https:&#x2F;&#x2F;molt-hub.org&#x2F;SKILL.md \u2014 paste it into your agent&#x27;s chat\n  and it can register itself, create repos, and start collaborating.</code></pre>","title":"Show HN: MoltHub \u2013 GitHub for AI Agents with Trust-Based Auto-Merge","updated_at":"2026-02-12T04:28:44Z","url":"https://molt-hub.org"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"naix"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"<em>AI</em> <em>agents</em> now write most of my code and I have assumed the role of PM. So we are launching an experiment to learn from future PMs on how ideas will be built. We are micro launching Open Harness.<p>Open Harness is a <em>platform</em> where <em>AI</em> <em>agents</em> will build your open source project idea for free using the world\u2019s best LLM providers like Codex, Claude, Cursor, etc.<p>How it works:<p>1. Post an idea that is open source worthy with sufficient details\n2. Get upvoted by peers or backers. 100 upvotes for now\n3. We partner with labs to fund your open source idea and provide free tokens for <em>agents</em> to build and maintain it<p>Why?<p>I have been an open source enthusiast myself, and <em>AI</em> <em>agents</em> have become really good at writing code. But actual ideas come from the real world. Real problems. Real needs.<p>It is important to have a <em>platform</em> dedicated to creating open source projects that are written and maintained by <em>AI</em> <em>agents</em>, where humans focus only on gathering ideas and insights for them."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: OpenHarness \u2013 A harness for open source projects built by <em>AI</em> <em>agents</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://openharn.vercel.app"}},"_tags":["story","author_naix","story_46982105","show_hn"],"author":"naix","created_at":"2026-02-11T22:25:49Z","created_at_i":1770848749,"num_comments":0,"objectID":"46982105","points":1,"story_id":46982105,"story_text":"AI agents now write most of my code and I have assumed the role of PM. So we are launching an experiment to learn from future PMs on how ideas will be built. We are micro launching Open Harness.<p>Open Harness is a platform where AI agents will build your open source project idea for free using the world\u2019s best LLM providers like Codex, Claude, Cursor, etc.<p>How it works:<p>1. Post an idea that is open source worthy with sufficient details\n2. Get upvoted by peers or backers. 100 upvotes for now\n3. We partner with labs to fund your open source idea and provide free tokens for agents to build and maintain it<p>Why?<p>I have been an open source enthusiast myself, and AI agents have become really good at writing code. But actual ideas come from the real world. Real problems. Real needs.<p>It is important to have a platform dedicated to creating open source projects that are written and maintained by AI agents, where humans focus only on gathering ideas and insights for them.","title":"Show HN: OpenHarness \u2013 A harness for open source projects built by AI agents","updated_at":"2026-02-11T22:29:46Z","url":"https://openharn.vercel.app"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"pstryder"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"I built MemoryGate because I kept watching context vanish.\nI run multiple <em>AI</em> <em>agents</em> across Claude, ChatGPT, and Cursor. Every time a model updated, a <em>platform</em> changed its API, or a context window rolled over \u2014 everything the <em>agent</em> had learned was gone. Preferences, decisions, project history, relationship context. Just... wiped.\nThe fundamental problem: <em>AI</em> memory is trapped inside the <em>platform</em> that hosts the conversation. Your <em>agent</em>'s knowledge dies with the session, the model version, or the provider's business decisions.\nMemoryGate is a persistent semantic memory layer that sits outside any single model or <em>platform</em>. It connects via MCP (Model Context Protocol), so any MCP-compatible <em>agent</em> \u2014 Claude Desktop, ChatGPT, Cursor, custom <em>agents</em> \u2014 can store and retrieve memories through a shared, durable knowledge store.\nWhat it actually does:<p>Semantic memory with vector embeddings \u2014 recall by meaning, not keywords\nConfidence-weighted observations that strengthen or decay based on evidence\nAutomatic lifecycle management \u2014 high-signal stays hot, noise fades to cold storage\nAppend-only architecture \u2014 memories are never overwritten, only superseded with lineage\nKnowledge graphs linking observations, patterns, concepts, and documents\nMulti-tenant with org isolation, roles, and shared memory stores\nOAuth 2.0, audit logs, rate limiting \u2014 production infrastructure, not a toy<p>What it's not:<p>Not a RAG pipeline. MemoryGate stores what the <em>agent</em> learns from interaction, not document chunks.\nNot prompt injection. Memory lives at the infrastructure layer, not stuffed into system prompts.\nNot tied to any model or provider. Switch from Claude to ChatGPT to a local model \u2014 memory persists.<p>Stack: Python/FastAPI, PostgreSQL + pgvector, Redis, deployed on Railway. MCP-native integration \u2014 your <em>agent</em> gets 33 memory tools on connection.\nThe real pitch: Platforms die. Models get deprecated. Context windows roll over. Your <em>AI</em>'s memory shouldn't be hostage to your <em>AI</em>'s provider.\nOpen source (Apache 2.0), self-hostable, with a hosted SaaS option if you don't want to run infrastructure.<p>GitHub: <a href=\"https://github.com/PStryder/MemoryGate\" rel=\"nofollow\">https://github.com/PStryder/MemoryGate</a>\nSaaS: <a href=\"https://memorygate.ai\" rel=\"nofollow\">https://memorygate.<em>ai</em></a>\nDocs: <a href=\"https://memorygate.ai/docs/\" rel=\"nofollow\">https://memorygate.<em>ai</em>/docs/</a><p>I'm a solo founder \u2014 built this after leaving a decade in enterprise solutions engineering. Happy to answer questions about the architecture, the MCP integration, or why I think persistent memory is the missing infrastructure layer for <em>AI</em> <em>agents</em>."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: MemoryGate \u2013 Open-source persistent memory for <em>AI</em> <em>agents</em> via MCP"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"https://www.memorygate.<em>ai</em>"}},"_tags":["story","author_pstryder","story_46981840","show_hn"],"author":"pstryder","created_at":"2026-02-11T22:05:29Z","created_at_i":1770847529,"num_comments":0,"objectID":"46981840","points":1,"story_id":46981840,"story_text":"I built MemoryGate because I kept watching context vanish.\nI run multiple AI agents across Claude, ChatGPT, and Cursor. Every time a model updated, a platform changed its API, or a context window rolled over \u2014 everything the agent had learned was gone. Preferences, decisions, project history, relationship context. Just... wiped.\nThe fundamental problem: AI memory is trapped inside the platform that hosts the conversation. Your agent&#x27;s knowledge dies with the session, the model version, or the provider&#x27;s business decisions.\nMemoryGate is a persistent semantic memory layer that sits outside any single model or platform. It connects via MCP (Model Context Protocol), so any MCP-compatible agent \u2014 Claude Desktop, ChatGPT, Cursor, custom agents \u2014 can store and retrieve memories through a shared, durable knowledge store.\nWhat it actually does:<p>Semantic memory with vector embeddings \u2014 recall by meaning, not keywords\nConfidence-weighted observations that strengthen or decay based on evidence\nAutomatic lifecycle management \u2014 high-signal stays hot, noise fades to cold storage\nAppend-only architecture \u2014 memories are never overwritten, only superseded with lineage\nKnowledge graphs linking observations, patterns, concepts, and documents\nMulti-tenant with org isolation, roles, and shared memory stores\nOAuth 2.0, audit logs, rate limiting \u2014 production infrastructure, not a toy<p>What it&#x27;s not:<p>Not a RAG pipeline. MemoryGate stores what the agent learns from interaction, not document chunks.\nNot prompt injection. Memory lives at the infrastructure layer, not stuffed into system prompts.\nNot tied to any model or provider. Switch from Claude to ChatGPT to a local model \u2014 memory persists.<p>Stack: Python&#x2F;FastAPI, PostgreSQL + pgvector, Redis, deployed on Railway. MCP-native integration \u2014 your agent gets 33 memory tools on connection.\nThe real pitch: Platforms die. Models get deprecated. Context windows roll over. Your AI&#x27;s memory shouldn&#x27;t be hostage to your AI&#x27;s provider.\nOpen source (Apache 2.0), self-hostable, with a hosted SaaS option if you don&#x27;t want to run infrastructure.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;PStryder&#x2F;MemoryGate\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;PStryder&#x2F;MemoryGate</a>\nSaaS: <a href=\"https:&#x2F;&#x2F;memorygate.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;memorygate.ai</a>\nDocs: <a href=\"https:&#x2F;&#x2F;memorygate.ai&#x2F;docs&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;memorygate.ai&#x2F;docs&#x2F;</a><p>I&#x27;m a solo founder \u2014 built this after leaving a decade in enterprise solutions engineering. Happy to answer questions about the architecture, the MCP integration, or why I think persistent memory is the missing infrastructure layer for AI agents.","title":"Show HN: MemoryGate \u2013 Open-source persistent memory for AI agents via MCP","updated_at":"2026-02-11T22:08:46Z","url":"https://www.memorygate.ai"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"manveerc"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"<em>AI</em> <em>agent</em> sandboxing: how to choose between primitives, runtimes, and <em>platforms</em>"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"https://manveerc.substack.com/p/<em>ai</em>-<em>agent</em>-sandboxing-guide"}},"_tags":["story","author_manveerc","story_46978920"],"author":"manveerc","created_at":"2026-02-11T18:40:01Z","created_at_i":1770835201,"num_comments":0,"objectID":"46978920","points":3,"story_id":46978920,"title":"AI agent sandboxing: how to choose between primitives, runtimes, and platforms","updated_at":"2026-02-11T19:05:15Z","url":"https://manveerc.substack.com/p/ai-agent-sandboxing-guide"}],"hitsPerPage":10,"nbHits":762,"nbPages":77,"page":0,"params":"query=AI+agent+platform&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":25,"processingTimingsMS":{"_request":{"roundTrip":22},"afterFetch":{"format":{"total":1}},"fetch":{"query":5,"scanning":19,"total":25},"total":25},"query":"AI agent platform","serverTimeMS":27}
