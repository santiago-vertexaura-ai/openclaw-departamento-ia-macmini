{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"alonsovm"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Hi HN,\nI've been a developer for some time now, and like many of you, I've been frustrated by the &quot;All-or-Nothing&quot; problem with AI coding tools.<p>You ask an AI to fix a bug or implement a <em>function</em>, and it rewrites the whole file. It changes your imports, renames your variables, or deletes comments it deems unnecessary. It\u2019s like giving a junior developer (like me) root access to your production server just to change a config file.<p>So, 29 days ago, I started building Yori to solve the trust problem.<p>The Concept: Semantic Containers\nYori introduces a syntax that acts like a firewall for AI. You define a $${ ... }$$ block inside a text file.<p>Outside the block (The Host): Your manual code, architecture, and structure. The AI cannot touch this.\nInside the block (The Container): You write natural language intent. The AI can only generate code here.<p>Example: myutils.md<p>```cpp\nEXPORT: &quot;myfile.cpp&quot;<p>// My manual architecture - AI cannot change this\n#include &quot;utils.h&quot;<p>void process_data() {\n    // Container: The AI is sandboxed here, but inherits the rest of the file as context\n    $${\n      Sort the incoming data vector using quicksort.\n      Filter out negative numbers.\n      Print the result.\n    }$$ \n}\nEXPORT: END\n```\nHow it works:\nYori is a C++ wrapper that parses these files. Whatever is within the EXPORT block and outside the containers ($${ }$$) will be copied as it is. When you run `yori myutils.md -make -series`, it sends the prompts to a local (Ollama) or cloud <em>LLM</em>, generates the syntax, fills the blocks, and compiles the result using your native toolchain (GCC/Clang/Python).<p>If compilation fails, it feeds the error back to the <em>LLM</em> in a retry loop (self-healing).<p>Why I think this matters:<p>1. Safety: You stop giving AI &quot;root access&quot; to your files.<p>2. Intent as Source: The prompt stays in the file. If you want to port your logic from C++ to Rust, you keep the prompts and just change the compile target.<p>3. Incremental Builds (to be added soon): Named containers allow for <em>caching</em>. If the prompt hasn't changed, you don't pay for an API call.<p>It\u2019s open source (MIT), C++17, and works locally.<p>I\u2019d love feedback on the &quot;Semantic Container&quot; concept. Is this the abstraction layer we've been missing for AI coding? Let me hear your ideas. Also, if you can't run yori.exe tell what went wrong and we see how to fix it. I opened a github issue on this. I am also working in making documentation for the project (github wiki). So expect that soon.<p>GitHub: https://github.com/alonsovm44/yori<p>Thanks!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Yori \u2013 Isolating AI Logic into \"Semantic Containers\" (Docker for Code)"}},"_tags":["story","author_alonsovm","story_46998870","show_hn"],"author":"alonsovm","created_at":"2026-02-13T04:17:52Z","created_at_i":1770956272,"num_comments":0,"objectID":"46998870","points":3,"story_id":46998870,"story_text":"Hi HN,\nI&#x27;ve been a developer for some time now, and like many of you, I&#x27;ve been frustrated by the &quot;All-or-Nothing&quot; problem with AI coding tools.<p>You ask an AI to fix a bug or implement a function, and it rewrites the whole file. It changes your imports, renames your variables, or deletes comments it deems unnecessary. It\u2019s like giving a junior developer (like me) root access to your production server just to change a config file.<p>So, 29 days ago, I started building Yori to solve the trust problem.<p>The Concept: Semantic Containers\nYori introduces a syntax that acts like a firewall for AI. You define a $${ ... }$$ block inside a text file.<p>Outside the block (The Host): Your manual code, architecture, and structure. The AI cannot touch this.\nInside the block (The Container): You write natural language intent. The AI can only generate code here.<p>Example: myutils.md<p>```cpp\nEXPORT: &quot;myfile.cpp&quot;<p>&#x2F;&#x2F; My manual architecture - AI cannot change this\n#include &quot;utils.h&quot;<p>void process_data() {\n    &#x2F;&#x2F; Container: The AI is sandboxed here, but inherits the rest of the file as context\n    $${\n      Sort the incoming data vector using quicksort.\n      Filter out negative numbers.\n      Print the result.\n    }$$ \n}\nEXPORT: END\n```\nHow it works:\nYori is a C++ wrapper that parses these files. Whatever is within the EXPORT block and outside the containers ($${ }$$) will be copied as it is. When you run `yori myutils.md -make -series`, it sends the prompts to a local (Ollama) or cloud LLM, generates the syntax, fills the blocks, and compiles the result using your native toolchain (GCC&#x2F;Clang&#x2F;Python).<p>If compilation fails, it feeds the error back to the LLM in a retry loop (self-healing).<p>Why I think this matters:<p>1. Safety: You stop giving AI &quot;root access&quot; to your files.<p>2. Intent as Source: The prompt stays in the file. If you want to port your logic from C++ to Rust, you keep the prompts and just change the compile target.<p>3. Incremental Builds (to be added soon): Named containers allow for caching. If the prompt hasn&#x27;t changed, you don&#x27;t pay for an API call.<p>It\u2019s open source (MIT), C++17, and works locally.<p>I\u2019d love feedback on the &quot;Semantic Container&quot; concept. Is this the abstraction layer we&#x27;ve been missing for AI coding? Let me hear your ideas. Also, if you can&#x27;t run yori.exe tell what went wrong and we see how to fix it. I opened a github issue on this. I am also working in making documentation for the project (github wiki). So expect that soon.<p>GitHub: https:&#x2F;&#x2F;github.com&#x2F;alonsovm44&#x2F;yori<p>Thanks!","title":"Show HN: Yori \u2013 Isolating AI Logic into \"Semantic Containers\" (Docker for Code)","updated_at":"2026-02-13T05:31:03Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"smart_coconut"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"We implemented a small INT8 CNN for handwritten digit classification (NIST SD19 subset) in pure VHDL and built it on two different FPGA families: Intel Agilex 3 and Lattice Certus-NX.<p>The design was originally targeting Agilex 3. We later rebuilt it for a Certus-NX board to see how portable the RTL actually was and what changed in terms of resource usage and timing.<p>## Model<p>Input: 128\u00d7128 grayscale images streamed over UART from a host PC webcam.<p>Architecture:\n- 3\u00d73 conv (8 filters) + pooling\n- 3\u00d73 conv (12 filters) + pooling\n- 3\u00d73 conv (16 filters) + pooling\n- 3\u00d73 conv (24 filters) + pooling\n- 3\u00d73 conv (32 filters) + pooling\n- fully connected 512 \u2192 10<p>All arithmetic is INT8.  \nThe design is single-clock and streaming; feature maps are buffered in block RAM between layers.<p>---<p>## Platform 1: Intel Agilex 3<p>Device: A3CY135BM16AE6S  \nBoard: Agilex 3 C-Series Development Kit  \nToolchain: Quartus Prime Pro 25.3<p>Resource usage:\n- ALMs: 2,526 / 45,800 (6%)\n- RAM blocks: 36 / 353 (10%)\n- DSP blocks: 17 / 184 (9%)<p>Fmax: 146 MHz<p>---<p>## Platform 2: Lattice Certus-NX<p>Device: LDN2NX-40-7BG196I  \nBoard: Cruvi CR00103-03  \nToolchain: Radiant 2025.2<p>Resource usage:\n- LUT4: 13,757 / 32,256 (42.6%)\n- MULT9: 66 / 112 (59%)\n- MULT18: 12 / 56 (21%)\n- Block RAM: 20 / 84 (24%)<p>Single clock domain: 273 MHz<p>The Certus design uses an on-chip PLL (12 \u2192 48 MHz) input adaptation for the board.  \nThe Agilex board required an external UART adapter; the Certus board had it integrated.<p>---<p>## Porting effort<p>The RTL is written in plain VHDL without vendor IP. No vendor-specific primitives are instantiated in the CNN datapath.<p>In practice:<p>- No DSP or RAM wrapper layer was required.\n- No changes to arithmetic or pipeline structure were necessary.\n- No timing constraint rework beyond board-specific clock definitions.\n- Only board-level adaptations (clocking, UART wiring).<p>The vendor itself was largely irrelevant for this design. The differences were at the board and toolchain level.<p>---<p>## Observations<p>- On Agilex 3, the design is small relative to the device (single-digit % utilization).\n- On Certus-NX-40, the same design consumes a significant <em>fraction</em> of LUTs and MULT9 blocks.\n- Achieved Fmax on Certus-NX is higher in this configuration (273 MHz vs 146 MHz), though the system clocking and board setup differ.<p>The DSP usage profile differs noticeably: Certus-NX\u2019s MULT9 blocks are heavily used (59%), which constrains <em>scaling</em> the number of parallel MAC units more quickly than on Agilex 3.<p>For this size of INT8 CNN, portability at the RTL level was straightforward. The <em>lim</em>iting factor when moving to the smaller device was resource headroom rather than functional incompatibility.<p>---<p>## Question<p>For those who have moved similar streaming CNN datapaths across vendors:\nHave you found cases where DSP inference or block RAM inference diverged enough to require structural RTL changes, or does that mostly appear once designs become more deeply pipelined or multi-clock?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Porting an INT8 VHDL CNN from Intel Agilex 3 to Lattice Certus-NX"}},"_tags":["story","author_smart_coconut","story_46988464","ask_hn"],"author":"smart_coconut","created_at":"2026-02-12T13:16:15Z","created_at_i":1770902175,"num_comments":0,"objectID":"46988464","points":1,"story_id":46988464,"story_text":"We implemented a small INT8 CNN for handwritten digit classification (NIST SD19 subset) in pure VHDL and built it on two different FPGA families: Intel Agilex 3 and Lattice Certus-NX.<p>The design was originally targeting Agilex 3. We later rebuilt it for a Certus-NX board to see how portable the RTL actually was and what changed in terms of resource usage and timing.<p>## Model<p>Input: 128\u00d7128 grayscale images streamed over UART from a host PC webcam.<p>Architecture:\n- 3\u00d73 conv (8 filters) + pooling\n- 3\u00d73 conv (12 filters) + pooling\n- 3\u00d73 conv (16 filters) + pooling\n- 3\u00d73 conv (24 filters) + pooling\n- 3\u00d73 conv (32 filters) + pooling\n- fully connected 512 \u2192 10<p>All arithmetic is INT8.  \nThe design is single-clock and streaming; feature maps are buffered in block RAM between layers.<p>---<p>## Platform 1: Intel Agilex 3<p>Device: A3CY135BM16AE6S  \nBoard: Agilex 3 C-Series Development Kit  \nToolchain: Quartus Prime Pro 25.3<p>Resource usage:\n- ALMs: 2,526 &#x2F; 45,800 (6%)\n- RAM blocks: 36 &#x2F; 353 (10%)\n- DSP blocks: 17 &#x2F; 184 (9%)<p>Fmax: 146 MHz<p>---<p>## Platform 2: Lattice Certus-NX<p>Device: LDN2NX-40-7BG196I  \nBoard: Cruvi CR00103-03  \nToolchain: Radiant 2025.2<p>Resource usage:\n- LUT4: 13,757 &#x2F; 32,256 (42.6%)\n- MULT9: 66 &#x2F; 112 (59%)\n- MULT18: 12 &#x2F; 56 (21%)\n- Block RAM: 20 &#x2F; 84 (24%)<p>Single clock domain: 273 MHz<p>The Certus design uses an on-chip PLL (12 \u2192 48 MHz) input adaptation for the board.  \nThe Agilex board required an external UART adapter; the Certus board had it integrated.<p>---<p>## Porting effort<p>The RTL is written in plain VHDL without vendor IP. No vendor-specific primitives are instantiated in the CNN datapath.<p>In practice:<p>- No DSP or RAM wrapper layer was required.\n- No changes to arithmetic or pipeline structure were necessary.\n- No timing constraint rework beyond board-specific clock definitions.\n- Only board-level adaptations (clocking, UART wiring).<p>The vendor itself was largely irrelevant for this design. The differences were at the board and toolchain level.<p>---<p>## Observations<p>- On Agilex 3, the design is small relative to the device (single-digit % utilization).\n- On Certus-NX-40, the same design consumes a significant fraction of LUTs and MULT9 blocks.\n- Achieved Fmax on Certus-NX is higher in this configuration (273 MHz vs 146 MHz), though the system clocking and board setup differ.<p>The DSP usage profile differs noticeably: Certus-NX\u2019s MULT9 blocks are heavily used (59%), which constrains scaling the number of parallel MAC units more quickly than on Agilex 3.<p>For this size of INT8 CNN, portability at the RTL level was straightforward. The limiting factor when moving to the smaller device was resource headroom rather than functional incompatibility.<p>---<p>## Question<p>For those who have moved similar streaming CNN datapaths across vendors:\nHave you found cases where DSP inference or block RAM inference diverged enough to require structural RTL changes, or does that mostly appear once designs become more deeply pipelined or multi-clock?","title":"Porting an INT8 VHDL CNN from Intel Agilex 3 to Lattice Certus-NX","updated_at":"2026-02-12T13:18:47Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"h4ch1"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Hi HN,<p>This is a new tab page I started building for myself close to 4 months ago, it started with a simple markdown editor with task extraction support and I kept expanding it depending on what I wanted.<p>I was sick of the current state of new tab pages, either they were very minimal (good morning, some quotes, etc) or were data collection agents.<p>I was also not really keen on downloading and using an electron note-taking app since my browser's already open and <em>falling</em> into configuration hell was a path I had taking numerous times.<p>There are quite a few features that people may find useful:<p>1. Timing <em>functions</em>: A clock and world clock with keyboard driven conversion - I was tired of Googling times in multiple timezones trying to co-ordinate between devs and clients.<p>2. Ambient Chaos: Upload your ambient sounds, use it as a central mixer - I usually listen to ambient music, nature sounds, fireplaces, etc while working. Having a youtube tab consume 400+MB while doing that wasn't very fun.<p>3. QuickLog: Not all thoughts need their dedicated note; a quick kb driven logging widget which rotates at the end of each day. Promotable to your main notes.<p>4. Notes &amp; Tasks: A CodeMirror based Markdown editor with nested task extraction to a separate widget - parses all your notes, extracts all tasks and gives a tree view of tasks across files or per-file.<p>5. Tab Debt: A nested tab widget which tracks what links were opened from where and how long they've been open. You can select and close/save them.<p>6. Weather: Just a detailed weather widget with forecasts and a <em>LLM</em> summary (if you add a key)<p>7. Discussions and Bookmark Chat - You can chat with HackerNews and Reddit discussions &amp; articles with the <em>LLM</em> of your choice (BYOK); supports OpenAI, Anthropic, Cerebras. It also has the ability to sync and parse your bookmarks with scraping support.<p>A little more on 7:<p>Sometimes when I use reddit to find product alternatives, the sheer volume of suggestions, often repeated with different usecases makes the process harder. The discussions chat supports proper backlinking to comments in the thread.\nWith bookmarks, I kept losing really cool stuff I had saved and never knew the gems I had just collecting dust. A quick &quot;find me bookmarks on 3d postprocessing&quot; really helps now.<p>It's up on the Chrome Web Store: <a href=\"https://chromewebstore.google.com/detail/pgfpnakgiejllklfaamjogeoamalobfp?utm_source=item-share-cb\" rel=\"nofollow\">https://chromewebstore.google.com/detail/pgfpnakgiejllklfaam...</a><p>The Firefox version is a little buggy so the publishing is delayed there but if you'd like to test it out you can download it here: <a href=\"https://deepflowdata.tech/commander-0.1.0-firefox.zip\" rel=\"nofollow\">https://deepflowdata.tech/commander-0.1.0-firefox.zip</a><p>Demo: <a href=\"https://www.youtube.com/watch?v=EAqbmOT_6wk\" rel=\"nofollow\">https://www.youtube.com/watch?v=EAqbmOT_6wk</a><p>This extension is something I personally use, doesn't collect any data and doesn't interact with the internet unless you use the <em>LLM</em> chat with your own key. Planning on making this OSS software after cleaning up the mess that the codebase is basically over 3-4 months of treating it like a sandbox.<p>Open to hearing feedback, critique and anything in between :) Have a great day/night!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Commander, an opinionated yet powerful new tab page"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://chromewebstore.google.com/detail/commander/pgfpnakgiejllklfaamjogeoamalobfp"}},"_tags":["story","author_h4ch1","story_46987439","show_hn"],"author":"h4ch1","created_at":"2026-02-12T11:27:22Z","created_at_i":1770895642,"num_comments":0,"objectID":"46987439","points":1,"story_id":46987439,"story_text":"Hi HN,<p>This is a new tab page I started building for myself close to 4 months ago, it started with a simple markdown editor with task extraction support and I kept expanding it depending on what I wanted.<p>I was sick of the current state of new tab pages, either they were very minimal (good morning, some quotes, etc) or were data collection agents.<p>I was also not really keen on downloading and using an electron note-taking app since my browser&#x27;s already open and falling into configuration hell was a path I had taking numerous times.<p>There are quite a few features that people may find useful:<p>1. Timing functions: A clock and world clock with keyboard driven conversion - I was tired of Googling times in multiple timezones trying to co-ordinate between devs and clients.<p>2. Ambient Chaos: Upload your ambient sounds, use it as a central mixer - I usually listen to ambient music, nature sounds, fireplaces, etc while working. Having a youtube tab consume 400+MB while doing that wasn&#x27;t very fun.<p>3. QuickLog: Not all thoughts need their dedicated note; a quick kb driven logging widget which rotates at the end of each day. Promotable to your main notes.<p>4. Notes &amp; Tasks: A CodeMirror based Markdown editor with nested task extraction to a separate widget - parses all your notes, extracts all tasks and gives a tree view of tasks across files or per-file.<p>5. Tab Debt: A nested tab widget which tracks what links were opened from where and how long they&#x27;ve been open. You can select and close&#x2F;save them.<p>6. Weather: Just a detailed weather widget with forecasts and a LLM summary (if you add a key)<p>7. Discussions and Bookmark Chat - You can chat with HackerNews and Reddit discussions &amp; articles with the LLM of your choice (BYOK); supports OpenAI, Anthropic, Cerebras. It also has the ability to sync and parse your bookmarks with scraping support.<p>A little more on 7:<p>Sometimes when I use reddit to find product alternatives, the sheer volume of suggestions, often repeated with different usecases makes the process harder. The discussions chat supports proper backlinking to comments in the thread.\nWith bookmarks, I kept losing really cool stuff I had saved and never knew the gems I had just collecting dust. A quick &quot;find me bookmarks on 3d postprocessing&quot; really helps now.<p>It&#x27;s up on the Chrome Web Store: <a href=\"https:&#x2F;&#x2F;chromewebstore.google.com&#x2F;detail&#x2F;pgfpnakgiejllklfaamjogeoamalobfp?utm_source=item-share-cb\" rel=\"nofollow\">https:&#x2F;&#x2F;chromewebstore.google.com&#x2F;detail&#x2F;pgfpnakgiejllklfaam...</a><p>The Firefox version is a little buggy so the publishing is delayed there but if you&#x27;d like to test it out you can download it here: <a href=\"https:&#x2F;&#x2F;deepflowdata.tech&#x2F;commander-0.1.0-firefox.zip\" rel=\"nofollow\">https:&#x2F;&#x2F;deepflowdata.tech&#x2F;commander-0.1.0-firefox.zip</a><p>Demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=EAqbmOT_6wk\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=EAqbmOT_6wk</a><p>This extension is something I personally use, doesn&#x27;t collect any data and doesn&#x27;t interact with the internet unless you use the LLM chat with your own key. Planning on making this OSS software after cleaning up the mess that the codebase is basically over 3-4 months of treating it like a sandbox.<p>Open to hearing feedback, critique and anything in between :) Have a great day&#x2F;night!","title":"Show HN: Commander, an opinionated yet powerful new tab page","updated_at":"2026-02-12T11:29:45Z","url":"https://chromewebstore.google.com/detail/commander/pgfpnakgiejllklfaamjogeoamalobfp"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"jared_stewart"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"I've been building a tool that changes how <em>LLM</em> coding agents explore codebases, and I wanted to share it along with some early observations.<p>Typically claude code globs directories, greps for patterns, and reads files with minimal guidance. It works in kind of the same way you'd learn to navigate a city by <em>walking</em> every street. You'll eventually build a mental map, but claude never does - at least not any that persists across different contexts.<p>The Recursive Language Models paper from Zhang, Kraska, and Khattab at MIT CSAIL introduced a cleaner framing. Instead of cramming everything into context, the model gets a searchable environment. The model can then query just for what it needs and can drill deeper where needed.<p>coderlm is my implementation of that idea for codebases. A Rust server indexes a project with tree-sitter, builds a symbol table with cross-references, and exposes an API. The agent queries for structure, symbols, implementations, callers, and grep results \u2014 getting back exactly the code it needs instead of scanning for it.<p>The agent workflow looks like:<p>1. `init` \u2014 register the project, get the top-level structure<p>2. `structure` \u2014 drill into specific directories<p>3. `search` \u2014 find symbols by name across the codebase<p>4. `impl` \u2014 retrieve the exact source of a <em>function</em> or class<p>5. `callers` \u2014 find everything that calls a given symbol<p>6. `grep` \u2014 fall back to text search when you need it<p>This replaces the glob/grep/read cycle with index-backed lookups. The server currently supports Rust, Python, TypeScript, JavaScript, and Go for symbol parsing, though all file types show up in the tree and are searchable via grep.<p>It ships as a Claude Code plugin with hooks that guide the agent to use indexed lookups instead of native file tools, plus a Python CLI wrapper with zero dependencies.<p>For anecdotal results, I ran the same prompt against a codebase to &quot;explore and identify opportunities to clarify the existing structure&quot;.<p>Using coderlm, claude was able to generate a plan in about 3 minutes. The coderlm enabled instance found a genuine bug (duplicated code with identical names), orphaned code for cleanup, mismatched naming conventions crossing module boundaries, and overlapping vocabulary. These are all <i>semantic</i> issues which clearly benefit from the tree-sitter centric approach.<p>Using the native tools, claude was able to identify various file clutter in the root of the project, out of date references, and a migration timestamp collision. These findings are more consistent with methodical walks of the filesystem and took about 8 minutes to produce.<p>The indexed approach did better at catching semantic issues than native tools and had a key benefit in being faster to resolve.<p>I've spent some effort to streamline the installation process, but it isn't turnkey yet. You'll need the rust toolchain to build the server which runs as a separate process. Installing the plugin from a claude marketplace is possible, but the skill isn't being added to your .claude yet so there are some manual steps to just getting to a point where claude could use it.<p>Claude continues to demonstrate significant resistance to using CodeRLM in exploration tasks. Typically to use you will need to explicitly direct claude to use it.<p>---<p>Repo: github.com/JaredStewart/coderlm<p>Paper: Recursive Language Models <a href=\"https://arxiv.org/abs/2512.24601\" rel=\"nofollow\">https://arxiv.org/abs/2512.24601</a> \u2014 Zhang, Kraska, Khattab (MIT CSAIL, 2025)<p>Inspired by: <a href=\"https://github.com/brainqub3/claude_code_RLM\" rel=\"nofollow\">https://github.com/brainqub3/claude_code_RLM</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: CodeRLM \u2013 Tree-sitter-backed code indexing for <em>LLM</em> agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/JaredStewart/coderlm/blob/main/server/REPL_to_API.md"}},"_tags":["story","author_jared_stewart","story_46974515","show_hn"],"author":"jared_stewart","children":[46984648,46987209,46984598,46985245,46986305,46984339,46984212,46983891,46983766],"created_at":"2026-02-11T13:10:23Z","created_at_i":1770815423,"num_comments":34,"objectID":"46974515","points":79,"story_id":46974515,"story_text":"I&#x27;ve been building a tool that changes how LLM coding agents explore codebases, and I wanted to share it along with some early observations.<p>Typically claude code globs directories, greps for patterns, and reads files with minimal guidance. It works in kind of the same way you&#x27;d learn to navigate a city by walking every street. You&#x27;ll eventually build a mental map, but claude never does - at least not any that persists across different contexts.<p>The Recursive Language Models paper from Zhang, Kraska, and Khattab at MIT CSAIL introduced a cleaner framing. Instead of cramming everything into context, the model gets a searchable environment. The model can then query just for what it needs and can drill deeper where needed.<p>coderlm is my implementation of that idea for codebases. A Rust server indexes a project with tree-sitter, builds a symbol table with cross-references, and exposes an API. The agent queries for structure, symbols, implementations, callers, and grep results \u2014 getting back exactly the code it needs instead of scanning for it.<p>The agent workflow looks like:<p>1. `init` \u2014 register the project, get the top-level structure<p>2. `structure` \u2014 drill into specific directories<p>3. `search` \u2014 find symbols by name across the codebase<p>4. `impl` \u2014 retrieve the exact source of a function or class<p>5. `callers` \u2014 find everything that calls a given symbol<p>6. `grep` \u2014 fall back to text search when you need it<p>This replaces the glob&#x2F;grep&#x2F;read cycle with index-backed lookups. The server currently supports Rust, Python, TypeScript, JavaScript, and Go for symbol parsing, though all file types show up in the tree and are searchable via grep.<p>It ships as a Claude Code plugin with hooks that guide the agent to use indexed lookups instead of native file tools, plus a Python CLI wrapper with zero dependencies.<p>For anecdotal results, I ran the same prompt against a codebase to &quot;explore and identify opportunities to clarify the existing structure&quot;.<p>Using coderlm, claude was able to generate a plan in about 3 minutes. The coderlm enabled instance found a genuine bug (duplicated code with identical names), orphaned code for cleanup, mismatched naming conventions crossing module boundaries, and overlapping vocabulary. These are all <i>semantic</i> issues which clearly benefit from the tree-sitter centric approach.<p>Using the native tools, claude was able to identify various file clutter in the root of the project, out of date references, and a migration timestamp collision. These findings are more consistent with methodical walks of the filesystem and took about 8 minutes to produce.<p>The indexed approach did better at catching semantic issues than native tools and had a key benefit in being faster to resolve.<p>I&#x27;ve spent some effort to streamline the installation process, but it isn&#x27;t turnkey yet. You&#x27;ll need the rust toolchain to build the server which runs as a separate process. Installing the plugin from a claude marketplace is possible, but the skill isn&#x27;t being added to your .claude yet so there are some manual steps to just getting to a point where claude could use it.<p>Claude continues to demonstrate significant resistance to using CodeRLM in exploration tasks. Typically to use you will need to explicitly direct claude to use it.<p>---<p>Repo: github.com&#x2F;JaredStewart&#x2F;coderlm<p>Paper: Recursive Language Models <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2512.24601\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2512.24601</a> \u2014 Zhang, Kraska, Khattab (MIT CSAIL, 2025)<p>Inspired by: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;brainqub3&#x2F;claude_code_RLM\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;brainqub3&#x2F;claude_code_RLM</a>","title":"Show HN: CodeRLM \u2013 Tree-sitter-backed code indexing for LLM agents","updated_at":"2026-02-13T11:25:04Z","url":"https://github.com/JaredStewart/coderlm/blob/main/server/REPL_to_API.md"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"simranmultani"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Hey HN,<p>I've been building <em>LLM</em>-based agents for a while and two things kept biting me.<p>1. Loops \u2014 an agent node would get stuck <em>calling</em> the same thing over and over, and I wouldn't notice until the API bill showed up. Lost $200+ on one run.\n2. <em>LLM</em> would return garbage that didn't match what downstream code expected, and everything would just crash.<p>I looked around and couldn't find something simple that handled both. Most frameworks assume your node <em>function</em> just works. In practice it doesn't \u2014 <em>LLM</em> calls fail, JSON comes back broken, state gets weird.<p>So I built AgentCircuit. It's a Python decorator that wraps your agent <em>functions</em> with circuit breaker-style protections:<p><pre><code>    from agentcircuit import reliable\n    from pydantic import BaseModel\n\n    class Output(BaseModel):\n        name: str\n        age: int\n\n    @reliable(sentinel_schema=Output)\n    def extract_data(state):\n        return call_<em>llm</em>(state[&quot;text&quot;])\n</code></pre>\nThat's it. Under the hood it:<p>- Fuse \u2014 detects when a node keeps seeing the same input and kills the loop\n- Sentinel \u2014 validates every output against a Pydantic schema\n- Medic \u2014 auto-repairs bad outputs using an <em>LLM</em>\n- Budget \u2014 per-node and global dollar/time limits so you never get a surprise bill\n- Pricing \u2014 built-in cost tracking for 40+ models (GPT-5, Claude 4.x, Gemini 3, Llama, etc.)<p>There's no server, no config files, no framework lock-in. It works at the <em>function</em> boundary so it composes with LangGraph, LangChain, CrewAI, AutoGen, or just plain <em>functions</em>.<p>GitHub: <a href=\"https://github.com/simranmultani197/AgentCircuit\" rel=\"nofollow\">https://github.com/simranmultani197/AgentCircuit</a>\nPyPI: <a href=\"https://pypi.org/project/agentcircuit/\" rel=\"nofollow\">https://pypi.org/project/agentcircuit/</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function"],"value":"Show HN: AgentCircuit \u2013 Circuit breaker for AI agent <em>functions</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/simranmultani197/AgentCircuit"}},"_tags":["story","author_simranmultani","story_46899775","show_hn"],"author":"simranmultani","children":[46947900],"created_at":"2026-02-05T14:07:27Z","created_at_i":1770300447,"num_comments":1,"objectID":"46899775","points":1,"story_id":46899775,"story_text":"Hey HN,<p>I&#x27;ve been building LLM-based agents for a while and two things kept biting me.<p>1. Loops \u2014 an agent node would get stuck calling the same thing over and over, and I wouldn&#x27;t notice until the API bill showed up. Lost $200+ on one run.\n2. LLM would return garbage that didn&#x27;t match what downstream code expected, and everything would just crash.<p>I looked around and couldn&#x27;t find something simple that handled both. Most frameworks assume your node function just works. In practice it doesn&#x27;t \u2014 LLM calls fail, JSON comes back broken, state gets weird.<p>So I built AgentCircuit. It&#x27;s a Python decorator that wraps your agent functions with circuit breaker-style protections:<p><pre><code>    from agentcircuit import reliable\n    from pydantic import BaseModel\n\n    class Output(BaseModel):\n        name: str\n        age: int\n\n    @reliable(sentinel_schema=Output)\n    def extract_data(state):\n        return call_llm(state[&quot;text&quot;])\n</code></pre>\nThat&#x27;s it. Under the hood it:<p>- Fuse \u2014 detects when a node keeps seeing the same input and kills the loop\n- Sentinel \u2014 validates every output against a Pydantic schema\n- Medic \u2014 auto-repairs bad outputs using an LLM\n- Budget \u2014 per-node and global dollar&#x2F;time limits so you never get a surprise bill\n- Pricing \u2014 built-in cost tracking for 40+ models (GPT-5, Claude 4.x, Gemini 3, Llama, etc.)<p>There&#x27;s no server, no config files, no framework lock-in. It works at the function boundary so it composes with LangGraph, LangChain, CrewAI, AutoGen, or just plain functions.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;simranmultani197&#x2F;AgentCircuit\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;simranmultani197&#x2F;AgentCircuit</a>\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;agentcircuit&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;agentcircuit&#x2F;</a>","title":"Show HN: AgentCircuit \u2013 Circuit breaker for AI agent functions","updated_at":"2026-02-10T11:38:37Z","url":"https://github.com/simranmultani197/AgentCircuit"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Hannah203"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Over the past year, many SaaS products have added AI chatbots to answer questions and reduce support load. It helped initially, but most of these systems still live in a chat window with no awareness of what\u2019s happening inside the application.\nThey don\u2019t know the current page, selected data, user permissions, or workflow state. Users end up repeating context the product already has.\nI recently came across an open-source Copilot SDK that approaches this differently by injecting live application state directly into the AI and letting it execute real frontend and backend <em>functions</em> instead of just responding with text.\nWhat it does:\n- Understands application state including current page, selected data, and user permissions\n- Executes backend and frontend <em>functions</em> instead of only responding with text\n- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons\n- Understands user workflow and intent based on in-product context\n- Maintains session context so interactions remain consistent\nExample: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.\nTechnical details:\n- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)\n- <em>LLM</em>-agnostic (bring your own model)\n- State injection via context providers\n- Tool execution layer for safe <em>function</em> <em>calling</em>\n- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https://copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https://copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: YourGPT Copilot SDK Open-source SDK for product-level intelligence"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://copilot-sdk.yourgpt.ai/docs"}},"_tags":["story","author_Hannah203","story_46885205","show_hn"],"author":"Hannah203","created_at":"2026-02-04T12:47:39Z","created_at_i":1770209259,"num_comments":0,"objectID":"46885205","points":2,"story_id":46885205,"story_text":"Over the past year, many SaaS products have added AI chatbots to answer questions and reduce support load. It helped initially, but most of these systems still live in a chat window with no awareness of what\u2019s happening inside the application.\nThey don\u2019t know the current page, selected data, user permissions, or workflow state. Users end up repeating context the product already has.\nI recently came across an open-source Copilot SDK that approaches this differently by injecting live application state directly into the AI and letting it execute real frontend and backend functions instead of just responding with text.\nWhat it does:\n- Understands application state including current page, selected data, and user permissions\n- Executes backend and frontend functions instead of only responding with text\n- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons\n- Understands user workflow and intent based on in-product context\n- Maintains session context so interactions remain consistent\nExample: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.\nTechnical details:\n- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)\n- LLM-agnostic (bring your own model)\n- State injection via context providers\n- Tool execution layer for safe function calling\n- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases.","title":"Show HN: YourGPT Copilot SDK Open-source SDK for product-level intelligence","updated_at":"2026-02-04T13:18:11Z","url":"https://copilot-sdk.yourgpt.ai/docs"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Roshni1990r"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Over the past year, many SaaS products have added AI chatbots. They answer questions, guide users, and reduce some support load. That was a useful first step, but it is no longer enough.<p>Most AI still live in a chat window with no awareness of the application. They don't understand product state, selected data, user permissions, or the current workflow, so users end up restating context the system already has.<p>That approach does not scale well for real products.<p>We\u2019ve always believed copilots are the way to deliver the best customer experience\u2014not by answering questions, but by actually doing things<p>We are releasing Copilot SDK as an open-source toolkit to explore this idea and make context-aware, action-driven copilots for product teams.<p>What it does:<p>- Understands application state including current page, selected data, and user permissions<p>- Executes backend and frontend <em>functions</em> instead of only responding with text<p>- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons<p>- Understands user workflow and intent based on in-product context<p>- Maintains session context so interactions remain consistent<p>Example: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.<p>Technical details:<p>- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)<p>- <em>LLM</em>-agnostic (bring your own model)<p>- State injection via context providers<p>- Tool execution layer for safe <em>function</em> <em>calling</em><p>- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https://copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https://copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: YourGPT Copilot SDK \u2013 Open-source toolkit for product-aware AI agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://copilot-sdk.yourgpt.ai/docs"}},"_tags":["story","author_Roshni1990r","story_46870742","show_hn"],"author":"Roshni1990r","created_at":"2026-02-03T13:31:12Z","created_at_i":1770125472,"num_comments":0,"objectID":"46870742","points":1,"story_id":46870742,"story_text":"Over the past year, many SaaS products have added AI chatbots. They answer questions, guide users, and reduce some support load. That was a useful first step, but it is no longer enough.<p>Most AI still live in a chat window with no awareness of the application. They don&#x27;t understand product state, selected data, user permissions, or the current workflow, so users end up restating context the system already has.<p>That approach does not scale well for real products.<p>We\u2019ve always believed copilots are the way to deliver the best customer experience\u2014not by answering questions, but by actually doing things<p>We are releasing Copilot SDK as an open-source toolkit to explore this idea and make context-aware, action-driven copilots for product teams.<p>What it does:<p>- Understands application state including current page, selected data, and user permissions<p>- Executes backend and frontend functions instead of only responding with text<p>- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons<p>- Understands user workflow and intent based on in-product context<p>- Maintains session context so interactions remain consistent<p>Example: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.<p>Technical details:<p>- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)<p>- LLM-agnostic (bring your own model)<p>- State injection via context providers<p>- Tool execution layer for safe function calling<p>- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases.","title":"Show HN: YourGPT Copilot SDK \u2013 Open-source toolkit for product-aware AI agents","updated_at":"2026-02-03T13:35:54Z","url":"https://copilot-sdk.yourgpt.ai/docs"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"yol"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function","calling"],"value":"How existing prompt management solutions work bothers me, it seems to go against programming best practices: the prompt templates are stored in completely separate system from its dependencies, and there\u2019s no interface definitions for using them. It\u2019s like <em>calling</em> a <em>function</em> (the prompt template) that takes ANY arguments and can silently return crap when the arguments don\u2019t align with its internal implementation.<p>So I made this project according to how I think prompt management should work - strongly typed interface, defined in the code; the prompt templates are co-located in the same codebase as their dependencies; and there\u2019s type-hint and validation for devEx. Doing this also brings additional benefit: because the variables are strong typed at compose time, it\u2019s save to support complex prompt templates with if/else/for control loops with full type safety.<p>I\u2019d love to know whether this resonate with others, or is it just my pet peeve."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: Pixie-prompts \u2013 manage <em>LLM</em> prompt templates like code"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://gopixie.ai"}},"_tags":["story","author_yol","story_46781675","show_hn"],"author":"yol","created_at":"2026-01-27T15:55:40Z","created_at_i":1769529340,"num_comments":0,"objectID":"46781675","points":1,"story_id":46781675,"story_text":"How existing prompt management solutions work bothers me, it seems to go against programming best practices: the prompt templates are stored in completely separate system from its dependencies, and there\u2019s no interface definitions for using them. It\u2019s like calling a function (the prompt template) that takes ANY arguments and can silently return crap when the arguments don\u2019t align with its internal implementation.<p>So I made this project according to how I think prompt management should work - strongly typed interface, defined in the code; the prompt templates are co-located in the same codebase as their dependencies; and there\u2019s type-hint and validation for devEx. Doing this also brings additional benefit: because the variables are strong typed at compose time, it\u2019s save to support complex prompt templates with if&#x2F;else&#x2F;for control loops with full type safety.<p>I\u2019d love to know whether this resonate with others, or is it just my pet peeve.","title":"Show HN: Pixie-prompts \u2013 manage LLM prompt templates like code","updated_at":"2026-01-27T15:57:44Z","url":"https://gopixie.ai"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"iCeGaming"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Hey everyone,\nI built <em>llm</em>-schema-guard because LLMs are amazing at spitting out JSON... until they suddenly aren't. Even with JSON mode or <em>function</em> <em>calling</em>, you still get missing fields, wrong types, or just plain broken syntax that kills your agents, RAG flows, or any tool-<em>calling</em> setup.\nThis is a lightweight Rust HTTP proxy that sits in front of any OpenAI-compatible API (think Ollama, vLLM, LocalAI, OpenAI itself, Groq, you name it). It grabs the generated output, checks it against a JSON Schema you provide, and only lets it through if it's valid.\nIf it's invalid, strict mode kicks back a clean 400 with details. Permissive mode tries auto-retrying a few times by tweaking the prompt with a fix instruction and exponential backoff.\nEverything else stays the same: full streaming support (it buffers the response to validate), Prometheus metrics so you can monitor validation fails, retries, latency, and more. Config is simple YAML for upstreams, schemas per model, rate limiting, caching, etc. There's even an offline CLI if you just want to test schemas locally.\nIt's built with Axum and Tokio for really low latency and high throughput, plus jsonschema-rs under the hood. Docker compose makes it dead simple to spin up with Ollama.<p>This grew out of my earlier schema-gateway project, and I'm happy to add stuff like Anthropic support, tool <em>calling</em> validation, or better streaming fixes if people find it useful.\nStars or contributions are very welcome!<p>Thanks for taking a look :)"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: <em>LLM</em>-schema-guard \u2013 Rust proxy enforcing JSON schemas on <em>LLM</em> outputs"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"https://github.com/AncientiCe/<em>llm</em>-schema-guard"}},"_tags":["story","author_iCeGaming","story_46778689","show_hn"],"author":"iCeGaming","created_at":"2026-01-27T11:39:24Z","created_at_i":1769513964,"num_comments":0,"objectID":"46778689","points":1,"story_id":46778689,"story_text":"Hey everyone,\nI built llm-schema-guard because LLMs are amazing at spitting out JSON... until they suddenly aren&#x27;t. Even with JSON mode or function calling, you still get missing fields, wrong types, or just plain broken syntax that kills your agents, RAG flows, or any tool-calling setup.\nThis is a lightweight Rust HTTP proxy that sits in front of any OpenAI-compatible API (think Ollama, vLLM, LocalAI, OpenAI itself, Groq, you name it). It grabs the generated output, checks it against a JSON Schema you provide, and only lets it through if it&#x27;s valid.\nIf it&#x27;s invalid, strict mode kicks back a clean 400 with details. Permissive mode tries auto-retrying a few times by tweaking the prompt with a fix instruction and exponential backoff.\nEverything else stays the same: full streaming support (it buffers the response to validate), Prometheus metrics so you can monitor validation fails, retries, latency, and more. Config is simple YAML for upstreams, schemas per model, rate limiting, caching, etc. There&#x27;s even an offline CLI if you just want to test schemas locally.\nIt&#x27;s built with Axum and Tokio for really low latency and high throughput, plus jsonschema-rs under the hood. Docker compose makes it dead simple to spin up with Ollama.<p>This grew out of my earlier schema-gateway project, and I&#x27;m happy to add stuff like Anthropic support, tool calling validation, or better streaming fixes if people find it useful.\nStars or contributions are very welcome!<p>Thanks for taking a look :)","title":"Show HN: LLM-schema-guard \u2013 Rust proxy enforcing JSON schemas on LLM outputs","updated_at":"2026-01-27T11:45:58Z","url":"https://github.com/AncientiCe/llm-schema-guard"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ashtadmir"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function","calling"],"value":"I've been playing around with the new official GitHub Copilot SDK and realized it's a goldmine for building programmatic bridges to their models.<p>I built this server in Go to act as a OpenAI-compatible proxy. It essentially lets you treat your GitHub Copilot subscription as a standard OpenAI backend for any tool that supports it. I have tested it against OpenWebUI and Langchain.<p>Key Highlights:<p>- Official SDK: Built using the new Github Copilot SDK. It\u2019s much more robust than the reverse-engineered solutions floating around and does not use unpublished APIs.<p>- Tool <em>Calling</em> Support: It maps OpenAI <em>function</em> definitions to Copilot's agentic tools. You can use your own tools/functions through the Copilot without copilot needing access to the said tools just the definitions is enough.<p>The goal was to create a reliable &quot;bridge&quot; so I can use my subscription models in my preferred interfaces."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: An OpenAI API compatible server that uses GitHub Copilot SDK for <em>LLMs</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/RajatGarga/copilot-openai-server"}},"_tags":["story","author_ashtadmir","story_46775889","show_hn"],"author":"ashtadmir","children":[46776731],"created_at":"2026-01-27T05:31:17Z","created_at_i":1769491877,"num_comments":1,"objectID":"46775889","points":2,"story_id":46775889,"story_text":"I&#x27;ve been playing around with the new official GitHub Copilot SDK and realized it&#x27;s a goldmine for building programmatic bridges to their models.<p>I built this server in Go to act as a OpenAI-compatible proxy. It essentially lets you treat your GitHub Copilot subscription as a standard OpenAI backend for any tool that supports it. I have tested it against OpenWebUI and Langchain.<p>Key Highlights:<p>- Official SDK: Built using the new Github Copilot SDK. It\u2019s much more robust than the reverse-engineered solutions floating around and does not use unpublished APIs.<p>- Tool Calling Support: It maps OpenAI function definitions to Copilot&#x27;s agentic tools. You can use your own tools&#x2F;functions through the Copilot without copilot needing access to the said tools just the definitions is enough.<p>The goal was to create a reliable &quot;bridge&quot; so I can use my subscription models in my preferred interfaces.","title":"Show HN: An OpenAI API compatible server that uses GitHub Copilot SDK for LLMs","updated_at":"2026-01-27T07:50:26Z","url":"https://github.com/RajatGarga/copilot-openai-server"}],"hitsPerPage":10,"nbHits":144,"nbPages":15,"page":0,"params":"query=function+calling+LLM&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":21,"processingTimingsMS":{"_request":{"roundTrip":18},"afterFetch":{"format":{"highlighting":1,"total":2}},"fetch":{"query":5,"scanning":14,"total":20},"total":21},"query":"function calling LLM","serverTimeMS":24}
