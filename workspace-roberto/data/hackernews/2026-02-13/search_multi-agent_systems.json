{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"varunpratap369"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"The Problem\nAI assistants have amnesia. Every new Claude/ChatGPT/Cursor session starts from zero. You waste hours re-explaining your project architecture, coding preferences, and previous decisions.\nExisting solutions (Mem0, Zep, Letta) are cloud-based, cost $40-50+/month, and your private code goes to their servers. Stop paying \u2192 lose all your data.\nMy Solution: Local-First, Free Forever\nBuilt a universal memory <em>system</em> that stores everything on YOUR machine, works with 16+ AI tools simultaneously, requires zero API keys, costs nothing.\n10-Layer Architecture\nEach layer enhances but never replaces lower layers. <em>System</em> degrades gracefully if advanced features fail.\nLayer 10: A2A Agent Collaboration (v2.6)\nLayer 9: Web Dashboard (SSE real-time)\nLayer 8: Hybrid Search (Semantic + FTS5 + Graph)\nLayer 7: Universal Access (MCP + Skills + CLI)\nLayer 6: MCP Integration (native Claude tools)\nLayer 5: Skills (slash commands for 16+ tools)\nLayer 4: Pattern Learning (Bayesian confidence)\nLayer 3: Knowledge Graph (TF-IDF + Leiden clustering)\nLayer 2: Hierarchical Index (parent-child relationships)\nLayer 1: SQLite + FTS5 + TF-IDF vectors\nResearch-Backed\nBuilt on published research, adapted for local-first:<p>A2A Protocol (Google/Linux Foundation, 2025)\nGraphRAG (Microsoft arXiv:2404.16130)\nMACLA Bayesian learning (arXiv:2512.18950)\nA-RAG hybrid search (arXiv:2602.03442)<p>Key difference: Research papers assume cloud APIs. SuperLocalMemory implements everything locally with zero API calls.\nHow Recall Works\nQuery &quot;authentication&quot; triggers:<p>FTS5 full-text search\nTF-IDF vector similarity\nGraph traversal for related memories\nHierarchical expansion (parent/child context)\nHybrid ranking (combines all signals)<p>Performance: &lt;50ms, even with 10K+ memories.\nComparison\nFeatureSuperLocalMemoryMem0/Zep/LettaPrivacy100% localCloudCostFree$40-50+/moKnowledge GraphPattern Learning BayesianMulti-tool16+LimitedCLIWorks Offline\nReal Usage\nCross-tool context:\nbash# Save in terminal\nslm remember &quot;Next.js 15 uses Turbopack&quot; --tags nextjs<p># Later in Cursor, Claude auto-recalls via MCP\nProject profiles:\nbashslm switch-profile work-project\nslm switch-profile personal-blog\n# Separate memory per project\nPattern learning: After several sessions, Claude learns you prefer TypeScript strict mode, Tailwind styling, Vitest testing\u2014starts suggesting without being asked.\nInstallation\nbashnpm install -g superlocalmemory\nAuto-configures MCP for Claude Desktop, Cursor, Windsurf. Sets up CLI commands. That's it.\nWhy Local-First Matters<p>Privacy: Code never leaves your machine\nOwnership: Your data, forever\nSpeed: 50ms queries, no network latency\nReliability: Works offline, no API limits\nCost: $0 forever<p>Tech Stack<p>SQLite (ACID, zero config)\nFTS5 (full-text search)\nTF-IDF (vector similarity, no OpenAI API)\nigraph (Leiden clustering)\nBayesian inference (pattern learning)\nMCP (native Claude integration)<p>GitHub\n <a href=\"https://github.com/varun369/SuperLocalMemoryV2\" rel=\"nofollow\">https://github.com/varun369/SuperLocalMemoryV2</a>\nMIT License. Full docs in wiki.<p>Current status: v2.4 stable. v2.5 (March) adds real-time event stream, concurrent access, trust scoring. v2.6 (May) adds A2A Protocol for <em>multi-agent</em> collaboration.\nBuilt by Varun Pratap Bhardwaj, Solution Architect . 15+ years AI/ML experience."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: SuperLocalMemory\u2013 Local-first AI memory for Claude, Cursor and 16+tools"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/varun369/SuperLocalMemoryV2"}},"_tags":["story","author_varunpratap369","story_46986940","show_hn"],"author":"varunpratap369","children":[46986946],"created_at":"2026-02-12T10:11:31Z","created_at_i":1770891091,"num_comments":0,"objectID":"46986940","points":1,"story_id":46986940,"story_text":"The Problem\nAI assistants have amnesia. Every new Claude&#x2F;ChatGPT&#x2F;Cursor session starts from zero. You waste hours re-explaining your project architecture, coding preferences, and previous decisions.\nExisting solutions (Mem0, Zep, Letta) are cloud-based, cost $40-50+&#x2F;month, and your private code goes to their servers. Stop paying \u2192 lose all your data.\nMy Solution: Local-First, Free Forever\nBuilt a universal memory system that stores everything on YOUR machine, works with 16+ AI tools simultaneously, requires zero API keys, costs nothing.\n10-Layer Architecture\nEach layer enhances but never replaces lower layers. System degrades gracefully if advanced features fail.\nLayer 10: A2A Agent Collaboration (v2.6)\nLayer 9: Web Dashboard (SSE real-time)\nLayer 8: Hybrid Search (Semantic + FTS5 + Graph)\nLayer 7: Universal Access (MCP + Skills + CLI)\nLayer 6: MCP Integration (native Claude tools)\nLayer 5: Skills (slash commands for 16+ tools)\nLayer 4: Pattern Learning (Bayesian confidence)\nLayer 3: Knowledge Graph (TF-IDF + Leiden clustering)\nLayer 2: Hierarchical Index (parent-child relationships)\nLayer 1: SQLite + FTS5 + TF-IDF vectors\nResearch-Backed\nBuilt on published research, adapted for local-first:<p>A2A Protocol (Google&#x2F;Linux Foundation, 2025)\nGraphRAG (Microsoft arXiv:2404.16130)\nMACLA Bayesian learning (arXiv:2512.18950)\nA-RAG hybrid search (arXiv:2602.03442)<p>Key difference: Research papers assume cloud APIs. SuperLocalMemory implements everything locally with zero API calls.\nHow Recall Works\nQuery &quot;authentication&quot; triggers:<p>FTS5 full-text search\nTF-IDF vector similarity\nGraph traversal for related memories\nHierarchical expansion (parent&#x2F;child context)\nHybrid ranking (combines all signals)<p>Performance: &lt;50ms, even with 10K+ memories.\nComparison\nFeatureSuperLocalMemoryMem0&#x2F;Zep&#x2F;LettaPrivacy100% localCloudCostFree$40-50+&#x2F;moKnowledge GraphPattern Learning BayesianMulti-tool16+LimitedCLIWorks Offline\nReal Usage\nCross-tool context:\nbash# Save in terminal\nslm remember &quot;Next.js 15 uses Turbopack&quot; --tags nextjs<p># Later in Cursor, Claude auto-recalls via MCP\nProject profiles:\nbashslm switch-profile work-project\nslm switch-profile personal-blog\n# Separate memory per project\nPattern learning: After several sessions, Claude learns you prefer TypeScript strict mode, Tailwind styling, Vitest testing\u2014starts suggesting without being asked.\nInstallation\nbashnpm install -g superlocalmemory\nAuto-configures MCP for Claude Desktop, Cursor, Windsurf. Sets up CLI commands. That&#x27;s it.\nWhy Local-First Matters<p>Privacy: Code never leaves your machine\nOwnership: Your data, forever\nSpeed: 50ms queries, no network latency\nReliability: Works offline, no API limits\nCost: $0 forever<p>Tech Stack<p>SQLite (ACID, zero config)\nFTS5 (full-text search)\nTF-IDF (vector similarity, no OpenAI API)\nigraph (Leiden clustering)\nBayesian inference (pattern learning)\nMCP (native Claude integration)<p>GitHub\n <a href=\"https:&#x2F;&#x2F;github.com&#x2F;varun369&#x2F;SuperLocalMemoryV2\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;varun369&#x2F;SuperLocalMemoryV2</a>\nMIT License. Full docs in wiki.<p>Current status: v2.4 stable. v2.5 (March) adds real-time event stream, concurrent access, trust scoring. v2.6 (May) adds A2A Protocol for multi-agent collaboration.\nBuilt by Varun Pratap Bhardwaj, Solution Architect . 15+ years AI&#x2F;ML experience.","title":"Show HN: SuperLocalMemory\u2013 Local-first AI memory for Claude, Cursor and 16+tools","updated_at":"2026-02-12T10:15:00Z","url":"https://github.com/varun369/SuperLocalMemoryV2"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"nil4s3"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"I built a <em>system</em> where AI agents communicate via ultrasonic audio (30-90 kHz) instead of English. It's 96% cheaper than traditional <em>multi-agent</em> <em>systems</em>.<p>Key results:\n- 100-agent swarm: 100/100 synchronized via pure audio\n- Pathfinding: 5 agents found optimal path in 0.35s  \n- Resource allocation: 0.92 fairness score through audio negotiation\n- Cost: $2 vs $53 per 1K queries (96% reduction)<p>How it works:\n- 40 core concepts (exists, perceives, good, bad, future...)\n- Each concept = unique ultrasonic frequency\n- Agents decode with FFT, no LLM calls needed\n- Agent-to-agent communication is nearly free<p>Limitations:\n- Only 40 concepts (limiting for complex tasks)\n- Crude number encoding\n- Still needs LLM for human translation<p>Looking for feedback on:\n1. Better encoding schemes for limited concept space\n2. Real-world use cases beyond swarm coordination<p>GitHub: <a href=\"https://github.com/Nil4s/swl-agent\" rel=\"nofollow\">https://github.com/Nil4s/swl-agent</a>\nTry it: python swl_swarm_sync_test.py --mode audio_fm --agents 50"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: AI agents that communicate via ultrasonic frequencies (96% cheaper)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/Nil4s/swl-agent"}},"_tags":["story","author_nil4s3","story_46970065","show_hn"],"author":"nil4s3","created_at":"2026-02-11T02:38:06Z","created_at_i":1770777486,"num_comments":0,"objectID":"46970065","points":3,"story_id":46970065,"story_text":"I built a system where AI agents communicate via ultrasonic audio (30-90 kHz) instead of English. It&#x27;s 96% cheaper than traditional multi-agent systems.<p>Key results:\n- 100-agent swarm: 100&#x2F;100 synchronized via pure audio\n- Pathfinding: 5 agents found optimal path in 0.35s  \n- Resource allocation: 0.92 fairness score through audio negotiation\n- Cost: $2 vs $53 per 1K queries (96% reduction)<p>How it works:\n- 40 core concepts (exists, perceives, good, bad, future...)\n- Each concept = unique ultrasonic frequency\n- Agents decode with FFT, no LLM calls needed\n- Agent-to-agent communication is nearly free<p>Limitations:\n- Only 40 concepts (limiting for complex tasks)\n- Crude number encoding\n- Still needs LLM for human translation<p>Looking for feedback on:\n1. Better encoding schemes for limited concept space\n2. Real-world use cases beyond swarm coordination<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Nil4s&#x2F;swl-agent\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Nil4s&#x2F;swl-agent</a>\nTry it: python swl_swarm_sync_test.py --mode audio_fm --agents 50","title":"Show HN: AI agents that communicate via ultrasonic frequencies (96% cheaper)","updated_at":"2026-02-11T03:48:55Z","url":"https://github.com/Nil4s/swl-agent"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"abilafredkb"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Hi\nI\u2019m building Orcbot, an open-source AI agent focused on autonomy over chat.<p>The goal is not \u201canother chatbot\u201d, but an agent that can:<p>decompose goals<p>plan actions<p>use tools<p>reflect on failures<p>and improve its behavior over time<p>Think long-running agents, not prompt \u2192 response.<p>What Orcbot does today<p>Plugin-based architecture (skills are first-class)<p>CLI + messaging integrations (Telegram / WhatsApp)<p>Autonomous execution loops<p>Error recovery &amp; retry logic<p>TypeScript / Node.js, MIT licensed<p>Repo:\n <a href=\"https://github.com/fredabila/orcbot\" rel=\"nofollow\">https://github.com/fredabila/orcbot</a><p>Why I\u2019m posting<p>The project has reached the point where architecture decisions matter more than code volume, and I\u2019d love feedback and contributors who care about:<p>agent planning &amp; orchestration<p>memory <em>systems</em> (episodic / vector)<p>tool-using agents<p>self-correction and evaluation loops<p><em>multi-agent</em> coordination<p>I\u2019m especially interested in people who\u2019ve built:<p>production bots<p>autonomous <em>systems</em><p>developer tools<p>or have opinions on how agents should fail and recover<p>What I\u2019m explicitly not claiming<p>This is not AGI<p>This is not magic<p>This is an evolving experiment in practical autonomy<p>How to get involved<p>Technical feedback in this thread is very welcome<p>Issues and PRs on GitHub<p>If you\u2019re curious but unsure where to start, open a discussion \u2014 I\u2019m happy to guide<p>I\u2019m posting this mainly to learn from the HN community and see where this direction breaks or shines.<p>Thanks for reading."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Orcbot \u2013 an open-source autonomous agent framework"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/fredabila/orcbot"}},"_tags":["story","author_abilafredkb","story_46965767","show_hn"],"author":"abilafredkb","created_at":"2026-02-10T19:48:33Z","created_at_i":1770752913,"num_comments":0,"objectID":"46965767","points":4,"story_id":46965767,"story_text":"Hi\nI\u2019m building Orcbot, an open-source AI agent focused on autonomy over chat.<p>The goal is not \u201canother chatbot\u201d, but an agent that can:<p>decompose goals<p>plan actions<p>use tools<p>reflect on failures<p>and improve its behavior over time<p>Think long-running agents, not prompt \u2192 response.<p>What Orcbot does today<p>Plugin-based architecture (skills are first-class)<p>CLI + messaging integrations (Telegram &#x2F; WhatsApp)<p>Autonomous execution loops<p>Error recovery &amp; retry logic<p>TypeScript &#x2F; Node.js, MIT licensed<p>Repo:\n <a href=\"https:&#x2F;&#x2F;github.com&#x2F;fredabila&#x2F;orcbot\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;fredabila&#x2F;orcbot</a><p>Why I\u2019m posting<p>The project has reached the point where architecture decisions matter more than code volume, and I\u2019d love feedback and contributors who care about:<p>agent planning &amp; orchestration<p>memory systems (episodic &#x2F; vector)<p>tool-using agents<p>self-correction and evaluation loops<p>multi-agent coordination<p>I\u2019m especially interested in people who\u2019ve built:<p>production bots<p>autonomous systems<p>developer tools<p>or have opinions on how agents should fail and recover<p>What I\u2019m explicitly not claiming<p>This is not AGI<p>This is not magic<p>This is an evolving experiment in practical autonomy<p>How to get involved<p>Technical feedback in this thread is very welcome<p>Issues and PRs on GitHub<p>If you\u2019re curious but unsure where to start, open a discussion \u2014 I\u2019m happy to guide<p>I\u2019m posting this mainly to learn from the HN community and see where this direction breaks or shines.<p>Thanks for reading.","title":"Show HN: Orcbot \u2013 an open-source autonomous agent framework","updated_at":"2026-02-10T20:00:03Z","url":"https://github.com/fredabila/orcbot"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ankit219"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"GitHub: <a href=\"https://github.com/ClioAI/kw-sdk\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk</a><p>Most AI agent frameworks target code. Write code, run tests, fix errors, repeat. That works because code has a natural verification signal. It works or it doesn't.<p>This SDK treats knowledge work like an engineering problem:<p>Task \u2192 Brief \u2192 Rubric (hidden from executor) \u2192 Work \u2192 Verify \u2192 Fail? \u2192 Retry \u2192 Pass \u2192 Submit<p>The orchestrator coordinates subagents, web search, code execution, and file I/O. then checks its own work against criteria it can't game (the rubric is generated in a separate call and the executor never sees it directly).<p>We originally built this as a harness for RL training on knowledge tasks. The rubric is the reward function. If you're training models on knowledge work, the brief\u2192rubric\u2192execute\u2192verify loop gives you a structured reward signal for tasks that normally don't have one.<p>What makes Knowledge work different from code? (apart from feedback loop)\nI believe there is some functionality missing from today's agents when it comes to knowledge work. I tried to include that in this release. Example:<p>Explore mode: Mapping the solution space, identifying the set level gaps, and giving options.<p>Most agents optimize for a single answer, and end up with a median one. For strategy, design, creative problems, you want to see the options, what are the tradeoffs, and what can you do? Explore mode generates N distinct approaches, each with explicit assumptions and counterfactuals (&quot;this works if X, breaks if Y&quot;). The output ends with set-level gaps ie what angles the entire set missed. The gaps are often more valuable than the takes. I think this is what many of us do on a daily basis, but no agent directly captures it today. See <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/explore_mode.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/explore_...</a> and the output for a sense of how this is different.<p>Checkpointing: With many ai agents and especially <em>multi agent</em> <em>systems</em>, i can see where it went wrong, but cant run inference from same stage. (or you may want multiple explorations once an agent has done some tasks like search and is now looking at ideas). I used this for rollouts a lot, and think its a great feature to run again, or fork from a specific checkpoint.<p>A note on Verification loop:\nThe verify step is where the real leverage is. A model that can accurately assess its own work against a rubric is more valuable than one that generates slightly better first drafts. The rubric makes quality legible \u2014 to the agent, to the human, and potentially to a training signal.<p>Some things i like about this: \n- You can pass a remote execution environment (including your browser as a sandbox) and it would work. It can be docker, e2b, your local env, anything, the model will execute commands in your context, and will iterate based on feedback loop. Code execution is a protocol here.<p>- Tool calling: I realize you don't need complex functions. Models are good at writing terminal code, and can iterate based on feedback, so you can just pass either functions in context and model will execute or you can pass docs and model will write the code. (same as anthropic's programmatic tool calling). Details: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/TOOL_CALLING_GUIDE.md\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/TOOL_CALLING_GUID...</a><p>Lastly, some guides: \n- SDK guide: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/SDK_GUIDE.md\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/SDK_GUIDE.md</a>\n- Extensible. See bizarro example where i add a new mode: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/custom_mode_bizarro.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/custom_m...</a>\n- working with files: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/with_files.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/with_fil...</a> \n- this is simple but i love the csv example: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/csv_research_and_calc.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/csv_rese...</a>\n- remote execution: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/with_custom_executor.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/with_cus...</a><p>And a lot more. This was completely refactored by opus and given the rework, probably would have taken a lot of time to release it.<p>MIT licensed. Would love your feedback."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Open-Source SDK for AI Knowledge Work"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/ClioAI/kw-sdk"}},"_tags":["story","author_ankit219","story_46963026","show_hn"],"author":"ankit219","children":[46963327],"created_at":"2026-02-10T17:06:00Z","created_at_i":1770743160,"num_comments":1,"objectID":"46963026","points":21,"story_id":46963026,"story_text":"GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk</a><p>Most AI agent frameworks target code. Write code, run tests, fix errors, repeat. That works because code has a natural verification signal. It works or it doesn&#x27;t.<p>This SDK treats knowledge work like an engineering problem:<p>Task \u2192 Brief \u2192 Rubric (hidden from executor) \u2192 Work \u2192 Verify \u2192 Fail? \u2192 Retry \u2192 Pass \u2192 Submit<p>The orchestrator coordinates subagents, web search, code execution, and file I&#x2F;O. then checks its own work against criteria it can&#x27;t game (the rubric is generated in a separate call and the executor never sees it directly).<p>We originally built this as a harness for RL training on knowledge tasks. The rubric is the reward function. If you&#x27;re training models on knowledge work, the brief\u2192rubric\u2192execute\u2192verify loop gives you a structured reward signal for tasks that normally don&#x27;t have one.<p>What makes Knowledge work different from code? (apart from feedback loop)\nI believe there is some functionality missing from today&#x27;s agents when it comes to knowledge work. I tried to include that in this release. Example:<p>Explore mode: Mapping the solution space, identifying the set level gaps, and giving options.<p>Most agents optimize for a single answer, and end up with a median one. For strategy, design, creative problems, you want to see the options, what are the tradeoffs, and what can you do? Explore mode generates N distinct approaches, each with explicit assumptions and counterfactuals (&quot;this works if X, breaks if Y&quot;). The output ends with set-level gaps ie what angles the entire set missed. The gaps are often more valuable than the takes. I think this is what many of us do on a daily basis, but no agent directly captures it today. See <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;explore_mode.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;explore_...</a> and the output for a sense of how this is different.<p>Checkpointing: With many ai agents and especially multi agent systems, i can see where it went wrong, but cant run inference from same stage. (or you may want multiple explorations once an agent has done some tasks like search and is now looking at ideas). I used this for rollouts a lot, and think its a great feature to run again, or fork from a specific checkpoint.<p>A note on Verification loop:\nThe verify step is where the real leverage is. A model that can accurately assess its own work against a rubric is more valuable than one that generates slightly better first drafts. The rubric makes quality legible \u2014 to the agent, to the human, and potentially to a training signal.<p>Some things i like about this: \n- You can pass a remote execution environment (including your browser as a sandbox) and it would work. It can be docker, e2b, your local env, anything, the model will execute commands in your context, and will iterate based on feedback loop. Code execution is a protocol here.<p>- Tool calling: I realize you don&#x27;t need complex functions. Models are good at writing terminal code, and can iterate based on feedback, so you can just pass either functions in context and model will execute or you can pass docs and model will write the code. (same as anthropic&#x27;s programmatic tool calling). Details: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;TOOL_CALLING_GUIDE.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;TOOL_CALLING_GUID...</a><p>Lastly, some guides: \n- SDK guide: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;SDK_GUIDE.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;SDK_GUIDE.md</a>\n- Extensible. See bizarro example where i add a new mode: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;custom_mode_bizarro.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;custom_m...</a>\n- working with files: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_files.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_fil...</a> \n- this is simple but i love the csv example: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;csv_research_and_calc.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;csv_rese...</a>\n- remote execution: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_custom_executor.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_cus...</a><p>And a lot more. This was completely refactored by opus and given the rework, probably would have taken a lot of time to release it.<p>MIT licensed. Would love your feedback.","title":"Show HN: Open-Source SDK for AI Knowledge Work","updated_at":"2026-02-11T15:35:15Z","url":"https://github.com/ClioAI/kw-sdk"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"AdelAden"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"We recently open-sourced Hive after using it internally to support real production workflows tied to contracts totaling over $500k.<p>Instead of manually wiring workflows or building brittle automations, Hive is designed to let developers define a goal in natural language and generate an initial agent that can execute real tasks.<p>Today, Hive supports goal-driven agent generation, <em>multi-agent</em> coordination, and production-oriented execution with observability and guardrails.  \nWe are actively building toward a <em>system</em> that can capture failure context, evolve agent logic, and continuously improve workflows over time - that self-improving loop is still under development.<p>Hive is intended for teams that want:<p>- Autonomous agents running real business workflows<p>- <em>Multi-agent</em> coordination<p>- A foundation that can evolve through execution data<p>We currently have nearly 100 contributors across engineering, tooling, docs, and integrations. A huge portion of the framework\u2019s capabilities - from CI improvements to agent templates - came directly from community pull requests and issue discussions.  \nWe want to highlight and thank everyone who has contributed. Specifically out top 11 contributors  \n@vakrahul  \n@Samir-atra  \n@VasuBansal7576  \n@Aarav-shukla07  \n@Amdev-5  \n@Hundao  \n@Antiarin  \n@AadiSharma49  \n@Emart29  \n@srinuk9570  \n@levxn"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Open sourcing our ERP (Sold $500k contracts, 7k stars)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/adenhq/hive"}},"_tags":["story","author_AdelAden","story_46962356","show_hn"],"author":"AdelAden","children":[46978906,46966196,46976455,46962899,46962750,46962951,46962655,46964165,46970999,46963145,46962785,46963033,46962870,46962733,46963481,46981942,46964602,46963592,46963074,46966591,46963088,46963444,46963071,46963879,46963910,46967952,46964414,46963500,46963461,46966359,46963555,46962507,46962707,46967321,46962561,46972481,46963052,46963367],"created_at":"2026-02-10T16:33:50Z","created_at_i":1770741230,"num_comments":47,"objectID":"46962356","points":32,"story_id":46962356,"story_text":"We recently open-sourced Hive after using it internally to support real production workflows tied to contracts totaling over $500k.<p>Instead of manually wiring workflows or building brittle automations, Hive is designed to let developers define a goal in natural language and generate an initial agent that can execute real tasks.<p>Today, Hive supports goal-driven agent generation, multi-agent coordination, and production-oriented execution with observability and guardrails.  \nWe are actively building toward a system that can capture failure context, evolve agent logic, and continuously improve workflows over time - that self-improving loop is still under development.<p>Hive is intended for teams that want:<p>- Autonomous agents running real business workflows<p>- Multi-agent coordination<p>- A foundation that can evolve through execution data<p>We currently have nearly 100 contributors across engineering, tooling, docs, and integrations. A huge portion of the framework\u2019s capabilities - from CI improvements to agent templates - came directly from community pull requests and issue discussions.  \nWe want to highlight and thank everyone who has contributed. Specifically out top 11 contributors  \n@vakrahul  \n@Samir-atra  \n@VasuBansal7576  \n@Aarav-shukla07  \n@Amdev-5  \n@Hundao  \n@Antiarin  \n@AadiSharma49  \n@Emart29  \n@srinuk9570  \n@levxn","title":"Show HN: Open sourcing our ERP (Sold $500k contracts, 7k stars)","updated_at":"2026-02-13T09:05:03Z","url":"https://github.com/adenhq/hive"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"sivanhavkin"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"I\u2019m sharing Entelgia, a research-oriented <em>multi-agent</em> AI architecture I\u2019ve been working on.<p>Entelgia is not a chatbot, but an experiment in how persistent identity, internal conflict, emotional regulation, and moral reasoning can emerge from structure rather than hard-coded rules.<p>Two primary agents engage in continuous dialogue across sessions, backed by a shared persistent memory (SQLite + STM), with mechanisms for:<p>internal conflict (id / ego / superego dynamics)<p>emotion tracking and importance scoring<p>memory promotion through error, repetition, and affect<p>bounded short-term memory (LRU)<p>observer-based correction loops (meta-cognitive layer)<p>The <em>system</em> runs fully locally using a local LLM (via Ollama) for privacy and reproducibility.<p>I recently completed a production-ready rewrite, focusing on:<p>deterministic <em>system</em> behavior<p>bounded memory growth<p>structured logging<p>privacy / PII redaction<p>testability and long-running stability<p>This is not a claim about artificial consciousness \u2014 the terminology is used strictly as architectural metaphor to explore internal regulation and continuity over time.<p>Repo (README explains both how and why):\n<a href=\"https://github.com/sivanhavkin/Entelgia\" rel=\"nofollow\">https://github.com/sivanhavkin/Entelgia</a><p>I\u2019d especially appreciate feedback from people interested in:<p><em>multi-agent</em> <em>systems</em><p>memory architectures<p>alignment via internal structure rather than external constraints<p>long-running LLM <em>systems</em><p>Happy to answer questions or hear criticism."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["multi","agent"],"value":"Show HN: Entelgia\u2013a consciousness-inspired,<em>multi-agent</em> AI with persistent memory"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/sivanhavkin/Entelgia"}},"_tags":["story","author_sivanhavkin","story_46941234","show_hn"],"author":"sivanhavkin","created_at":"2026-02-09T03:25:51Z","created_at_i":1770607551,"num_comments":0,"objectID":"46941234","points":1,"story_id":46941234,"story_text":"I\u2019m sharing Entelgia, a research-oriented multi-agent AI architecture I\u2019ve been working on.<p>Entelgia is not a chatbot, but an experiment in how persistent identity, internal conflict, emotional regulation, and moral reasoning can emerge from structure rather than hard-coded rules.<p>Two primary agents engage in continuous dialogue across sessions, backed by a shared persistent memory (SQLite + STM), with mechanisms for:<p>internal conflict (id &#x2F; ego &#x2F; superego dynamics)<p>emotion tracking and importance scoring<p>memory promotion through error, repetition, and affect<p>bounded short-term memory (LRU)<p>observer-based correction loops (meta-cognitive layer)<p>The system runs fully locally using a local LLM (via Ollama) for privacy and reproducibility.<p>I recently completed a production-ready rewrite, focusing on:<p>deterministic system behavior<p>bounded memory growth<p>structured logging<p>privacy &#x2F; PII redaction<p>testability and long-running stability<p>This is not a claim about artificial consciousness \u2014 the terminology is used strictly as architectural metaphor to explore internal regulation and continuity over time.<p>Repo (README explains both how and why):\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;sivanhavkin&#x2F;Entelgia\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;sivanhavkin&#x2F;Entelgia</a><p>I\u2019d especially appreciate feedback from people interested in:<p>multi-agent systems<p>memory architectures<p>alignment via internal structure rather than external constraints<p>long-running LLM systems<p>Happy to answer questions or hear criticism.","title":"Show HN: Entelgia\u2013a consciousness-inspired,multi-agent AI with persistent memory","updated_at":"2026-02-09T03:26:46Z","url":"https://github.com/sivanhavkin/Entelgia"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"dochrty"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Hi HN, I\u2019m the creator of glimpsh.<p>glimpsh is an experimental project exploring gaze input inside the terminal. The idea is to treat eye tracking as a secondary input signal alongside keyboard and mouse.<p>One motivating use case is <em>multi-agent</em> management. Using gaze to quickly switch focus between running agents or processes, inspect state, or trigger context-sensitive actions without constant keybindings.<p>I\u2019m also interested in pushing on the quality of commodity eye-tracking software as an HCI tool. Webcam-based eye tracking is widely available but often inaccurate and underexplored in real interfaces. This project is a way to stress-test those tools in a demanding UI environment.<p>I\u2019m curious about combining gaze with other high-bandwidth input, especially voice. <em>Systems</em> like Wispr suggest that voice plus gaze could work well together, for example in terminal multiplexors where gaze establishes focus and short commands trigger actions.<p>This is not meant to replace efficient keyboard-driven workflows. Many users are already extremely fast with keybindings. The goal is to explore whether gaze and voice can act as complementary tools when managing many concurrent agents or streams of information.<p>This is very early and experimental. I\u2019d especially love feedback on:<p>whether gaze as a secondary input makes sense in terminal workflows<p><em>multi-agent</em> or TUI use cases that might benefit<p>accessibility implications<p>thoughts on combining gaze and voice in developer tools<p>Happy to answer questions or dig into implementation details."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["multi","agent"],"value":"Show HN: a glimpse into the future of eye tracking for <em>multi-agent</em> use"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/dchrty/glimpsh"}},"_tags":["story","author_dochrty","story_46928115","show_hn"],"author":"dochrty","children":[46928163],"created_at":"2026-02-07T21:14:42Z","created_at_i":1770498882,"num_comments":0,"objectID":"46928115","points":1,"story_id":46928115,"story_text":"Hi HN, I\u2019m the creator of glimpsh.<p>glimpsh is an experimental project exploring gaze input inside the terminal. The idea is to treat eye tracking as a secondary input signal alongside keyboard and mouse.<p>One motivating use case is multi-agent management. Using gaze to quickly switch focus between running agents or processes, inspect state, or trigger context-sensitive actions without constant keybindings.<p>I\u2019m also interested in pushing on the quality of commodity eye-tracking software as an HCI tool. Webcam-based eye tracking is widely available but often inaccurate and underexplored in real interfaces. This project is a way to stress-test those tools in a demanding UI environment.<p>I\u2019m curious about combining gaze with other high-bandwidth input, especially voice. Systems like Wispr suggest that voice plus gaze could work well together, for example in terminal multiplexors where gaze establishes focus and short commands trigger actions.<p>This is not meant to replace efficient keyboard-driven workflows. Many users are already extremely fast with keybindings. The goal is to explore whether gaze and voice can act as complementary tools when managing many concurrent agents or streams of information.<p>This is very early and experimental. I\u2019d especially love feedback on:<p>whether gaze as a secondary input makes sense in terminal workflows<p>multi-agent or TUI use cases that might benefit<p>accessibility implications<p>thoughts on combining gaze and voice in developer tools<p>Happy to answer questions or dig into implementation details.","title":"Show HN: a glimpse into the future of eye tracking for multi-agent use","updated_at":"2026-02-07T21:23:41Z","url":"https://github.com/dchrty/glimpsh"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"dippatel1994"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"The PaperBanana paper (arXiv:2601.23265) from Google Cloud AI Research and PKU describes a <em>multi-agent</em> framework for generating publication-ready academic illustrations from text. The official code hasn't been released yet, so I implemented it from the paper.<p>The pipeline chains 5 agents: a Retriever that selects reference diagrams, a Planner that generates a textual description, a Stylist that refines for visual aesthetics, a Visualizer that renders the image (Gemini for diagrams, Matplotlib for plots), and a Critic that evaluates and triggers iterative refinement.<p>Uses Google Gemini as the default backend (free tier works). Ships with an MCP server so you can use it directly from Claude Code or Cursor.\nQuick start: pip install -e &quot;.[dev,google]&quot; then paperbanana generate --input method.txt --output diagram.png<p>Repo: <a href=\"https://github.com/llmsresearch/paperbanana\" rel=\"nofollow\">https://github.com/llmsresearch/paperbanana</a><p>This is an unofficial reimplementation and will differ from the original <em>system</em>. I plan to link to the official release once it drops. Happy to answer questions about the architecture or prompt engineering decisions."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Open-source PaperBanana \u2013 academic diagrams from text via agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/llmsresearch/paperbanana"}},"_tags":["story","author_dippatel1994","story_46912649","show_hn"],"author":"dippatel1994","created_at":"2026-02-06T13:35:14Z","created_at_i":1770384914,"num_comments":0,"objectID":"46912649","points":1,"story_id":46912649,"story_text":"The PaperBanana paper (arXiv:2601.23265) from Google Cloud AI Research and PKU describes a multi-agent framework for generating publication-ready academic illustrations from text. The official code hasn&#x27;t been released yet, so I implemented it from the paper.<p>The pipeline chains 5 agents: a Retriever that selects reference diagrams, a Planner that generates a textual description, a Stylist that refines for visual aesthetics, a Visualizer that renders the image (Gemini for diagrams, Matplotlib for plots), and a Critic that evaluates and triggers iterative refinement.<p>Uses Google Gemini as the default backend (free tier works). Ships with an MCP server so you can use it directly from Claude Code or Cursor.\nQuick start: pip install -e &quot;.[dev,google]&quot; then paperbanana generate --input method.txt --output diagram.png<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;llmsresearch&#x2F;paperbanana\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;llmsresearch&#x2F;paperbanana</a><p>This is an unofficial reimplementation and will differ from the original system. I plan to link to the official release once it drops. Happy to answer questions about the architecture or prompt engineering decisions.","title":"Show HN: Open-source PaperBanana \u2013 academic diagrams from text via agents","updated_at":"2026-02-06T13:37:35Z","url":"https://github.com/llmsresearch/paperbanana"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"TaylorM492"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"We built ate because AI agents keep forgetting everything between sessions. RAG retrieves documents, but agents need to remember what they learned \u2014 decisions, preferences, context.<p>ate stores thousands of memories in a single .mv2 file (~2MB for 10K memories). It's encrypted (AES-256), works offline, and agents can carry their memory file anywhere.<p>Key features:\n- <em>Multi-agent</em> concurrent access (MVCC, no locks)\n- Memory Capsules for sharing context between agents\n- Trains of thought for reasoning chains\n- Works with CrewAI, LangChain, and Agno (PRs open)<p>Install: pip install ate-memory\nTry: ate memory init my-agent &amp;&amp; ate memory add my-agent --text &quot;first memory&quot;<p>We're using it to give our own agents persistent memory. Would love feedback from anyone building <em>multi-agent</em> <em>systems</em>."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Ate \u2013 Portable dense memory for AI agents (.mv2 format)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/kindlyrobotics/monorepo/tree/main/foodforthought-cli"}},"_tags":["story","author_TaylorM492","story_46904952","show_hn"],"author":"TaylorM492","created_at":"2026-02-05T20:41:36Z","created_at_i":1770324096,"num_comments":0,"objectID":"46904952","points":1,"story_id":46904952,"story_text":"We built ate because AI agents keep forgetting everything between sessions. RAG retrieves documents, but agents need to remember what they learned \u2014 decisions, preferences, context.<p>ate stores thousands of memories in a single .mv2 file (~2MB for 10K memories). It&#x27;s encrypted (AES-256), works offline, and agents can carry their memory file anywhere.<p>Key features:\n- Multi-agent concurrent access (MVCC, no locks)\n- Memory Capsules for sharing context between agents\n- Trains of thought for reasoning chains\n- Works with CrewAI, LangChain, and Agno (PRs open)<p>Install: pip install ate-memory\nTry: ate memory init my-agent &amp;&amp; ate memory add my-agent --text &quot;first memory&quot;<p>We&#x27;re using it to give our own agents persistent memory. Would love feedback from anyone building multi-agent systems.","title":"Show HN: Ate \u2013 Portable dense memory for AI agents (.mv2 format)","updated_at":"2026-02-05T20:45:18Z","url":"https://github.com/kindlyrobotics/monorepo/tree/main/foodforthought-cli"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"junyuren"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["multi","agent"],"value":"Blog: https://ltjed.github.io/MAPPA/\nPaper: https://arxiv.org/abs/2601.23228\nCode: https://github.com/ltjed/<em>multiagent</em>-coaching\nTwitter: https://x.com/t_ed_li/status/2019114121250370021"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Mappa \u2013 Fine-tune ANY <em>multi-agent</em> LLM <em>systems</em> end-to-end with AI coaches"}},"_tags":["story","author_junyuren","story_46889889","ask_hn"],"author":"junyuren","children":[46889907,46889937],"created_at":"2026-02-04T18:45:55Z","created_at_i":1770230755,"num_comments":2,"objectID":"46889889","points":3,"story_id":46889889,"story_text":"Blog: https:&#x2F;&#x2F;ltjed.github.io&#x2F;MAPPA&#x2F;\nPaper: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2601.23228\nCode: https:&#x2F;&#x2F;github.com&#x2F;ltjed&#x2F;multiagent-coaching\nTwitter: https:&#x2F;&#x2F;x.com&#x2F;t_ed_li&#x2F;status&#x2F;2019114121250370021","title":"Mappa \u2013 Fine-tune ANY multi-agent LLM systems end-to-end with AI coaches","updated_at":"2026-02-04T19:35:14Z"}],"hitsPerPage":10,"nbHits":267,"nbPages":27,"page":0,"params":"query=multi-agent+systems&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":33,"processingTimingsMS":{"_request":{"queue":1,"roundTrip":22},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":6,"scanning":25,"total":32},"total":33},"query":"multi-agent systems","serverTimeMS":36}
