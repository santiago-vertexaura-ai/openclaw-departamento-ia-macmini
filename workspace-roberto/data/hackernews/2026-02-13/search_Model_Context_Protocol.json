{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"hacker27369"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["model","context","protocol"],"value":"I spent the last few years in High-Frequency Trading. In that world, &quot;probabilistic&quot; automation is a non-starter; if an AI hallucinations or clicks the wrong pixel, it's a compliance disaster.\nMost current AI agents rely on vision (pixels). It\u2019s slow, brittle, and introduces a massive margin of error. I built GodHands to treat desktop apps like structured APIs instead of screenshots. It\u2019s an MCP (<em>Model</em> <em>Context</em> <em>Protocol</em>) server that acts as a deterministic &quot;action layer.&quot;<p>The Approach:<p>Object <em>Model</em> &gt; Pixels: Instead of vision, the engine hooks into application object models directly. It maps data structures dynamically so the automation doesn't break when the UI changes.<p>Architect vs. Builder: The LLM acts only as the Architect (intent). A local execution engine acts as the Builder. The AI never &quot;guesses&quot; the math; it triggers verified code primitives.<p>Local-First: All data processing stays on your machine. The LLM handles the reasoning, but the GodHands engine handles the data locally.<p>The Workflow: It bridges siloed apps into a unified <em>protocol</em>\u2014e.g., scraping a Gmail statement, running a cross-sheet reconciliation in Excel, and piping anomalies to Slack or a Calendar invite.<p>I\u2019m looking for technical feedback on this architecture, specifically the deterministic vs. probabilistic trade-off in RPA. I\u2019ll be around to answer questions about the implementation or the MCP layer. If you are interested in trying out our beta version, please reach out to us at founders@godhands.dev"},"title":{"matchLevel":"none","matchedWords":[],"value":"GodHands \u2013 Deterministic Desktop Automation via MCP"}},"_tags":["story","author_hacker27369","story_46996023","ask_hn"],"author":"hacker27369","created_at":"2026-02-12T22:13:58Z","created_at_i":1770934438,"num_comments":0,"objectID":"46996023","points":1,"story_id":46996023,"story_text":"I spent the last few years in High-Frequency Trading. In that world, &quot;probabilistic&quot; automation is a non-starter; if an AI hallucinations or clicks the wrong pixel, it&#x27;s a compliance disaster.\nMost current AI agents rely on vision (pixels). It\u2019s slow, brittle, and introduces a massive margin of error. I built GodHands to treat desktop apps like structured APIs instead of screenshots. It\u2019s an MCP (Model Context Protocol) server that acts as a deterministic &quot;action layer.&quot;<p>The Approach:<p>Object Model &gt; Pixels: Instead of vision, the engine hooks into application object models directly. It maps data structures dynamically so the automation doesn&#x27;t break when the UI changes.<p>Architect vs. Builder: The LLM acts only as the Architect (intent). A local execution engine acts as the Builder. The AI never &quot;guesses&quot; the math; it triggers verified code primitives.<p>Local-First: All data processing stays on your machine. The LLM handles the reasoning, but the GodHands engine handles the data locally.<p>The Workflow: It bridges siloed apps into a unified protocol\u2014e.g., scraping a Gmail statement, running a cross-sheet reconciliation in Excel, and piping anomalies to Slack or a Calendar invite.<p>I\u2019m looking for technical feedback on this architecture, specifically the deterministic vs. probabilistic trade-off in RPA. I\u2019ll be around to answer questions about the implementation or the MCP layer. If you are interested in trying out our beta version, please reach out to us at founders@godhands.dev","title":"GodHands \u2013 Deterministic Desktop Automation via MCP","updated_at":"2026-02-13T04:43:48Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"huseyinsari"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["model","context","protocol"],"value":"Hi HN,<p>I built NoSpamPro because I was tired of &quot;spam blockers&quot; that required uploading my entire <em>contact</em> list or call logs to their servers just to work. Most current solutions are essentially data-harvesting tools disguised as security apps.<p>NoSpamPro is an Android SMS/Call blocker designed with a privacy-first, local-only philosophy. It uses a hybrid 8-layer filtering system to handle threats without compromising your data.<p>How it works (The Engineering Side):<p>95% of the filtering happens on-device. The app follows a decision-tree logic to minimize battery impact and maximize privacy:<p>Local Heuristics: We use Room DB for instant lookups against known malicious prefixes, B-Codes (Business Sender IDs like B043), and user-defined Regex patterns.<p>B-Code Registry: A specialized layer that verifies official business codes to detect &quot;spoofing&quot; attempts common in banking fraud.<p>The AI Layer (Optional): This is where it gets interesting. If the local layers are inconclusive (e.g., a &quot;Likely Spam&quot; score), the app can use Gemini AI for contextual analysis.<p>Privacy <em>Protocol</em>: Before any text leaves the device, it is anonymized. We only ask the <em>model</em>: &quot;Is this intent-based spam?&quot;.<p>Zero-Log: We implemented a <em>protocol</em> where analysis results are returned and the source data is immediately vaporized. No logs, no archives.<p>Technical Stack:<p>- Language: 100% Kotlin.\n- UI: Jetpack Compose (Material 3).\n- Storage: Room Persistence Library.\n- Async: Kotlin Coroutines &amp; Flow.<p>Why use this instead of the system default? While Google\u2019s default filter is decent, it often misses localized scams (like country-specific betting spam) and has limited customization. NoSpamPro gives you &quot;Advanced Protection&quot; toggles like:<p>Burst Protection: Detecting &quot;SMS Bomb&quot; attacks and silencing them.<p>De-shortening URLs: Analyzing the final destination of bit.ly/t.co links within the app's &quot;Ghost Browser.&quot;<p>International Guard: Blocking specific high-risk area codes while whitelisting others.<p>The app is currently available for Android. I'm looking for feedback specifically on the filtering logic and any edge cases in call screening you might have encountered.<p>Google Play: <a href=\"https://play.google.com/store/apps/details?id=com.byauth.nospampro&amp;pli=1\">https://play.google.com/store/apps/details?id=com.byauth.nos...</a><p>I'll be around to answer any technical questions about the implementation or the privacy <em>model</em>!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: NoSpamPro \u2013 AI Spam Blocker for Android (Privacy-First)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://byauth.com/en"}},"_tags":["story","author_huseyinsari","story_46988195","show_hn"],"author":"huseyinsari","created_at":"2026-02-12T12:52:22Z","created_at_i":1770900742,"num_comments":0,"objectID":"46988195","points":1,"story_id":46988195,"story_text":"Hi HN,<p>I built NoSpamPro because I was tired of &quot;spam blockers&quot; that required uploading my entire contact list or call logs to their servers just to work. Most current solutions are essentially data-harvesting tools disguised as security apps.<p>NoSpamPro is an Android SMS&#x2F;Call blocker designed with a privacy-first, local-only philosophy. It uses a hybrid 8-layer filtering system to handle threats without compromising your data.<p>How it works (The Engineering Side):<p>95% of the filtering happens on-device. The app follows a decision-tree logic to minimize battery impact and maximize privacy:<p>Local Heuristics: We use Room DB for instant lookups against known malicious prefixes, B-Codes (Business Sender IDs like B043), and user-defined Regex patterns.<p>B-Code Registry: A specialized layer that verifies official business codes to detect &quot;spoofing&quot; attempts common in banking fraud.<p>The AI Layer (Optional): This is where it gets interesting. If the local layers are inconclusive (e.g., a &quot;Likely Spam&quot; score), the app can use Gemini AI for contextual analysis.<p>Privacy Protocol: Before any text leaves the device, it is anonymized. We only ask the model: &quot;Is this intent-based spam?&quot;.<p>Zero-Log: We implemented a protocol where analysis results are returned and the source data is immediately vaporized. No logs, no archives.<p>Technical Stack:<p>- Language: 100% Kotlin.\n- UI: Jetpack Compose (Material 3).\n- Storage: Room Persistence Library.\n- Async: Kotlin Coroutines &amp; Flow.<p>Why use this instead of the system default? While Google\u2019s default filter is decent, it often misses localized scams (like country-specific betting spam) and has limited customization. NoSpamPro gives you &quot;Advanced Protection&quot; toggles like:<p>Burst Protection: Detecting &quot;SMS Bomb&quot; attacks and silencing them.<p>De-shortening URLs: Analyzing the final destination of bit.ly&#x2F;t.co links within the app&#x27;s &quot;Ghost Browser.&quot;<p>International Guard: Blocking specific high-risk area codes while whitelisting others.<p>The app is currently available for Android. I&#x27;m looking for feedback specifically on the filtering logic and any edge cases in call screening you might have encountered.<p>Google Play: <a href=\"https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.byauth.nospampro&amp;pli=1\">https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.byauth.nos...</a><p>I&#x27;ll be around to answer any technical questions about the implementation or the privacy model!","title":"Show HN: NoSpamPro \u2013 AI Spam Blocker for Android (Privacy-First)","updated_at":"2026-02-12T12:52:31Z","url":"https://byauth.com/en"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"lucamoretti"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["model","context","protocol"],"value":"Comprehensive Secrets Management Guide for MCP (<em>Model</em> <em>Context</em> <em>Protocol</em>) Servers"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/rsdouglas/janee/blob/main/docs/mcp-secrets-guide.md"}},"_tags":["story","author_lucamoretti","story_46988171"],"author":"lucamoretti","children":[46988172],"created_at":"2026-02-12T12:50:50Z","created_at_i":1770900650,"num_comments":1,"objectID":"46988171","points":1,"story_id":46988171,"title":"Comprehensive Secrets Management Guide for MCP (Model Context Protocol) Servers","updated_at":"2026-02-12T12:52:31Z","url":"https://github.com/rsdouglas/janee/blob/main/docs/mcp-secrets-guide.md"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"pstryder"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["model","context","protocol"],"value":"I built MemoryGate because I kept watching <em>context</em> vanish.\nI run multiple AI agents across Claude, ChatGPT, and Cursor. Every time a <em>model</em> updated, a platform changed its API, or a <em>context</em> window rolled over \u2014 everything the agent had learned was gone. Preferences, decisions, project history, relationship <em>context</em>. Just... wiped.\nThe fundamental problem: AI memory is trapped inside the platform that hosts the conversation. Your agent's knowledge dies with the session, the <em>model</em> version, or the provider's business decisions.\nMemoryGate is a persistent semantic memory layer that sits outside any single <em>model</em> or platform. It connects via MCP (<em>Model</em> <em>Context</em> <em>Protocol</em>), so any MCP-compatible agent \u2014 Claude Desktop, ChatGPT, Cursor, custom agents \u2014 can store and retrieve memories through a shared, durable knowledge store.\nWhat it actually does:<p>Semantic memory with vector embeddings \u2014 recall by meaning, not keywords\nConfidence-weighted observations that strengthen or decay based on evidence\nAutomatic lifecycle management \u2014 high-signal stays hot, noise fades to cold storage\nAppend-only architecture \u2014 memories are never overwritten, only superseded with lineage\nKnowledge graphs linking observations, patterns, concepts, and documents\nMulti-tenant with org isolation, roles, and shared memory stores\nOAuth 2.0, audit logs, rate limiting \u2014 production infrastructure, not a toy<p>What it's not:<p>Not a RAG pipeline. MemoryGate stores what the agent learns from interaction, not document chunks.\nNot prompt injection. Memory lives at the infrastructure layer, not stuffed into system prompts.\nNot tied to any <em>model</em> or provider. Switch from Claude to ChatGPT to a local <em>model</em> \u2014 memory persists.<p>Stack: Python/FastAPI, PostgreSQL + pgvector, Redis, deployed on Railway. MCP-native integration \u2014 your agent gets 33 memory tools on connection.\nThe real pitch: Platforms die. Models get deprecated. <em>Context</em> windows roll over. Your AI's memory shouldn't be hostage to your AI's provider.\nOpen source (Apache 2.0), self-hostable, with a hosted SaaS option if you don't want to run infrastructure.<p>GitHub: <a href=\"https://github.com/PStryder/MemoryGate\" rel=\"nofollow\">https://github.com/PStryder/MemoryGate</a>\nSaaS: <a href=\"https://memorygate.ai\" rel=\"nofollow\">https://memorygate.ai</a>\nDocs: <a href=\"https://memorygate.ai/docs/\" rel=\"nofollow\">https://memorygate.ai/docs/</a><p>I'm a solo founder \u2014 built this after leaving a decade in enterprise solutions engineering. Happy to answer questions about the architecture, the MCP integration, or why I think persistent memory is the missing infrastructure layer for AI agents."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: MemoryGate \u2013 Open-source persistent memory for AI agents via MCP"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://www.memorygate.ai"}},"_tags":["story","author_pstryder","story_46981840","show_hn"],"author":"pstryder","created_at":"2026-02-11T22:05:29Z","created_at_i":1770847529,"num_comments":0,"objectID":"46981840","points":1,"story_id":46981840,"story_text":"I built MemoryGate because I kept watching context vanish.\nI run multiple AI agents across Claude, ChatGPT, and Cursor. Every time a model updated, a platform changed its API, or a context window rolled over \u2014 everything the agent had learned was gone. Preferences, decisions, project history, relationship context. Just... wiped.\nThe fundamental problem: AI memory is trapped inside the platform that hosts the conversation. Your agent&#x27;s knowledge dies with the session, the model version, or the provider&#x27;s business decisions.\nMemoryGate is a persistent semantic memory layer that sits outside any single model or platform. It connects via MCP (Model Context Protocol), so any MCP-compatible agent \u2014 Claude Desktop, ChatGPT, Cursor, custom agents \u2014 can store and retrieve memories through a shared, durable knowledge store.\nWhat it actually does:<p>Semantic memory with vector embeddings \u2014 recall by meaning, not keywords\nConfidence-weighted observations that strengthen or decay based on evidence\nAutomatic lifecycle management \u2014 high-signal stays hot, noise fades to cold storage\nAppend-only architecture \u2014 memories are never overwritten, only superseded with lineage\nKnowledge graphs linking observations, patterns, concepts, and documents\nMulti-tenant with org isolation, roles, and shared memory stores\nOAuth 2.0, audit logs, rate limiting \u2014 production infrastructure, not a toy<p>What it&#x27;s not:<p>Not a RAG pipeline. MemoryGate stores what the agent learns from interaction, not document chunks.\nNot prompt injection. Memory lives at the infrastructure layer, not stuffed into system prompts.\nNot tied to any model or provider. Switch from Claude to ChatGPT to a local model \u2014 memory persists.<p>Stack: Python&#x2F;FastAPI, PostgreSQL + pgvector, Redis, deployed on Railway. MCP-native integration \u2014 your agent gets 33 memory tools on connection.\nThe real pitch: Platforms die. Models get deprecated. Context windows roll over. Your AI&#x27;s memory shouldn&#x27;t be hostage to your AI&#x27;s provider.\nOpen source (Apache 2.0), self-hostable, with a hosted SaaS option if you don&#x27;t want to run infrastructure.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;PStryder&#x2F;MemoryGate\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;PStryder&#x2F;MemoryGate</a>\nSaaS: <a href=\"https:&#x2F;&#x2F;memorygate.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;memorygate.ai</a>\nDocs: <a href=\"https:&#x2F;&#x2F;memorygate.ai&#x2F;docs&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;memorygate.ai&#x2F;docs&#x2F;</a><p>I&#x27;m a solo founder \u2014 built this after leaving a decade in enterprise solutions engineering. Happy to answer questions about the architecture, the MCP integration, or why I think persistent memory is the missing infrastructure layer for AI agents.","title":"Show HN: MemoryGate \u2013 Open-source persistent memory for AI agents via MCP","updated_at":"2026-02-11T22:08:46Z","url":"https://www.memorygate.ai"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["model","context","protocol"],"value":"Hi HN,<p>I built PolyMCP, an open-source framework around the <em>Model</em> <em>Context</em> <em>Protocol</em> (MCP) that lets you expose existing Python functions as AI-callable tools \u2014 without rewriting them or adopting a custom SDK.<p>The goal is simple:\nIf you already have working Python code, you should be able to make it accessible to LLM agents in minutes.<p>What it does<p>PolyMCP introspects regular Python functions and exposes them as MCP tools automatically. No decorators required. No framework lock-in.<p>It grew into a small ecosystem:\n \u2022 PolyMCP (core) \u2013 Turn Python functions into MCP tools\n \u2022 PolyMCP Inspector \u2013 A visual UI to browse, test, and debug MCP servers\n \u2022 MCP SDK Apps \u2013 A lightweight way to build AI-powered apps with tools + UI resources<p>Why I built this<p>While experimenting with MCP and AI agents, I found that integrating existing codebases was often the painful part.\nMost solutions require rewriting logic around a specific SDK or heavily annotating functions.<p>PolyMCP focuses on:\n \u2022 Minimal intrusion into existing code\n \u2022 Clean separation between business logic and AI tooling\n \u2022 Easy debugging via a visual inspector<p>Example use cases\n \u2022 Expose internal APIs or legacy scripts to LLM agents\n \u2022 Automate operational workflows\n \u2022 Build internal copilots over real systems\n \u2022 Prototype AI agents that interact with production services<p>Works with OpenAI, Anthropic, and Ollama (including local models).<p>It\u2019s still evolving and I\u2019m actively iterating.\nI\u2019d really appreciate feedback \u2014 especially from people building agents or experimenting with MCP in production environments.<p>GitHub:\n \u2022 Core: <a href=\"https://github.com/poly-mcp/PolyMCP\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP</a>\n \u2022 Inspector: <a href=\"https://github.com/poly-mcp/PolyMCP-Inspector\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-Inspector</a>\n \u2022 SDK Apps: <a href=\"https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps</a><p>Happy to answer technical questions."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: PolyMCP \u2013 Expose Python functions as MCP tools"}},"_tags":["story","author_justvugg","story_46980134","show_hn"],"author":"justvugg","created_at":"2026-02-11T20:06:17Z","created_at_i":1770840377,"num_comments":0,"objectID":"46980134","points":2,"story_id":46980134,"story_text":"Hi HN,<p>I built PolyMCP, an open-source framework around the Model Context Protocol (MCP) that lets you expose existing Python functions as AI-callable tools \u2014 without rewriting them or adopting a custom SDK.<p>The goal is simple:\nIf you already have working Python code, you should be able to make it accessible to LLM agents in minutes.<p>What it does<p>PolyMCP introspects regular Python functions and exposes them as MCP tools automatically. No decorators required. No framework lock-in.<p>It grew into a small ecosystem:\n \u2022 PolyMCP (core) \u2013 Turn Python functions into MCP tools\n \u2022 PolyMCP Inspector \u2013 A visual UI to browse, test, and debug MCP servers\n \u2022 MCP SDK Apps \u2013 A lightweight way to build AI-powered apps with tools + UI resources<p>Why I built this<p>While experimenting with MCP and AI agents, I found that integrating existing codebases was often the painful part.\nMost solutions require rewriting logic around a specific SDK or heavily annotating functions.<p>PolyMCP focuses on:\n \u2022 Minimal intrusion into existing code\n \u2022 Clean separation between business logic and AI tooling\n \u2022 Easy debugging via a visual inspector<p>Example use cases\n \u2022 Expose internal APIs or legacy scripts to LLM agents\n \u2022 Automate operational workflows\n \u2022 Build internal copilots over real systems\n \u2022 Prototype AI agents that interact with production services<p>Works with OpenAI, Anthropic, and Ollama (including local models).<p>It\u2019s still evolving and I\u2019m actively iterating.\nI\u2019d really appreciate feedback \u2014 especially from people building agents or experimenting with MCP in production environments.<p>GitHub:\n \u2022 Core: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP</a>\n \u2022 Inspector: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector</a>\n \u2022 SDK Apps: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps</a><p>Happy to answer technical questions.","title":"Show HN: PolyMCP \u2013 Expose Python functions as MCP tools","updated_at":"2026-02-11T20:47:00Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"shatzakis"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["model","context","protocol"],"value":"Hi Everyone,<p>I\u2019m an independent researcher (and professionally, the Global Director of Research at Reink Media) looking for an endorsement for the cs.AI (Computer Science/Artificial Intelligence) category on arXiv.<p>The <em>Context</em> I didn't start by writing a paper; I started by building a system. Over the last year, I developed a production-grade <em>Model</em> <em>Context</em> <em>Protocol</em> (MCP) server for the forex market. It\u2019s currently live with 45 distinct tools and renders 28 different dynamic widgets across Claude, ChatGPT, and a custom web app.<p>The Paper The paper is titled <em>Protocol</em>-Constrained Agentic Systems: A Neuro-Symbolic Architecture for Hallucination-Resistant Financial Execution.<p>My core argument is that in high-stakes domains like finance, we cannot rely on LLMs to be &quot;smart enough&quot; to avoid critical errors. Instead, I propose an architecture that uses MCP as a &quot;hallucination firewall.&quot; This strictly decouples the probabilistic layer (the LLM parsing intent) from the deterministic layer (the tool executing the trade). It effectively treats the <em>protocol</em> schema as a type system for agent actions, guaranteeing by construction that invalid tool calls cannot reach the execution layer.<p>You can read the full paper and see the architecture in the PDF here: https://www.stevenhatzakis.com/research/<em>protocol</em>-constrained-agentic-systems<p>The Request If you are a registered endorser for cs.AI and find this work relevant, I would appreciate your support so I can submit this paper on arXiv.org.<p>Endorsement Code: LZRTFH Link: https://arxiv.org/auth/endorse?x=LZRTFH<p>Thanks for your time.<p>Steven Hatzakis"},"title":{"matchLevel":"none","matchedWords":[],"value":"ArXiv Endorsement for Paper on Neuro-Symbolic Architecture for Financial Agents"}},"_tags":["story","author_shatzakis","story_46975121","ask_hn"],"author":"shatzakis","children":[46975342],"created_at":"2026-02-11T14:09:47Z","created_at_i":1770818987,"num_comments":2,"objectID":"46975121","points":2,"story_id":46975121,"story_text":"Hi Everyone,<p>I\u2019m an independent researcher (and professionally, the Global Director of Research at Reink Media) looking for an endorsement for the cs.AI (Computer Science&#x2F;Artificial Intelligence) category on arXiv.<p>The Context I didn&#x27;t start by writing a paper; I started by building a system. Over the last year, I developed a production-grade Model Context Protocol (MCP) server for the forex market. It\u2019s currently live with 45 distinct tools and renders 28 different dynamic widgets across Claude, ChatGPT, and a custom web app.<p>The Paper The paper is titled Protocol-Constrained Agentic Systems: A Neuro-Symbolic Architecture for Hallucination-Resistant Financial Execution.<p>My core argument is that in high-stakes domains like finance, we cannot rely on LLMs to be &quot;smart enough&quot; to avoid critical errors. Instead, I propose an architecture that uses MCP as a &quot;hallucination firewall.&quot; This strictly decouples the probabilistic layer (the LLM parsing intent) from the deterministic layer (the tool executing the trade). It effectively treats the protocol schema as a type system for agent actions, guaranteeing by construction that invalid tool calls cannot reach the execution layer.<p>You can read the full paper and see the architecture in the PDF here: https:&#x2F;&#x2F;www.stevenhatzakis.com&#x2F;research&#x2F;protocol-constrained-agentic-systems<p>The Request If you are a registered endorser for cs.AI and find this work relevant, I would appreciate your support so I can submit this paper on arXiv.org.<p>Endorsement Code: LZRTFH Link: https:&#x2F;&#x2F;arxiv.org&#x2F;auth&#x2F;endorse?x=LZRTFH<p>Thanks for your time.<p>Steven Hatzakis","title":"ArXiv Endorsement for Paper on Neuro-Symbolic Architecture for Financial Agents","updated_at":"2026-02-12T00:54:58Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["model","context","protocol"],"value":"I built PolyMCP, an open-source framework around the <em>Model</em> <em>Context</em> <em>Protocol</em> (MCP) that lets you turn any existing Python function into an MCP tool usable by AI agents \u2014 with no rewrites, no glue code, no custom wrappers.<p>Over the last weeks, PolyMCP has grown into a small ecosystem:\n \u2022 PolyMCP (core) \u2013 expose Python functions as MCP tools\n \u2022 PolyMCP Inspector \u2013 visual UI to explore, test, and debug MCP servers\n \u2022 PolyMCP MCP SDK Apps \u2013 build MCP-powered apps with tools + UI resources<p>\u2e3b<p>1) Turn any Python function into an MCP tool<p>Basic example:<p>from polymcp import expose_tools_http<p>def add(a: int, b: int) -&gt; int:\n    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;\n    return a + b<p>app = expose_tools_http(\n    tools=[add],\n    title=&quot;Math Tools&quot;\n)<p>Run it:<p>uvicorn server_mcp:app --reload<p>Now add is an MCP-compliant tool that any AI agent can discover and call.<p>No decorators, no schema files, no agent-specific SDKs.<p>\u2e3b<p>2) Real APIs, not toy examples<p>Existing API code works as-is:<p>import requests\nfrom polymcp import expose_tools_http<p>def get_weather(city: str):\n    &quot;&quot;&quot;Return current weather data for a city&quot;&quot;&quot;\n    response = requests.get(\n        f&quot;<a href=\"https://api.weatherapi.com/v1/current.json?q={city}\" rel=\"nofollow\">https://api.weatherapi.com/v1/current.json?q={city}</a>&quot;\n    )\n    return response.json()<p>app = expose_tools_http([get_weather], title=&quot;Weather Tools&quot;)<p>Agents can now call:<p>get_weather(&quot;London&quot;)<p>and receive real-time data.<p>\u2e3b<p>3) Business &amp; internal workflows<p>Example: internal reporting logic reused directly by agents.<p>import pandas as pd\nfrom polymcp import expose_tools_http<p>def calculate_commissions(sales_data: list[dict]):\n    &quot;&quot;&quot;Calculate sales commissions from sales data&quot;&quot;&quot;\n    df = pd.DataFrame(sales_data)\n    df[&quot;commission&quot;] = df[&quot;sales_amount&quot;] * 0.05\n    return df.to_dict(orient=&quot;records&quot;)<p>app = expose_tools_http([calculate_commissions], title=&quot;Business Tools&quot;)<p>No rewriting legacy logic.<p>4) PolyMCP Inspector (visual debugging)<p>To make MCP development usable in practice, I added PolyMCP Inspector:\n \u2022 Visual UI to browse tools, prompts, and resources\n \u2022 Call MCP tools interactively\n \u2022 Inspect schemas, inputs, outputs, and errors\n \u2022 Multi-server support (HTTP + stdio)\n \u2022 Built-in chat playground (OpenAI / Anthropic / Ollama)<p>Think \u201cPostman + DevTools\u201d for MCP servers.<p>Repo: <a href=\"https://github.com/poly-mcp/PolyMCP-Inspector\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-Inspector</a><p>\u2e3b<p>5) MCP SDK Apps (tools + UI)<p>The latest addition is PolyMCP MCP SDK Apps:\n \u2022 Build MCP apps, not just tools\n \u2022 Expose:\n \u2022 tools\n \u2022 UI resources (HTML/JS dashboards)\n \u2022 app-level workflows\n \u2022 Let agents interact with both tools and UIs<p>This is useful for:\n \u2022 internal copilots\n \u2022 ops dashboards\n \u2022 support tools\n \u2022 enterprise AI frontends<p>Repo: <a href=\"https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps</a><p>\u2e3b<p>Why this matters (especially for companies)\n \u2022 Reuse existing code immediately (scripts, APIs, internal libs)\n \u2022 Standard MCP interface instead of vendor-specific agent SDKs\n \u2022 Multiple tools, one server\n \u2022 Agent-driven orchestration, not hardcoded flows\n \u2022 Faster AI adoption without refactoring everything<p>PolyMCP treats AI agents as clients of your software, not magic wrappers around it.<p>\u2e3b<p>Repos\n \u2022 Core framework: <a href=\"https://github.com/poly-mcp/PolyMCP\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP</a>\n \u2022 Inspector UI: <a href=\"https://github.com/poly-mcp/PolyMCP-Inspector\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-Inspector</a>\n \u2022 MCP SDK Apps: <a href=\"https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps</a><p>Happy to hear feedback from people building MCP servers, agents, or internal AI tools."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: PolyMCP \u2013 AI-Callable Python and TS Tools with Inspector and Apps"}},"_tags":["story","author_justvugg","story_46966000","show_hn"],"author":"justvugg","created_at":"2026-02-10T20:03:43Z","created_at_i":1770753823,"num_comments":0,"objectID":"46966000","points":2,"story_id":46966000,"story_text":"I built PolyMCP, an open-source framework around the Model Context Protocol (MCP) that lets you turn any existing Python function into an MCP tool usable by AI agents \u2014 with no rewrites, no glue code, no custom wrappers.<p>Over the last weeks, PolyMCP has grown into a small ecosystem:\n \u2022 PolyMCP (core) \u2013 expose Python functions as MCP tools\n \u2022 PolyMCP Inspector \u2013 visual UI to explore, test, and debug MCP servers\n \u2022 PolyMCP MCP SDK Apps \u2013 build MCP-powered apps with tools + UI resources<p>\u2e3b<p>1) Turn any Python function into an MCP tool<p>Basic example:<p>from polymcp import expose_tools_http<p>def add(a: int, b: int) -&gt; int:\n    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;\n    return a + b<p>app = expose_tools_http(\n    tools=[add],\n    title=&quot;Math Tools&quot;\n)<p>Run it:<p>uvicorn server_mcp:app --reload<p>Now add is an MCP-compliant tool that any AI agent can discover and call.<p>No decorators, no schema files, no agent-specific SDKs.<p>\u2e3b<p>2) Real APIs, not toy examples<p>Existing API code works as-is:<p>import requests\nfrom polymcp import expose_tools_http<p>def get_weather(city: str):\n    &quot;&quot;&quot;Return current weather data for a city&quot;&quot;&quot;\n    response = requests.get(\n        f&quot;<a href=\"https:&#x2F;&#x2F;api.weatherapi.com&#x2F;v1&#x2F;current.json?q={city}\" rel=\"nofollow\">https:&#x2F;&#x2F;api.weatherapi.com&#x2F;v1&#x2F;current.json?q={city}</a>&quot;\n    )\n    return response.json()<p>app = expose_tools_http([get_weather], title=&quot;Weather Tools&quot;)<p>Agents can now call:<p>get_weather(&quot;London&quot;)<p>and receive real-time data.<p>\u2e3b<p>3) Business &amp; internal workflows<p>Example: internal reporting logic reused directly by agents.<p>import pandas as pd\nfrom polymcp import expose_tools_http<p>def calculate_commissions(sales_data: list[dict]):\n    &quot;&quot;&quot;Calculate sales commissions from sales data&quot;&quot;&quot;\n    df = pd.DataFrame(sales_data)\n    df[&quot;commission&quot;] = df[&quot;sales_amount&quot;] * 0.05\n    return df.to_dict(orient=&quot;records&quot;)<p>app = expose_tools_http([calculate_commissions], title=&quot;Business Tools&quot;)<p>No rewriting legacy logic.<p>4) PolyMCP Inspector (visual debugging)<p>To make MCP development usable in practice, I added PolyMCP Inspector:\n \u2022 Visual UI to browse tools, prompts, and resources\n \u2022 Call MCP tools interactively\n \u2022 Inspect schemas, inputs, outputs, and errors\n \u2022 Multi-server support (HTTP + stdio)\n \u2022 Built-in chat playground (OpenAI &#x2F; Anthropic &#x2F; Ollama)<p>Think \u201cPostman + DevTools\u201d for MCP servers.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector</a><p>\u2e3b<p>5) MCP SDK Apps (tools + UI)<p>The latest addition is PolyMCP MCP SDK Apps:\n \u2022 Build MCP apps, not just tools\n \u2022 Expose:\n \u2022 tools\n \u2022 UI resources (HTML&#x2F;JS dashboards)\n \u2022 app-level workflows\n \u2022 Let agents interact with both tools and UIs<p>This is useful for:\n \u2022 internal copilots\n \u2022 ops dashboards\n \u2022 support tools\n \u2022 enterprise AI frontends<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps</a><p>\u2e3b<p>Why this matters (especially for companies)\n \u2022 Reuse existing code immediately (scripts, APIs, internal libs)\n \u2022 Standard MCP interface instead of vendor-specific agent SDKs\n \u2022 Multiple tools, one server\n \u2022 Agent-driven orchestration, not hardcoded flows\n \u2022 Faster AI adoption without refactoring everything<p>PolyMCP treats AI agents as clients of your software, not magic wrappers around it.<p>\u2e3b<p>Repos\n \u2022 Core framework: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP</a>\n \u2022 Inspector UI: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector</a>\n \u2022 MCP SDK Apps: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps</a><p>Happy to hear feedback from people building MCP servers, agents, or internal AI tools.","title":"Show HN: PolyMCP \u2013 AI-Callable Python and TS Tools with Inspector and Apps","updated_at":"2026-02-11T03:47:55Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mattv8"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["model","context","protocol"],"value":"RAGtime is a self-hosted MCP server and FAISS/PGVector manager that lets AI assistants run real operations on your infrastructure: SSH commands, SQL queries through SSH tunnels, git repo indexing, filesystem searches. It connects Claude, OpenAI, or Ollama to your environment via both MCP <em>protocol</em> and OpenAI-compatible chat completions API.<p>I originally built this over Christmas break as a &quot;self-serve business intelligence&quot; tool to stop fielding repetitive coworker questions (&quot;write me a query for X,&quot; &quot;where's the logic for Y?&quot;) but now it's morphed into a dev tool as well. I couldn't find anything that centralized these tools and served them over chat in one place.<p>I've been using it daily (via MCP) for work and it's been such a huge development accelerator for me. I want to share what I've built with the community, get feedback, or if you wish, contributions. Happy to answer questions about the architecture or use cases.<p>(You can stop here, unless you want more technical details...)<p>Tools: The agent gets access to configurable tools you define: SSH connections to servers (run commands, check logs, restart services), database queries via SSH tunnels (PostgreSQL, MySQL, MSSQL) with parameterized queries to prevent injection, and vector search over your indexed content. Each tool is defined in a config with connection details, and you can enable/disable them per use case. The database tools return structured results the LLM can reason about. SSH tools stream output for long-running commands. There's also a Python REPL tool for data manipulation when the LLM needs to transform query results.<p>On the RAG/indexing side: Chunking uses Chonkie's CodeChunker with Magika (Google's ML <em>model</em>) for automatic language detection, then tree-sitter for AST-aware splitting that respects semantic boundaries (functions, classes, blocks). Each code chunk gets a header with file path and import <em>context</em> so the LLM knows where it came from. Retrieval uses MMR (Maximal Marginal Relevance) to reduce near-duplicate results, balancing relevance with diversity via a configurable lambda parameter. FAISS indexes are portable and can be exported."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Self-hosted MCP server for SQL, SSH, and FAISS indexing"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/mattv8/ragtime"}},"_tags":["story","author_mattv8","story_46964772","show_hn"],"author":"mattv8","created_at":"2026-02-10T18:42:37Z","created_at_i":1770748957,"num_comments":0,"objectID":"46964772","points":3,"story_id":46964772,"story_text":"RAGtime is a self-hosted MCP server and FAISS&#x2F;PGVector manager that lets AI assistants run real operations on your infrastructure: SSH commands, SQL queries through SSH tunnels, git repo indexing, filesystem searches. It connects Claude, OpenAI, or Ollama to your environment via both MCP protocol and OpenAI-compatible chat completions API.<p>I originally built this over Christmas break as a &quot;self-serve business intelligence&quot; tool to stop fielding repetitive coworker questions (&quot;write me a query for X,&quot; &quot;where&#x27;s the logic for Y?&quot;) but now it&#x27;s morphed into a dev tool as well. I couldn&#x27;t find anything that centralized these tools and served them over chat in one place.<p>I&#x27;ve been using it daily (via MCP) for work and it&#x27;s been such a huge development accelerator for me. I want to share what I&#x27;ve built with the community, get feedback, or if you wish, contributions. Happy to answer questions about the architecture or use cases.<p>(You can stop here, unless you want more technical details...)<p>Tools: The agent gets access to configurable tools you define: SSH connections to servers (run commands, check logs, restart services), database queries via SSH tunnels (PostgreSQL, MySQL, MSSQL) with parameterized queries to prevent injection, and vector search over your indexed content. Each tool is defined in a config with connection details, and you can enable&#x2F;disable them per use case. The database tools return structured results the LLM can reason about. SSH tools stream output for long-running commands. There&#x27;s also a Python REPL tool for data manipulation when the LLM needs to transform query results.<p>On the RAG&#x2F;indexing side: Chunking uses Chonkie&#x27;s CodeChunker with Magika (Google&#x27;s ML model) for automatic language detection, then tree-sitter for AST-aware splitting that respects semantic boundaries (functions, classes, blocks). Each code chunk gets a header with file path and import context so the LLM knows where it came from. Retrieval uses MMR (Maximal Marginal Relevance) to reduce near-duplicate results, balancing relevance with diversity via a configurable lambda parameter. FAISS indexes are portable and can be exported.","title":"Show HN: Self-hosted MCP server for SQL, SSH, and FAISS indexing","updated_at":"2026-02-10T18:57:24Z","url":"https://github.com/mattv8/ragtime"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"aytuakarlar"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["model","context"],"value":"Hi HN, I'm Aytug, the creator of CSL-Core.<p>We built this because we realized that &quot;prompt engineering&quot; isn't enough for critical AI systems (like in finance or governance). You can't just ask an LLM nicely not to delete a database\u2014you need a runtime guarantee.<p>CSL-Core is a policy language designed to bring &quot;Policy-as-Code&quot; to AI agents.<p>Instead of relying on the <em>model</em>'s probabilistic nature, CSL enforces constraints that are:<p>1. Formally Verified: Policies are compiled into Z3 constraints to mathematically prove they have no logical conflicts or loopholes.<p>2. Deterministic: The checks happen in a separate runtime engine, independent of the LLM's <em>context</em> window.<p>3. <em>Model</em> Agnostic: It acts as a firewall between the LLM and your tools/API.<p>It's currently in Alpha (v0.2). Currently working on TLA+ specifications for the dual formal verification engine and governance architecture because we believe AI safety needs mathematical rigor.<p>I'd appreciate any feedback on the DSL syntax and our verification approach."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: CSL-Core \u2013 Formally Verified Neuro-Symbolic Safety Engine for AI"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["protocol"],"value":"https://github.com/Chimera-<em>Protocol</em>/csl-core"}},"_tags":["story","author_aytuakarlar","story_46963250","show_hn"],"author":"aytuakarlar","children":[46963343,46963924,46963300],"created_at":"2026-02-10T17:18:21Z","created_at_i":1770743901,"num_comments":5,"objectID":"46963250","points":2,"story_id":46963250,"story_text":"Hi HN, I&#x27;m Aytug, the creator of CSL-Core.<p>We built this because we realized that &quot;prompt engineering&quot; isn&#x27;t enough for critical AI systems (like in finance or governance). You can&#x27;t just ask an LLM nicely not to delete a database\u2014you need a runtime guarantee.<p>CSL-Core is a policy language designed to bring &quot;Policy-as-Code&quot; to AI agents.<p>Instead of relying on the model&#x27;s probabilistic nature, CSL enforces constraints that are:<p>1. Formally Verified: Policies are compiled into Z3 constraints to mathematically prove they have no logical conflicts or loopholes.<p>2. Deterministic: The checks happen in a separate runtime engine, independent of the LLM&#x27;s context window.<p>3. Model Agnostic: It acts as a firewall between the LLM and your tools&#x2F;API.<p>It&#x27;s currently in Alpha (v0.2). Currently working on TLA+ specifications for the dual formal verification engine and governance architecture because we believe AI safety needs mathematical rigor.<p>I&#x27;d appreciate any feedback on the DSL syntax and our verification approach.","title":"Show HN: CSL-Core \u2013 Formally Verified Neuro-Symbolic Safety Engine for AI","updated_at":"2026-02-12T18:02:48Z","url":"https://github.com/Chimera-Protocol/csl-core"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ankit219"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["model","context","protocol"],"value":"GitHub: <a href=\"https://github.com/ClioAI/kw-sdk\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk</a><p>Most AI agent frameworks target code. Write code, run tests, fix errors, repeat. That works because code has a natural verification signal. It works or it doesn't.<p>This SDK treats knowledge work like an engineering problem:<p>Task \u2192 Brief \u2192 Rubric (hidden from executor) \u2192 Work \u2192 Verify \u2192 Fail? \u2192 Retry \u2192 Pass \u2192 Submit<p>The orchestrator coordinates subagents, web search, code execution, and file I/O. then checks its own work against criteria it can't game (the rubric is generated in a separate call and the executor never sees it directly).<p>We originally built this as a harness for RL training on knowledge tasks. The rubric is the reward function. If you're training <em>models</em> on knowledge work, the brief\u2192rubric\u2192execute\u2192verify loop gives you a structured reward signal for tasks that normally don't have one.<p>What makes Knowledge work different from code? (apart from feedback loop)\nI believe there is some functionality missing from today's agents when it comes to knowledge work. I tried to include that in this release. Example:<p>Explore mode: Mapping the solution space, identifying the set level gaps, and giving options.<p>Most agents optimize for a single answer, and end up with a median one. For strategy, design, creative problems, you want to see the options, what are the tradeoffs, and what can you do? Explore mode generates N distinct approaches, each with explicit assumptions and counterfactuals (&quot;this works if X, breaks if Y&quot;). The output ends with set-level gaps ie what angles the entire set missed. The gaps are often more valuable than the takes. I think this is what many of us do on a daily basis, but no agent directly captures it today. See <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/explore_mode.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/explore_...</a> and the output for a sense of how this is different.<p>Checkpointing: With many ai agents and especially multi agent systems, i can see where it went wrong, but cant run inference from same stage. (or you may want multiple explorations once an agent has done some tasks like search and is now looking at ideas). I used this for rollouts a lot, and think its a great feature to run again, or fork from a specific checkpoint.<p>A note on Verification loop:\nThe verify step is where the real leverage is. A <em>model</em> that can accurately assess its own work against a rubric is more valuable than one that generates slightly better first drafts. The rubric makes quality legible \u2014 to the agent, to the human, and potentially to a training signal.<p>Some things i like about this: \n- You can pass a remote execution environment (including your browser as a sandbox) and it would work. It can be docker, e2b, your local env, anything, the <em>model</em> will execute commands in your <em>context</em>, and will iterate based on feedback loop. Code execution is a <em>protocol</em> here.<p>- Tool calling: I realize you don't need complex functions. <em>Models</em> are good at writing terminal code, and can iterate based on feedback, so you can just pass either functions in <em>context</em> and <em>model</em> will execute or you can pass docs and <em>model</em> will write the code. (same as anthropic's programmatic tool calling). Details: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/TOOL_CALLING_GUIDE.md\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/TOOL_CALLING_GUID...</a><p>Lastly, some guides: \n- SDK guide: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/SDK_GUIDE.md\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/SDK_GUIDE.md</a>\n- Extensible. See bizarro example where i add a new mode: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/custom_mode_bizarro.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/custom_m...</a>\n- working with files: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/with_files.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/with_fil...</a> \n- this is simple but i love the csv example: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/csv_research_and_calc.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/csv_rese...</a>\n- remote execution: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/with_custom_executor.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/with_cus...</a><p>And a lot more. This was completely refactored by opus and given the rework, probably would have taken a lot of time to release it.<p>MIT licensed. Would love your feedback."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Open-Source SDK for AI Knowledge Work"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/ClioAI/kw-sdk"}},"_tags":["story","author_ankit219","story_46963026","show_hn"],"author":"ankit219","children":[46963327],"created_at":"2026-02-10T17:06:00Z","created_at_i":1770743160,"num_comments":1,"objectID":"46963026","points":21,"story_id":46963026,"story_text":"GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk</a><p>Most AI agent frameworks target code. Write code, run tests, fix errors, repeat. That works because code has a natural verification signal. It works or it doesn&#x27;t.<p>This SDK treats knowledge work like an engineering problem:<p>Task \u2192 Brief \u2192 Rubric (hidden from executor) \u2192 Work \u2192 Verify \u2192 Fail? \u2192 Retry \u2192 Pass \u2192 Submit<p>The orchestrator coordinates subagents, web search, code execution, and file I&#x2F;O. then checks its own work against criteria it can&#x27;t game (the rubric is generated in a separate call and the executor never sees it directly).<p>We originally built this as a harness for RL training on knowledge tasks. The rubric is the reward function. If you&#x27;re training models on knowledge work, the brief\u2192rubric\u2192execute\u2192verify loop gives you a structured reward signal for tasks that normally don&#x27;t have one.<p>What makes Knowledge work different from code? (apart from feedback loop)\nI believe there is some functionality missing from today&#x27;s agents when it comes to knowledge work. I tried to include that in this release. Example:<p>Explore mode: Mapping the solution space, identifying the set level gaps, and giving options.<p>Most agents optimize for a single answer, and end up with a median one. For strategy, design, creative problems, you want to see the options, what are the tradeoffs, and what can you do? Explore mode generates N distinct approaches, each with explicit assumptions and counterfactuals (&quot;this works if X, breaks if Y&quot;). The output ends with set-level gaps ie what angles the entire set missed. The gaps are often more valuable than the takes. I think this is what many of us do on a daily basis, but no agent directly captures it today. See <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;explore_mode.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;explore_...</a> and the output for a sense of how this is different.<p>Checkpointing: With many ai agents and especially multi agent systems, i can see where it went wrong, but cant run inference from same stage. (or you may want multiple explorations once an agent has done some tasks like search and is now looking at ideas). I used this for rollouts a lot, and think its a great feature to run again, or fork from a specific checkpoint.<p>A note on Verification loop:\nThe verify step is where the real leverage is. A model that can accurately assess its own work against a rubric is more valuable than one that generates slightly better first drafts. The rubric makes quality legible \u2014 to the agent, to the human, and potentially to a training signal.<p>Some things i like about this: \n- You can pass a remote execution environment (including your browser as a sandbox) and it would work. It can be docker, e2b, your local env, anything, the model will execute commands in your context, and will iterate based on feedback loop. Code execution is a protocol here.<p>- Tool calling: I realize you don&#x27;t need complex functions. Models are good at writing terminal code, and can iterate based on feedback, so you can just pass either functions in context and model will execute or you can pass docs and model will write the code. (same as anthropic&#x27;s programmatic tool calling). Details: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;TOOL_CALLING_GUIDE.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;TOOL_CALLING_GUID...</a><p>Lastly, some guides: \n- SDK guide: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;SDK_GUIDE.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;SDK_GUIDE.md</a>\n- Extensible. See bizarro example where i add a new mode: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;custom_mode_bizarro.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;custom_m...</a>\n- working with files: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_files.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_fil...</a> \n- this is simple but i love the csv example: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;csv_research_and_calc.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;csv_rese...</a>\n- remote execution: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_custom_executor.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_cus...</a><p>And a lot more. This was completely refactored by opus and given the rework, probably would have taken a lot of time to release it.<p>MIT licensed. Would love your feedback.","title":"Show HN: Open-Source SDK for AI Knowledge Work","updated_at":"2026-02-11T15:35:15Z","url":"https://github.com/ClioAI/kw-sdk"}],"hitsPerPage":10,"nbHits":524,"nbPages":53,"page":0,"params":"query=Model+Context+Protocol&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":30,"processingTimingsMS":{"_request":{"queue":1,"roundTrip":26},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":10,"scanning":18,"total":29},"total":30},"query":"Model Context Protocol","serverTimeMS":33}
