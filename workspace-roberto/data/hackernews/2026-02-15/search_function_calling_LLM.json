{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"gucduck"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function","llm"],"value":"Hey HN<p>I\u2019ve been thinking a lot about barriers to civic participation. Most people don\u2019t show up to town halls. They don\u2019t respond to traditional surveys. And a lot of online political spaces feel loud, adversarial, or exhausting.<p>So I started wondering what happens if you make participation radically lightweight.<p>That\u2019s where Civie comes in.<p>Each day you answer one to three short civic questions. It takes under a minute. Responses are anonymous. Results are shown only in aggregate. Over time, those responses accumulate into an open dataset anyone can explore.<p>This isn\u2019t scientific polling, and it\u2019s not pretending to be. The samples are self selected and nuance gets flattened. That\u2019s an intentional tradeoff. The bet is that if participation is fast, anonymous, and recurring, more people might actually show up.<p>Civie is built around a few core pillars. Lower the barrier to participation. Make it safe to answer honestly. Keep the results transparent and open. And allow signal to emerge both from individual questions and from patterns over time.<p>I\u2019d really value feedback on the core concept. Does a daily cadence make sense? Is anonymity enough to meaningfully lower <em>friction</em>? Are open aggregate results actually useful, or just interesting? What would make this something you\u2019d return to?<p>If you\u2019re curious, you can join the beta waitlist at civie.org. I\u2019m onboarding early users in small batches and would love thoughtful testers, skeptics, and critics.<p>Any level of feedback and discussion welcome. Thanks!<p>--<p>A few implementation details<p>Civie is currently built with Next.js on the frontend and Firebase (Auth + Firestore) on the backend. It\u2019s deployed on Vercel. The data model is intentionally simple: questions are versioned objects, and responses are stored as structured documents tied to a question ID and timestamp.<p>On anonymity: responses are not publicly tied to user profiles. There are no public accounts, no comment threads, and no way to see how an individual answered. The system stores responses separately from any identifying information, and aggregation happens at the query layer before display. The UI only ever exposes aggregate counts and distributions.<p>On identity verification: participation can require account creation to <em>lim</em>it spam and abuse. Verification status (for example, SMS verification or identity verification via a third-party provider) is stored separately from response data. The system tracks whether a response came from a verified participant, but does not attach identity to the answer itself."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["calling"],"value":"Show HN: Civie. Anonymous civic questions. Open results. No <em>yelling</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://www.civie.org/"}},"_tags":["story","author_gucduck","story_47008422","show_hn"],"author":"gucduck","children":[47009859,47009554,47008605,47008462,47008584],"created_at":"2026-02-13T22:01:32Z","created_at_i":1771020092,"num_comments":21,"objectID":"47008422","points":9,"story_id":47008422,"story_text":"Hey HN<p>I\u2019ve been thinking a lot about barriers to civic participation. Most people don\u2019t show up to town halls. They don\u2019t respond to traditional surveys. And a lot of online political spaces feel loud, adversarial, or exhausting.<p>So I started wondering what happens if you make participation radically lightweight.<p>That\u2019s where Civie comes in.<p>Each day you answer one to three short civic questions. It takes under a minute. Responses are anonymous. Results are shown only in aggregate. Over time, those responses accumulate into an open dataset anyone can explore.<p>This isn\u2019t scientific polling, and it\u2019s not pretending to be. The samples are self selected and nuance gets flattened. That\u2019s an intentional tradeoff. The bet is that if participation is fast, anonymous, and recurring, more people might actually show up.<p>Civie is built around a few core pillars. Lower the barrier to participation. Make it safe to answer honestly. Keep the results transparent and open. And allow signal to emerge both from individual questions and from patterns over time.<p>I\u2019d really value feedback on the core concept. Does a daily cadence make sense? Is anonymity enough to meaningfully lower friction? Are open aggregate results actually useful, or just interesting? What would make this something you\u2019d return to?<p>If you\u2019re curious, you can join the beta waitlist at civie.org. I\u2019m onboarding early users in small batches and would love thoughtful testers, skeptics, and critics.<p>Any level of feedback and discussion welcome. Thanks!<p>--<p>A few implementation details<p>Civie is currently built with Next.js on the frontend and Firebase (Auth + Firestore) on the backend. It\u2019s deployed on Vercel. The data model is intentionally simple: questions are versioned objects, and responses are stored as structured documents tied to a question ID and timestamp.<p>On anonymity: responses are not publicly tied to user profiles. There are no public accounts, no comment threads, and no way to see how an individual answered. The system stores responses separately from any identifying information, and aggregation happens at the query layer before display. The UI only ever exposes aggregate counts and distributions.<p>On identity verification: participation can require account creation to limit spam and abuse. Verification status (for example, SMS verification or identity verification via a third-party provider) is stored separately from response data. The system tracks whether a response came from a verified participant, but does not attach identity to the answer itself.","title":"Show HN: Civie. Anonymous civic questions. Open results. No yelling","updated_at":"2026-02-14T23:03:38Z","url":"https://www.civie.org/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"jared_stewart"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"I've been building a tool that changes how <em>LLM</em> coding agents explore codebases, and I wanted to share it along with some early observations.<p>Typically claude code globs directories, greps for patterns, and reads files with minimal guidance. It works in kind of the same way you'd learn to navigate a city by <em>walking</em> every street. You'll eventually build a mental map, but claude never does - at least not any that persists across different contexts.<p>The Recursive Language Models paper from Zhang, Kraska, and Khattab at MIT CSAIL introduced a cleaner framing. Instead of cramming everything into context, the model gets a searchable environment. The model can then query just for what it needs and can drill deeper where needed.<p>coderlm is my implementation of that idea for codebases. A Rust server indexes a project with tree-sitter, builds a symbol table with cross-references, and exposes an API. The agent queries for structure, symbols, implementations, callers, and grep results \u2014 getting back exactly the code it needs instead of scanning for it.<p>The agent workflow looks like:<p>1. `init` \u2014 register the project, get the top-level structure<p>2. `structure` \u2014 drill into specific directories<p>3. `search` \u2014 find symbols by name across the codebase<p>4. `impl` \u2014 retrieve the exact source of a <em>function</em> or class<p>5. `callers` \u2014 find everything that calls a given symbol<p>6. `grep` \u2014 fall back to text search when you need it<p>This replaces the glob/grep/read cycle with index-backed lookups. The server currently supports Rust, Python, TypeScript, JavaScript, and Go for symbol parsing, though all file types show up in the tree and are searchable via grep.<p>It ships as a Claude Code plugin with hooks that guide the agent to use indexed lookups instead of native file tools, plus a Python CLI wrapper with zero dependencies.<p>For anecdotal results, I ran the same prompt against a codebase to &quot;explore and identify opportunities to clarify the existing structure&quot;.<p>Using coderlm, claude was able to generate a plan in about 3 minutes. The coderlm enabled instance found a genuine bug (duplicated code with identical names), orphaned code for cleanup, mismatched naming conventions crossing module boundaries, and overlapping vocabulary. These are all <i>semantic</i> issues which clearly benefit from the tree-sitter centric approach.<p>Using the native tools, claude was able to identify various file clutter in the root of the project, out of date references, and a migration timestamp collision. These findings are more consistent with methodical walks of the filesystem and took about 8 minutes to produce.<p>The indexed approach did better at catching semantic issues than native tools and had a key benefit in being faster to resolve.<p>I've spent some effort to streamline the installation process, but it isn't turnkey yet. You'll need the rust toolchain to build the server which runs as a separate process. Installing the plugin from a claude marketplace is possible, but the skill isn't being added to your .claude yet so there are some manual steps to just getting to a point where claude could use it.<p>Claude continues to demonstrate significant resistance to using CodeRLM in exploration tasks. Typically to use you will need to explicitly direct claude to use it.<p>---<p>Repo: github.com/JaredStewart/coderlm<p>Paper: Recursive Language Models <a href=\"https://arxiv.org/abs/2512.24601\" rel=\"nofollow\">https://arxiv.org/abs/2512.24601</a> \u2014 Zhang, Kraska, Khattab (MIT CSAIL, 2025)<p>Inspired by: <a href=\"https://github.com/brainqub3/claude_code_RLM\" rel=\"nofollow\">https://github.com/brainqub3/claude_code_RLM</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: CodeRLM \u2013 Tree-sitter-backed code indexing for <em>LLM</em> agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/JaredStewart/coderlm/blob/main/server/REPL_to_API.md"}},"_tags":["story","author_jared_stewart","story_46974515","show_hn"],"author":"jared_stewart","children":[46984648,46984598,46987209,46985245,46986305,47013576,46984339,46984212,46983891,46983766],"created_at":"2026-02-11T13:10:23Z","created_at_i":1770815423,"num_comments":37,"objectID":"46974515","points":79,"story_id":46974515,"story_text":"I&#x27;ve been building a tool that changes how LLM coding agents explore codebases, and I wanted to share it along with some early observations.<p>Typically claude code globs directories, greps for patterns, and reads files with minimal guidance. It works in kind of the same way you&#x27;d learn to navigate a city by walking every street. You&#x27;ll eventually build a mental map, but claude never does - at least not any that persists across different contexts.<p>The Recursive Language Models paper from Zhang, Kraska, and Khattab at MIT CSAIL introduced a cleaner framing. Instead of cramming everything into context, the model gets a searchable environment. The model can then query just for what it needs and can drill deeper where needed.<p>coderlm is my implementation of that idea for codebases. A Rust server indexes a project with tree-sitter, builds a symbol table with cross-references, and exposes an API. The agent queries for structure, symbols, implementations, callers, and grep results \u2014 getting back exactly the code it needs instead of scanning for it.<p>The agent workflow looks like:<p>1. `init` \u2014 register the project, get the top-level structure<p>2. `structure` \u2014 drill into specific directories<p>3. `search` \u2014 find symbols by name across the codebase<p>4. `impl` \u2014 retrieve the exact source of a function or class<p>5. `callers` \u2014 find everything that calls a given symbol<p>6. `grep` \u2014 fall back to text search when you need it<p>This replaces the glob&#x2F;grep&#x2F;read cycle with index-backed lookups. The server currently supports Rust, Python, TypeScript, JavaScript, and Go for symbol parsing, though all file types show up in the tree and are searchable via grep.<p>It ships as a Claude Code plugin with hooks that guide the agent to use indexed lookups instead of native file tools, plus a Python CLI wrapper with zero dependencies.<p>For anecdotal results, I ran the same prompt against a codebase to &quot;explore and identify opportunities to clarify the existing structure&quot;.<p>Using coderlm, claude was able to generate a plan in about 3 minutes. The coderlm enabled instance found a genuine bug (duplicated code with identical names), orphaned code for cleanup, mismatched naming conventions crossing module boundaries, and overlapping vocabulary. These are all <i>semantic</i> issues which clearly benefit from the tree-sitter centric approach.<p>Using the native tools, claude was able to identify various file clutter in the root of the project, out of date references, and a migration timestamp collision. These findings are more consistent with methodical walks of the filesystem and took about 8 minutes to produce.<p>The indexed approach did better at catching semantic issues than native tools and had a key benefit in being faster to resolve.<p>I&#x27;ve spent some effort to streamline the installation process, but it isn&#x27;t turnkey yet. You&#x27;ll need the rust toolchain to build the server which runs as a separate process. Installing the plugin from a claude marketplace is possible, but the skill isn&#x27;t being added to your .claude yet so there are some manual steps to just getting to a point where claude could use it.<p>Claude continues to demonstrate significant resistance to using CodeRLM in exploration tasks. Typically to use you will need to explicitly direct claude to use it.<p>---<p>Repo: github.com&#x2F;JaredStewart&#x2F;coderlm<p>Paper: Recursive Language Models <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2512.24601\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2512.24601</a> \u2014 Zhang, Kraska, Khattab (MIT CSAIL, 2025)<p>Inspired by: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;brainqub3&#x2F;claude_code_RLM\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;brainqub3&#x2F;claude_code_RLM</a>","title":"Show HN: CodeRLM \u2013 Tree-sitter-backed code indexing for LLM agents","updated_at":"2026-02-15T10:23:10Z","url":"https://github.com/JaredStewart/coderlm/blob/main/server/REPL_to_API.md"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"nr378"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Recently I was trying to use an MCP server to pull data from a service, but hit a limitation: the MCP didn't expose the data I needed, even though the service's REST API supported it. So I wrote a quick CLI wrapper around the API. Worked great, except Claude Code had no structured way to know what my CLI does or how to call it. For `gh` or `curl` the model can learn from the extensive training data, but for a tool I just wrote, it was stabbing in the dark.<p>MCP solves this discovery problem, but it does it by rebuilding tool interaction from scratch: server processes, JSON-RPC transport, client-host handshakes. It got discovery right but threw out composability to get there. You can't pipe one MCP tool into another or run one in a cron job without a host process. <em>Pulling</em> a Confluence page, checking Jira for duplicates, and filing a ticket is three inference round-trips for work that should be a bash one-liner. I also seem to endlessly get asked to re-login to my MCPs, something `gh` CLI never asks me to do.<p>I think the industry took a wrong turn here. We didn't need a new execution model for tools, we needed to add one capability to the execution model we already had. That's what Model Tools Protocol (MTP) is: a spec for making any CLI self-describing so <em>LLMs</em> can discover and use it.<p>MTP does that with a single convention: your CLI responds to `--mtp-describe` with a JSON schema describing its commands, args, types, and examples. No server, no transport, no handshake. I wrote SDKs for Click (Python), Commander.js (TypeScript), Cobra (Go), and Clap (Rust) that introspect the types and help strings your framework already has, so adding `--mtp-describe` to an existing CLI is a single <em>function</em> call.<p>I don't think MCP should disappear, so there's a bidirectional bridge. `mtpcli serve` exposes any `--mtp-describe` CLI as an MCP server, and `mtpcli wrap` goes the other direction, turning MCP servers into pipeable CLIs. The ~2,500 MCP servers out there become composable CLI tools you can script and run in CI without an <em>LLM</em> in the loop.<p>The real payoff is composition: your custom CLI, a third-party MCP server, and jq in a single pipeline, no tokens burned. I'll post a concrete example in the comments.<p>Try it:<p><pre><code>  npm i -g @modeltoolsprotocol/mtpcli &amp;&amp; mtpcli --mtp-describe\n</code></pre>\nI know it's unlikely this will take off as I can't compete with the great might of Anthropic, but I very much welcome collaborators on this. PRs are welcome on the spec, additional SDKs, or anything else. Happy building!<p>Spec and rationale: &lt;<a href=\"https://github.com/modeltoolsprotocol/modeltoolsprotocol\" rel=\"nofollow\">https://github.com/modeltoolsprotocol/modeltoolsprotocol</a>&gt;<p>CLI tool: &lt;<a href=\"https://github.com/modeltoolsprotocol/mtpcli\" rel=\"nofollow\">https://github.com/modeltoolsprotocol/mtpcli</a>&gt;<p>SDKs: TypeScript (&lt;<a href=\"https://github.com/modeltoolsprotocol/typescript-sdk\" rel=\"nofollow\">https://github.com/modeltoolsprotocol/typescript-sdk</a>&gt;) | Python (&lt;<a href=\"https://github.com/modeltoolsprotocol/python-sdk\" rel=\"nofollow\">https://github.com/modeltoolsprotocol/python-sdk</a>&gt;) | Go (&lt;<a href=\"https://github.com/modeltoolsprotocol/go-sdk\" rel=\"nofollow\">https://github.com/modeltoolsprotocol/go-sdk</a>&gt;) | Rust (&lt;<a href=\"https://github.com/modeltoolsprotocol/rust-sdk\" rel=\"nofollow\">https://github.com/modeltoolsprotocol/rust-sdk</a>&gt;)"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Model Tools Protocol (MTP) \u2013 Forget MCP, bash is all you need"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/modeltoolsprotocol/modeltoolsprotocol"}},"_tags":["story","author_nr378","story_46959655","show_hn"],"author":"nr378","children":[46959663,46959732,46960042],"created_at":"2026-02-10T13:47:03Z","created_at_i":1770731223,"num_comments":3,"objectID":"46959655","points":8,"story_id":46959655,"story_text":"Recently I was trying to use an MCP server to pull data from a service, but hit a limitation: the MCP didn&#x27;t expose the data I needed, even though the service&#x27;s REST API supported it. So I wrote a quick CLI wrapper around the API. Worked great, except Claude Code had no structured way to know what my CLI does or how to call it. For `gh` or `curl` the model can learn from the extensive training data, but for a tool I just wrote, it was stabbing in the dark.<p>MCP solves this discovery problem, but it does it by rebuilding tool interaction from scratch: server processes, JSON-RPC transport, client-host handshakes. It got discovery right but threw out composability to get there. You can&#x27;t pipe one MCP tool into another or run one in a cron job without a host process. Pulling a Confluence page, checking Jira for duplicates, and filing a ticket is three inference round-trips for work that should be a bash one-liner. I also seem to endlessly get asked to re-login to my MCPs, something `gh` CLI never asks me to do.<p>I think the industry took a wrong turn here. We didn&#x27;t need a new execution model for tools, we needed to add one capability to the execution model we already had. That&#x27;s what Model Tools Protocol (MTP) is: a spec for making any CLI self-describing so LLMs can discover and use it.<p>MTP does that with a single convention: your CLI responds to `--mtp-describe` with a JSON schema describing its commands, args, types, and examples. No server, no transport, no handshake. I wrote SDKs for Click (Python), Commander.js (TypeScript), Cobra (Go), and Clap (Rust) that introspect the types and help strings your framework already has, so adding `--mtp-describe` to an existing CLI is a single function call.<p>I don&#x27;t think MCP should disappear, so there&#x27;s a bidirectional bridge. `mtpcli serve` exposes any `--mtp-describe` CLI as an MCP server, and `mtpcli wrap` goes the other direction, turning MCP servers into pipeable CLIs. The ~2,500 MCP servers out there become composable CLI tools you can script and run in CI without an LLM in the loop.<p>The real payoff is composition: your custom CLI, a third-party MCP server, and jq in a single pipeline, no tokens burned. I&#x27;ll post a concrete example in the comments.<p>Try it:<p><pre><code>  npm i -g @modeltoolsprotocol&#x2F;mtpcli &amp;&amp; mtpcli --mtp-describe\n</code></pre>\nI know it&#x27;s unlikely this will take off as I can&#x27;t compete with the great might of Anthropic, but I very much welcome collaborators on this. PRs are welcome on the spec, additional SDKs, or anything else. Happy building!<p>Spec and rationale: &lt;<a href=\"https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;modeltoolsprotocol\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;modeltoolsprotocol</a>&gt;<p>CLI tool: &lt;<a href=\"https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;mtpcli\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;mtpcli</a>&gt;<p>SDKs: TypeScript (&lt;<a href=\"https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;typescript-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;typescript-sdk</a>&gt;) | Python (&lt;<a href=\"https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;python-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;python-sdk</a>&gt;) | Go (&lt;<a href=\"https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;go-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;go-sdk</a>&gt;) | Rust (&lt;<a href=\"https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;rust-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;modeltoolsprotocol&#x2F;rust-sdk</a>&gt;)","title":"Show HN: Model Tools Protocol (MTP) \u2013 Forget MCP, bash is all you need","updated_at":"2026-02-15T07:48:55Z","url":"https://github.com/modeltoolsprotocol/modeltoolsprotocol"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"simranmultani"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Hey HN,<p>I've been building <em>LLM</em>-based agents for a while and two things kept biting me.<p>1. Loops \u2014 an agent node would get stuck <em>calling</em> the same thing over and over, and I wouldn't notice until the API bill showed up. Lost $200+ on one run.\n2. <em>LLM</em> would return garbage that didn't match what downstream code expected, and everything would just crash.<p>I looked around and couldn't find something simple that handled both. Most frameworks assume your node <em>function</em> just works. In practice it doesn't \u2014 <em>LLM</em> calls fail, JSON comes back broken, state gets weird.<p>So I built AgentCircuit. It's a Python decorator that wraps your agent <em>functions</em> with circuit breaker-style protections:<p><pre><code>    from agentcircuit import reliable\n    from pydantic import BaseModel\n\n    class Output(BaseModel):\n        name: str\n        age: int\n\n    @reliable(sentinel_schema=Output)\n    def extract_data(state):\n        return call_<em>llm</em>(state[&quot;text&quot;])\n</code></pre>\nThat's it. Under the hood it:<p>- Fuse \u2014 detects when a node keeps seeing the same input and kills the loop\n- Sentinel \u2014 validates every output against a Pydantic schema\n- Medic \u2014 auto-repairs bad outputs using an <em>LLM</em>\n- Budget \u2014 per-node and global dollar/time limits so you never get a surprise bill\n- Pricing \u2014 built-in cost tracking for 40+ models (GPT-5, Claude 4.x, Gemini 3, Llama, etc.)<p>There's no server, no config files, no framework lock-in. It works at the <em>function</em> boundary so it composes with LangGraph, LangChain, CrewAI, AutoGen, or just plain <em>functions</em>.<p>GitHub: <a href=\"https://github.com/simranmultani197/AgentCircuit\" rel=\"nofollow\">https://github.com/simranmultani197/AgentCircuit</a>\nPyPI: <a href=\"https://pypi.org/project/agentcircuit/\" rel=\"nofollow\">https://pypi.org/project/agentcircuit/</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function"],"value":"Show HN: AgentCircuit \u2013 Circuit breaker for AI agent <em>functions</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/simranmultani197/AgentCircuit"}},"_tags":["story","author_simranmultani","story_46899775","show_hn"],"author":"simranmultani","children":[46947900],"created_at":"2026-02-05T14:07:27Z","created_at_i":1770300447,"num_comments":1,"objectID":"46899775","points":1,"story_id":46899775,"story_text":"Hey HN,<p>I&#x27;ve been building LLM-based agents for a while and two things kept biting me.<p>1. Loops \u2014 an agent node would get stuck calling the same thing over and over, and I wouldn&#x27;t notice until the API bill showed up. Lost $200+ on one run.\n2. LLM would return garbage that didn&#x27;t match what downstream code expected, and everything would just crash.<p>I looked around and couldn&#x27;t find something simple that handled both. Most frameworks assume your node function just works. In practice it doesn&#x27;t \u2014 LLM calls fail, JSON comes back broken, state gets weird.<p>So I built AgentCircuit. It&#x27;s a Python decorator that wraps your agent functions with circuit breaker-style protections:<p><pre><code>    from agentcircuit import reliable\n    from pydantic import BaseModel\n\n    class Output(BaseModel):\n        name: str\n        age: int\n\n    @reliable(sentinel_schema=Output)\n    def extract_data(state):\n        return call_llm(state[&quot;text&quot;])\n</code></pre>\nThat&#x27;s it. Under the hood it:<p>- Fuse \u2014 detects when a node keeps seeing the same input and kills the loop\n- Sentinel \u2014 validates every output against a Pydantic schema\n- Medic \u2014 auto-repairs bad outputs using an LLM\n- Budget \u2014 per-node and global dollar&#x2F;time limits so you never get a surprise bill\n- Pricing \u2014 built-in cost tracking for 40+ models (GPT-5, Claude 4.x, Gemini 3, Llama, etc.)<p>There&#x27;s no server, no config files, no framework lock-in. It works at the function boundary so it composes with LangGraph, LangChain, CrewAI, AutoGen, or just plain functions.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;simranmultani197&#x2F;AgentCircuit\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;simranmultani197&#x2F;AgentCircuit</a>\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;agentcircuit&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;agentcircuit&#x2F;</a>","title":"Show HN: AgentCircuit \u2013 Circuit breaker for AI agent functions","updated_at":"2026-02-10T11:38:37Z","url":"https://github.com/simranmultani197/AgentCircuit"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Hannah203"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Over the past year, many SaaS products have added AI chatbots to answer questions and reduce support load. It helped initially, but most of these systems still live in a chat window with no awareness of what\u2019s happening inside the application.\nThey don\u2019t know the current page, selected data, user permissions, or workflow state. Users end up repeating context the product already has.\nI recently came across an open-source Copilot SDK that approaches this differently by injecting live application state directly into the AI and letting it execute real frontend and backend <em>functions</em> instead of just responding with text.\nWhat it does:\n- Understands application state including current page, selected data, and user permissions\n- Executes backend and frontend <em>functions</em> instead of only responding with text\n- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons\n- Understands user workflow and intent based on in-product context\n- Maintains session context so interactions remain consistent\nExample: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.\nTechnical details:\n- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)\n- <em>LLM</em>-agnostic (bring your own model)\n- State injection via context providers\n- Tool execution layer for safe <em>function</em> <em>calling</em>\n- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https://copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https://copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: YourGPT Copilot SDK Open-source SDK for product-level intelligence"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://copilot-sdk.yourgpt.ai/docs"}},"_tags":["story","author_Hannah203","story_46885205","show_hn"],"author":"Hannah203","created_at":"2026-02-04T12:47:39Z","created_at_i":1770209259,"num_comments":0,"objectID":"46885205","points":2,"story_id":46885205,"story_text":"Over the past year, many SaaS products have added AI chatbots to answer questions and reduce support load. It helped initially, but most of these systems still live in a chat window with no awareness of what\u2019s happening inside the application.\nThey don\u2019t know the current page, selected data, user permissions, or workflow state. Users end up repeating context the product already has.\nI recently came across an open-source Copilot SDK that approaches this differently by injecting live application state directly into the AI and letting it execute real frontend and backend functions instead of just responding with text.\nWhat it does:\n- Understands application state including current page, selected data, and user permissions\n- Executes backend and frontend functions instead of only responding with text\n- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons\n- Understands user workflow and intent based on in-product context\n- Maintains session context so interactions remain consistent\nExample: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.\nTechnical details:\n- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)\n- LLM-agnostic (bring your own model)\n- State injection via context providers\n- Tool execution layer for safe function calling\n- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases.","title":"Show HN: YourGPT Copilot SDK Open-source SDK for product-level intelligence","updated_at":"2026-02-04T13:18:11Z","url":"https://copilot-sdk.yourgpt.ai/docs"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Roshni1990r"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Over the past year, many SaaS products have added AI chatbots. They answer questions, guide users, and reduce some support load. That was a useful first step, but it is no longer enough.<p>Most AI still live in a chat window with no awareness of the application. They don't understand product state, selected data, user permissions, or the current workflow, so users end up restating context the system already has.<p>That approach does not scale well for real products.<p>We\u2019ve always believed copilots are the way to deliver the best customer experience\u2014not by answering questions, but by actually doing things<p>We are releasing Copilot SDK as an open-source toolkit to explore this idea and make context-aware, action-driven copilots for product teams.<p>What it does:<p>- Understands application state including current page, selected data, and user permissions<p>- Executes backend and frontend <em>functions</em> instead of only responding with text<p>- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons<p>- Understands user workflow and intent based on in-product context<p>- Maintains session context so interactions remain consistent<p>Example: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.<p>Technical details:<p>- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)<p>- <em>LLM</em>-agnostic (bring your own model)<p>- State injection via context providers<p>- Tool execution layer for safe <em>function</em> <em>calling</em><p>- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https://copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https://copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: YourGPT Copilot SDK \u2013 Open-source toolkit for product-aware AI agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://copilot-sdk.yourgpt.ai/docs"}},"_tags":["story","author_Roshni1990r","story_46870742","show_hn"],"author":"Roshni1990r","created_at":"2026-02-03T13:31:12Z","created_at_i":1770125472,"num_comments":0,"objectID":"46870742","points":1,"story_id":46870742,"story_text":"Over the past year, many SaaS products have added AI chatbots. They answer questions, guide users, and reduce some support load. That was a useful first step, but it is no longer enough.<p>Most AI still live in a chat window with no awareness of the application. They don&#x27;t understand product state, selected data, user permissions, or the current workflow, so users end up restating context the system already has.<p>That approach does not scale well for real products.<p>We\u2019ve always believed copilots are the way to deliver the best customer experience\u2014not by answering questions, but by actually doing things<p>We are releasing Copilot SDK as an open-source toolkit to explore this idea and make context-aware, action-driven copilots for product teams.<p>What it does:<p>- Understands application state including current page, selected data, and user permissions<p>- Executes backend and frontend functions instead of only responding with text<p>- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons<p>- Understands user workflow and intent based on in-product context<p>- Maintains session context so interactions remain consistent<p>Example: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.<p>Technical details:<p>- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)<p>- LLM-agnostic (bring your own model)<p>- State injection via context providers<p>- Tool execution layer for safe function calling<p>- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases.","title":"Show HN: YourGPT Copilot SDK \u2013 Open-source toolkit for product-aware AI agents","updated_at":"2026-02-03T13:35:54Z","url":"https://copilot-sdk.yourgpt.ai/docs"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"yol"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function","calling"],"value":"How existing prompt management solutions work bothers me, it seems to go against programming best practices: the prompt templates are stored in completely separate system from its dependencies, and there\u2019s no interface definitions for using them. It\u2019s like <em>calling</em> a <em>function</em> (the prompt template) that takes ANY arguments and can silently return crap when the arguments don\u2019t align with its internal implementation.<p>So I made this project according to how I think prompt management should work - strongly typed interface, defined in the code; the prompt templates are co-located in the same codebase as their dependencies; and there\u2019s type-hint and validation for devEx. Doing this also brings additional benefit: because the variables are strong typed at compose time, it\u2019s save to support complex prompt templates with if/else/for control loops with full type safety.<p>I\u2019d love to know whether this resonate with others, or is it just my pet peeve."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: Pixie-prompts \u2013 manage <em>LLM</em> prompt templates like code"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://gopixie.ai"}},"_tags":["story","author_yol","story_46781675","show_hn"],"author":"yol","created_at":"2026-01-27T15:55:40Z","created_at_i":1769529340,"num_comments":0,"objectID":"46781675","points":1,"story_id":46781675,"story_text":"How existing prompt management solutions work bothers me, it seems to go against programming best practices: the prompt templates are stored in completely separate system from its dependencies, and there\u2019s no interface definitions for using them. It\u2019s like calling a function (the prompt template) that takes ANY arguments and can silently return crap when the arguments don\u2019t align with its internal implementation.<p>So I made this project according to how I think prompt management should work - strongly typed interface, defined in the code; the prompt templates are co-located in the same codebase as their dependencies; and there\u2019s type-hint and validation for devEx. Doing this also brings additional benefit: because the variables are strong typed at compose time, it\u2019s save to support complex prompt templates with if&#x2F;else&#x2F;for control loops with full type safety.<p>I\u2019d love to know whether this resonate with others, or is it just my pet peeve.","title":"Show HN: Pixie-prompts \u2013 manage LLM prompt templates like code","updated_at":"2026-01-27T15:57:44Z","url":"https://gopixie.ai"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"iCeGaming"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Hey everyone,\nI built <em>llm</em>-schema-guard because LLMs are amazing at spitting out JSON... until they suddenly aren't. Even with JSON mode or <em>function</em> <em>calling</em>, you still get missing fields, wrong types, or just plain broken syntax that kills your agents, RAG flows, or any tool-<em>calling</em> setup.\nThis is a lightweight Rust HTTP proxy that sits in front of any OpenAI-compatible API (think Ollama, vLLM, LocalAI, OpenAI itself, Groq, you name it). It grabs the generated output, checks it against a JSON Schema you provide, and only lets it through if it's valid.\nIf it's invalid, strict mode kicks back a clean 400 with details. Permissive mode tries auto-retrying a few times by tweaking the prompt with a fix instruction and exponential backoff.\nEverything else stays the same: full streaming support (it buffers the response to validate), Prometheus metrics so you can monitor validation fails, retries, latency, and more. Config is simple YAML for upstreams, schemas per model, rate limiting, caching, etc. There's even an offline CLI if you just want to test schemas locally.\nIt's built with Axum and Tokio for really low latency and high throughput, plus jsonschema-rs under the hood. Docker compose makes it dead simple to spin up with Ollama.<p>This grew out of my earlier schema-gateway project, and I'm happy to add stuff like Anthropic support, tool <em>calling</em> validation, or better streaming fixes if people find it useful.\nStars or contributions are very welcome!<p>Thanks for taking a look :)"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: <em>LLM</em>-schema-guard \u2013 Rust proxy enforcing JSON schemas on <em>LLM</em> outputs"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"https://github.com/AncientiCe/<em>llm</em>-schema-guard"}},"_tags":["story","author_iCeGaming","story_46778689","show_hn"],"author":"iCeGaming","created_at":"2026-01-27T11:39:24Z","created_at_i":1769513964,"num_comments":0,"objectID":"46778689","points":1,"story_id":46778689,"story_text":"Hey everyone,\nI built llm-schema-guard because LLMs are amazing at spitting out JSON... until they suddenly aren&#x27;t. Even with JSON mode or function calling, you still get missing fields, wrong types, or just plain broken syntax that kills your agents, RAG flows, or any tool-calling setup.\nThis is a lightweight Rust HTTP proxy that sits in front of any OpenAI-compatible API (think Ollama, vLLM, LocalAI, OpenAI itself, Groq, you name it). It grabs the generated output, checks it against a JSON Schema you provide, and only lets it through if it&#x27;s valid.\nIf it&#x27;s invalid, strict mode kicks back a clean 400 with details. Permissive mode tries auto-retrying a few times by tweaking the prompt with a fix instruction and exponential backoff.\nEverything else stays the same: full streaming support (it buffers the response to validate), Prometheus metrics so you can monitor validation fails, retries, latency, and more. Config is simple YAML for upstreams, schemas per model, rate limiting, caching, etc. There&#x27;s even an offline CLI if you just want to test schemas locally.\nIt&#x27;s built with Axum and Tokio for really low latency and high throughput, plus jsonschema-rs under the hood. Docker compose makes it dead simple to spin up with Ollama.<p>This grew out of my earlier schema-gateway project, and I&#x27;m happy to add stuff like Anthropic support, tool calling validation, or better streaming fixes if people find it useful.\nStars or contributions are very welcome!<p>Thanks for taking a look :)","title":"Show HN: LLM-schema-guard \u2013 Rust proxy enforcing JSON schemas on LLM outputs","updated_at":"2026-01-27T11:45:58Z","url":"https://github.com/AncientiCe/llm-schema-guard"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ashtadmir"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function","calling"],"value":"I've been playing around with the new official GitHub Copilot SDK and realized it's a goldmine for building programmatic bridges to their models.<p>I built this server in Go to act as a OpenAI-compatible proxy. It essentially lets you treat your GitHub Copilot subscription as a standard OpenAI backend for any tool that supports it. I have tested it against OpenWebUI and Langchain.<p>Key Highlights:<p>- Official SDK: Built using the new Github Copilot SDK. It\u2019s much more robust than the reverse-engineered solutions floating around and does not use unpublished APIs.<p>- Tool <em>Calling</em> Support: It maps OpenAI <em>function</em> definitions to Copilot's agentic tools. You can use your own tools/functions through the Copilot without copilot needing access to the said tools just the definitions is enough.<p>The goal was to create a reliable &quot;bridge&quot; so I can use my subscription models in my preferred interfaces."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: An OpenAI API compatible server that uses GitHub Copilot SDK for <em>LLMs</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/RajatGarga/copilot-openai-server"}},"_tags":["story","author_ashtadmir","story_46775889","show_hn"],"author":"ashtadmir","children":[46776731],"created_at":"2026-01-27T05:31:17Z","created_at_i":1769491877,"num_comments":1,"objectID":"46775889","points":2,"story_id":46775889,"story_text":"I&#x27;ve been playing around with the new official GitHub Copilot SDK and realized it&#x27;s a goldmine for building programmatic bridges to their models.<p>I built this server in Go to act as a OpenAI-compatible proxy. It essentially lets you treat your GitHub Copilot subscription as a standard OpenAI backend for any tool that supports it. I have tested it against OpenWebUI and Langchain.<p>Key Highlights:<p>- Official SDK: Built using the new Github Copilot SDK. It\u2019s much more robust than the reverse-engineered solutions floating around and does not use unpublished APIs.<p>- Tool Calling Support: It maps OpenAI function definitions to Copilot&#x27;s agentic tools. You can use your own tools&#x2F;functions through the Copilot without copilot needing access to the said tools just the definitions is enough.<p>The goal was to create a reliable &quot;bridge&quot; so I can use my subscription models in my preferred interfaces.","title":"Show HN: An OpenAI API compatible server that uses GitHub Copilot SDK for LLMs","updated_at":"2026-01-27T07:50:26Z","url":"https://github.com/RajatGarga/copilot-openai-server"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"charlielidbury"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function"],"value":"If agent's tools are exposed as <em>functions</em>/objects in a Python REPL (as opposed to JSON schemas) they perform better, I linked the explainer article we wrote, but if you want to jump straight in check out the docs! <a href=\"https://docs.symbolica.ai/\" rel=\"nofollow\">https://docs.symbolica.ai/</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["calling","llm"],"value":"Show HN: <em>Calling</em> tools w/ Python improves <em>LLM</em> perf. vs MCP (77.1% on BrowseComp)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://www.symbolica.ai/blog/beyond-code-mode-agentica?theme=dark"}},"_tags":["story","author_charlielidbury","story_46287394","show_hn"],"author":"charlielidbury","created_at":"2025-12-16T11:41:34Z","created_at_i":1765885294,"num_comments":0,"objectID":"46287394","points":10,"story_id":46287394,"story_text":"If agent&#x27;s tools are exposed as functions&#x2F;objects in a Python REPL (as opposed to JSON schemas) they perform better, I linked the explainer article we wrote, but if you want to jump straight in check out the docs! <a href=\"https:&#x2F;&#x2F;docs.symbolica.ai&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.symbolica.ai&#x2F;</a>","title":"Show HN: Calling tools w/ Python improves LLM perf. vs MCP (77.1% on BrowseComp)","updated_at":"2025-12-22T21:48:03Z","url":"https://www.symbolica.ai/blog/beyond-code-mode-agentica?theme=dark"}],"hitsPerPage":10,"nbHits":143,"nbPages":15,"page":0,"params":"query=function+calling+LLM&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":26,"processingTimingsMS":{"_request":{"roundTrip":20},"afterFetch":{"format":{"highlighting":1,"total":2}},"fetch":{"query":11,"scanning":13,"total":25},"total":26},"query":"function calling LLM","serverTimeMS":28}
