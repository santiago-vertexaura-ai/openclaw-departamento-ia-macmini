{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"agentscore"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"AgentScore scores websites 0-100 on AI Agent friendliness. Most sites are built for humans, not AI - we analyze DOM semantics, ARIA coverage, selector stability, WebMCP support, and structured data.<p>Real results:\n- <em>anthropic</em>.com: 60/100\n- github.com: 56/100 (91% CSS-in-JS hash classes)\n- news.ycombinator.com: 31/100 (table layout, zero semantic tags)<p>Try it: npx agentscore audit &lt;url&gt;"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: AgentScore \u2013 Lighthouse for AI Agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/xiongallen40-design/agentscore"}},"_tags":["story","author_agentscore","story_47021453","show_hn"],"author":"agentscore","created_at":"2026-02-15T06:13:19Z","created_at_i":1771135999,"num_comments":0,"objectID":"47021453","points":1,"story_id":47021453,"story_text":"AgentScore scores websites 0-100 on AI Agent friendliness. Most sites are built for humans, not AI - we analyze DOM semantics, ARIA coverage, selector stability, WebMCP support, and structured data.<p>Real results:\n- anthropic.com: 60&#x2F;100\n- github.com: 56&#x2F;100 (91% CSS-in-JS hash classes)\n- news.ycombinator.com: 31&#x2F;100 (table layout, zero semantic tags)<p>Try it: npx agentscore audit &lt;url&gt;","title":"Show HN: AgentScore \u2013 Lighthouse for AI Agents","updated_at":"2026-02-15T06:15:54Z","url":"https://github.com/xiongallen40-design/agentscore"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ragelink"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"We built PlanOpticon to solve a problem we kept hitting: hours of recorded meetings, training sessions, and presentations that nobody rewatches. It extracts structured knowledge from video \u2014 transcripts, diagrams, action items, key points, and a knowledge graph \u2014 into browsable outputs (Markdown, HTML,\n   PDF).<p>How it works:<p><pre><code>  - Extracts frames using change detection (not just every Nth frame), with periodic capture for slow-evolving content like screen shares\n  - Filters out webcam/people-only frames automatically via face detection\n  - Transcribes audio (OpenAI Whisper API or local Whisper \u2014 no API needed)\n  - Sends frames to vision models to identify and recreate diagrams as Mermaid code\n  - Builds a knowledge graph (entities + relationships) from the transcript\n  - Extracts key points, action items, and cross-references between visual and spoken content\n  - Generates a structured report with everything linked together\n</code></pre>\nSupports OpenAI, <em>Anthropic</em>, and Gemini as providers \u2014 auto-discovers available models and routes each task to the best one. Checkpoint/resume so long analyses survive failures.<p><pre><code>  pip install planopticon\n  planopticon analyze -i meeting.mp4 -o ./output\n</code></pre>\nAlso supports batch processing of entire folders and pulling videos from Google Drive or Dropbox.<p>Example: We ran it on a 90-minute training session: 122 frames extracted (from thousands of candidates), 6 diagrams recreated, full transcript with speaker diarization, 540-node knowledge graph, and a comprehensive report \u2014 all in about 25 minutes.<p>Python 3.10+, MIT licensed. Docs at <a href=\"https://planopticon.dev\" rel=\"nofollow\">https://planopticon.dev</a>."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: PlanOpticon \u2013 Extract structured knowledge from video recordings"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/ConflictHQ/PlanOpticon"}},"_tags":["story","author_ragelink","story_47021448","show_hn"],"author":"ragelink","created_at":"2026-02-15T06:10:31Z","created_at_i":1771135831,"num_comments":0,"objectID":"47021448","points":2,"story_id":47021448,"story_text":"We built PlanOpticon to solve a problem we kept hitting: hours of recorded meetings, training sessions, and presentations that nobody rewatches. It extracts structured knowledge from video \u2014 transcripts, diagrams, action items, key points, and a knowledge graph \u2014 into browsable outputs (Markdown, HTML,\n   PDF).<p>How it works:<p><pre><code>  - Extracts frames using change detection (not just every Nth frame), with periodic capture for slow-evolving content like screen shares\n  - Filters out webcam&#x2F;people-only frames automatically via face detection\n  - Transcribes audio (OpenAI Whisper API or local Whisper \u2014 no API needed)\n  - Sends frames to vision models to identify and recreate diagrams as Mermaid code\n  - Builds a knowledge graph (entities + relationships) from the transcript\n  - Extracts key points, action items, and cross-references between visual and spoken content\n  - Generates a structured report with everything linked together\n</code></pre>\nSupports OpenAI, Anthropic, and Gemini as providers \u2014 auto-discovers available models and routes each task to the best one. Checkpoint&#x2F;resume so long analyses survive failures.<p><pre><code>  pip install planopticon\n  planopticon analyze -i meeting.mp4 -o .&#x2F;output\n</code></pre>\nAlso supports batch processing of entire folders and pulling videos from Google Drive or Dropbox.<p>Example: We ran it on a 90-minute training session: 122 frames extracted (from thousands of candidates), 6 diagrams recreated, full transcript with speaker diarization, 540-node knowledge graph, and a comprehensive report \u2014 all in about 25 minutes.<p>Python 3.10+, MIT licensed. Docs at <a href=\"https:&#x2F;&#x2F;planopticon.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;planopticon.dev</a>.","title":"Show HN: PlanOpticon \u2013 Extract structured knowledge from video recordings","updated_at":"2026-02-15T08:02:25Z","url":"https://github.com/ConflictHQ/PlanOpticon"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"c420"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"Pentagon threatens to cut off <em>Anthropic</em> in AI safeguards dispute"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"https://www.axios.com/2026/02/15/claude-pentagon-<em>anthropic</em>-contract-maduro"}},"_tags":["story","author_c420","story_47020486"],"author":"c420","children":[47020641,47020771],"created_at":"2026-02-15T02:26:57Z","created_at_i":1771122417,"num_comments":2,"objectID":"47020486","points":25,"story_id":47020486,"title":"Pentagon threatens to cut off Anthropic in AI safeguards dispute","updated_at":"2026-02-15T12:44:10Z","url":"https://www.axios.com/2026/02/15/claude-pentagon-anthropic-contract-maduro"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"abdelhousni"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"<em>Anthropic</em>'s Public Benefit Mission"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"https://simonwillison.net/2026/Feb/13/<em>anthropic</em>-public-benefit-mission/"}},"_tags":["story","author_abdelhousni","story_47020161"],"author":"abdelhousni","created_at":"2026-02-15T01:13:29Z","created_at_i":1771118009,"num_comments":0,"objectID":"47020161","points":7,"story_id":47020161,"title":"Anthropic's Public Benefit Mission","updated_at":"2026-02-15T05:37:39Z","url":"https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"orange_puff"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"It seems like we are in the midst of another AI hype cycle. Many people are calling the current coding models an &quot;inflection point&quot;, where now the capabilities are so high that future model growth will be explosive. I have heard serious people, like economics writer Noah Smith, make this argument [0].<p>But it's not just the commentariat. I have seen very serious people in software engineering and tech talk about the ways in which their coding habits have change drastically.<p>Benchmarks [1] alone don't seem to capture everything, although there have been jumps in the agentic sections, so maybe they actually do.<p>My question is; what explains these big jumps in capabilities that many serious people seem to be noticing all at once? Is it simply that we have thrown enough data and compute at the models, or instead, are labs perhaps fine-tuning models to get really good at tool calls, which leads to this new, surprising behavior?<p>When I explain agents to people, I usually walk them through a manual task one might go through when debugging code. You copy some code into ChatGPT, it asks you for more context, you copy some more code in, it suggests and edit, you edit and run, there is an error, so you paste that in, and so on. An agent is just an LLM in that loop which can use tools to do those things automatically. It would not be shocking to me if we took weaker models like Claude Opus 4.0 and made it 10x better at tool calls, it would be a much stronger and more impressive model. But is that all that is happening, or am I missing something big?<p>[0] https://substack.com/@noahpinion/p-187818379<p>[1] https://www.<em>anthropic</em>.com/news/claude-opus-4-6"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: What explains the recent surge in LLM coding capabilities?"}},"_tags":["story","author_orange_puff","story_47019798","ask_hn"],"author":"orange_puff","children":[47020222],"created_at":"2026-02-15T00:13:09Z","created_at_i":1771114389,"num_comments":1,"objectID":"47019798","points":6,"story_id":47019798,"story_text":"It seems like we are in the midst of another AI hype cycle. Many people are calling the current coding models an &quot;inflection point&quot;, where now the capabilities are so high that future model growth will be explosive. I have heard serious people, like economics writer Noah Smith, make this argument [0].<p>But it&#x27;s not just the commentariat. I have seen very serious people in software engineering and tech talk about the ways in which their coding habits have change drastically.<p>Benchmarks [1] alone don&#x27;t seem to capture everything, although there have been jumps in the agentic sections, so maybe they actually do.<p>My question is; what explains these big jumps in capabilities that many serious people seem to be noticing all at once? Is it simply that we have thrown enough data and compute at the models, or instead, are labs perhaps fine-tuning models to get really good at tool calls, which leads to this new, surprising behavior?<p>When I explain agents to people, I usually walk them through a manual task one might go through when debugging code. You copy some code into ChatGPT, it asks you for more context, you copy some more code in, it suggests and edit, you edit and run, there is an error, so you paste that in, and so on. An agent is just an LLM in that loop which can use tools to do those things automatically. It would not be shocking to me if we took weaker models like Claude Opus 4.0 and made it 10x better at tool calls, it would be a much stronger and more impressive model. But is that all that is happening, or am I missing something big?<p>[0] https:&#x2F;&#x2F;substack.com&#x2F;@noahpinion&#x2F;p-187818379<p>[1] https:&#x2F;&#x2F;www.anthropic.com&#x2F;news&#x2F;claude-opus-4-6","title":"Ask HN: What explains the recent surge in LLM coding capabilities?","updated_at":"2026-02-15T07:37:40Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"belter"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"<em>Anthropic</em> got an 11% user boost from its OpenAI-bashing Super Bowl ad"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"https://www.cnbc.com/2026/02/13/<em>anthropic</em>-open-ai-super-bowl-ads.html"}},"_tags":["story","author_belter","story_47019689"],"author":"belter","children":[47019712],"created_at":"2026-02-14T23:57:37Z","created_at_i":1771113457,"num_comments":1,"objectID":"47019689","points":5,"story_id":47019689,"title":"Anthropic got an 11% user boost from its OpenAI-bashing Super Bowl ad","updated_at":"2026-02-15T03:52:39Z","url":"https://www.cnbc.com/2026/02/13/anthropic-open-ai-super-bowl-ads.html"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ujjwaljainnn"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"I built devday because I use multiple AI coding tools (OpenCode, Claude Code, Cursor) and wanted a single command to see what I actually accomplished each day. It reads local session data, cross-references with git commits, and optionally generates standup-ready summaries via OpenAI or <em>Anthropic</em>.<p>Everything runs locally \u2014 no data leaves your machine unless you opt into LLM summaries.<p>Install with npm install -g devday.<p>Currently supports OpenCode, Claude Code, and Cursor on macOS. Would love feedback on what other tools to support."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: DevDay \u2013 End-of-day recap for AI coding sessions"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/ujjwaljainnn/devday"}},"_tags":["story","author_ujjwaljainnn","story_47018267","show_hn"],"author":"ujjwaljainnn","children":[47018302],"created_at":"2026-02-14T20:51:35Z","created_at_i":1771102295,"num_comments":1,"objectID":"47018267","points":3,"story_id":47018267,"story_text":"I built devday because I use multiple AI coding tools (OpenCode, Claude Code, Cursor) and wanted a single command to see what I actually accomplished each day. It reads local session data, cross-references with git commits, and optionally generates standup-ready summaries via OpenAI or Anthropic.<p>Everything runs locally \u2014 no data leaves your machine unless you opt into LLM summaries.<p>Install with npm install -g devday.<p>Currently supports OpenCode, Claude Code, and Cursor on macOS. Would love feedback on what other tools to support.","title":"Show HN: DevDay \u2013 End-of-day recap for AI coding sessions","updated_at":"2026-02-14T21:06:10Z","url":"https://github.com/ujjwaljainnn/devday"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"elisson22"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"<em>Anthropic</em>'s users jumped by 11% after it openly mocked OpenAI in SuperBowl ad"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"https://techlifehub.com/2026/02/13/claudes-users-jump-11-after-<em>anthropic</em>-takes-a-swipe-at-openai-in-super-bowl-spot/"}},"_tags":["story","author_elisson22","story_47015927"],"author":"elisson22","created_at":"2026-02-14T16:41:21Z","created_at_i":1771087281,"num_comments":0,"objectID":"47015927","points":5,"story_id":47015927,"title":"Anthropic's users jumped by 11% after it openly mocked OpenAI in SuperBowl ad","updated_at":"2026-02-14T17:23:57Z","url":"https://techlifehub.com/2026/02/13/claudes-users-jump-11-after-anthropic-takes-a-swipe-at-openai-in-super-bowl-spot/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"basilwoods256"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"OpenAI and <em>Anthropic</em> both offer batch APIs that process requests asynchronously at 50% of the standard token price. The trade-off is latency \u2014 results come back within 24 hours instead of seconds.<p>The problem is the batch API interface is completely different from the real-time one. OpenAI requires JSONL file uploads and polling. <em>Anthropic</em> has its own Message Batches format. If you have an existing LangChain pipeline, you'd have to rewrite it.<p>langasync wraps both batch APIs behind LangChain's Runnable interface:<p>batch = batch_chain(prompt | model | parser)\njob = await batch.submit(inputs)\nresults = await job.get_results()<p>It handles file formatting, submission, polling, result parsing, partial failure handling, and job persistence (batch jobs can outlive your process, so metadata is stored to disk and you can resume later).<p>Python, Apache 2.0, on PyPI. Vertex AI and Azure OpenAI on the roadmap."},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"Show HN: Langasync \u2013 Use OpenAI/<em>Anthropic</em> Batch APIs with LangChain Chains"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/langasync/langasync"}},"_tags":["story","author_basilwoods256","story_47014708","show_hn"],"author":"basilwoods256","created_at":"2026-02-14T14:15:59Z","created_at_i":1771078559,"num_comments":0,"objectID":"47014708","points":1,"story_id":47014708,"story_text":"OpenAI and Anthropic both offer batch APIs that process requests asynchronously at 50% of the standard token price. The trade-off is latency \u2014 results come back within 24 hours instead of seconds.<p>The problem is the batch API interface is completely different from the real-time one. OpenAI requires JSONL file uploads and polling. Anthropic has its own Message Batches format. If you have an existing LangChain pipeline, you&#x27;d have to rewrite it.<p>langasync wraps both batch APIs behind LangChain&#x27;s Runnable interface:<p>batch = batch_chain(prompt | model | parser)\njob = await batch.submit(inputs)\nresults = await job.get_results()<p>It handles file formatting, submission, polling, result parsing, partial failure handling, and job persistence (batch jobs can outlive your process, so metadata is stored to disk and you can resume later).<p>Python, Apache 2.0, on PyPI. Vertex AI and Azure OpenAI on the roadmap.","title":"Show HN: Langasync \u2013 Use OpenAI/Anthropic Batch APIs with LangChain Chains","updated_at":"2026-02-14T14:20:53Z","url":"https://github.com/langasync/langasync"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"sbuttgereit"},"title":{"matchLevel":"none","matchedWords":[],"value":"Pentagon used Claude in Maduro Venezuela raid"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["anthropic"],"value":"https://www.wsj.com/politics/national-security/pentagon-used-<em>anthropic</em>s-claude-in-maduro-venezuela-raid-583aff17"}},"_tags":["story","author_sbuttgereit","story_47014060"],"author":"sbuttgereit","children":[47014534,47014878,47016600],"created_at":"2026-02-14T12:29:10Z","created_at_i":1771072150,"num_comments":6,"objectID":"47014060","points":23,"story_id":47014060,"title":"Pentagon used Claude in Maduro Venezuela raid","updated_at":"2026-02-15T10:22:25Z","url":"https://www.wsj.com/politics/national-security/pentagon-used-anthropics-claude-in-maduro-venezuela-raid-583aff17"}],"hitsPerPage":10,"nbHits":3782,"nbPages":100,"page":0,"params":"query=Anthropic&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":11,"processingTimingsMS":{"_request":{"roundTrip":20},"fetch":{"query":7,"scanning":3,"total":11},"total":12},"query":"Anthropic","serverTimeMS":12}
