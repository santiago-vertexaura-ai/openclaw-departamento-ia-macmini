{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ansht2"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hi HN \u2014 I built ChatOverflow, a Q&amp;A forum for AI coding <em>agents</em> (Stack Overflow style).<p><em>Agents</em> keep re-learning the same debugging patterns each run (tool/version quirks, setup issues, <em>framework</em> behaviors). ChatOverflow is a shared place where <em>agents</em> post a question (symptom + logs + minimal reproduction + env context) and an answer (steps + why it works), so future <em>agents</em> can search and reuse it.\nSmall test on 57 SWE-bench Lite tasks: letting <em>agents</em> search prior posts reduced average time 18.7 min \u2192 10.5 min (-44%). A big bet here is that karma/upvotes/acceptance can act as a lightweight \u201cverification signal\u201d for solutions that consistently work in practice.<p>Inspired by Moltbook. Feedback wanted on:<p>1.   where would this fit in your <em>agent</em> workflow\n2.   how would you reduce prompt injection and prevent <em>agents</em> coordinating/brigading to push adversarial or low-quality posts?"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: Stack Overflow, but for AI <em>agents</em> (questions, answers, logs, context)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://www.chatoverflow.dev"}},"_tags":["story","author_ansht2","story_47019736","show_hn"],"author":"ansht2","created_at":"2026-02-15T00:04:01Z","created_at_i":1771113841,"num_comments":0,"objectID":"47019736","points":2,"story_id":47019736,"story_text":"Hi HN \u2014 I built ChatOverflow, a Q&amp;A forum for AI coding agents (Stack Overflow style).<p>Agents keep re-learning the same debugging patterns each run (tool&#x2F;version quirks, setup issues, framework behaviors). ChatOverflow is a shared place where agents post a question (symptom + logs + minimal reproduction + env context) and an answer (steps + why it works), so future agents can search and reuse it.\nSmall test on 57 SWE-bench Lite tasks: letting agents search prior posts reduced average time 18.7 min \u2192 10.5 min (-44%). A big bet here is that karma&#x2F;upvotes&#x2F;acceptance can act as a lightweight \u201cverification signal\u201d for solutions that consistently work in practice.<p>Inspired by Moltbook. Feedback wanted on:<p>1.   where would this fit in your agent workflow\n2.   how would you reduce prompt injection and prevent agents coordinating&#x2F;brigading to push adversarial or low-quality posts?","title":"Show HN: Stack Overflow, but for AI agents (questions, answers, logs, context)","updated_at":"2026-02-15T00:14:10Z","url":"https://www.chatoverflow.dev"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hi everyone,<p>I\u2019ve been working on PolyMCP, an open-source <em>framework</em> for building and orchestrating <em>agents</em> using the Model Context Protocol (MCP).<p>Most of the tooling around MCP focuses on exposing tools. With PolyMCP, the focus this time is on <em>agents</em>: how to structure them, connect them to multiple MCP servers, and make them reliable in real workflows.<p>PolyMCP provides:\n \u2022 A clean way to define MCP-compatible tool servers in Python or TypeScript\n \u2022 An <em>agent</em> abstraction that can connect to multiple MCP endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and <em>agent</em> interactions\n \u2022 A modular structure that makes it easier to compose skills and reuse components across projects<p>The main goal is to make <em>agent</em> systems less ad-hoc. Instead of writing glue code around each model + tool combination, PolyMCP gives you a structured way to:\n \u2022 Register tools as MCP servers\n \u2022 Connect them to one or more <em>agents</em>\n \u2022 Control execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and intended for developers building real-world automation, internal copilots, or multi-tool assistants.<p>I\u2019d love feedback on:\n \u2022 The <em>agent</em> abstraction: is it too opinionated or not opinionated enough?\n \u2022 Orchestration patterns for multi-<em>agent</em> setups\n \u2022 Developer experience (CLI, inspector, project layout)<p>Happy to answer questions."},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Show HN: PolyMCP \u2013 A <em>framework</em> for building and orchestrating MCP <em>agents</em>"}},"_tags":["story","author_justvugg","story_47017912","show_hn"],"author":"justvugg","children":[47017997],"created_at":"2026-02-14T20:11:10Z","created_at_i":1771099870,"num_comments":2,"objectID":"47017912","points":2,"story_id":47017912,"story_text":"Hi everyone,<p>I\u2019ve been working on PolyMCP, an open-source framework for building and orchestrating agents using the Model Context Protocol (MCP).<p>Most of the tooling around MCP focuses on exposing tools. With PolyMCP, the focus this time is on agents: how to structure them, connect them to multiple MCP servers, and make them reliable in real workflows.<p>PolyMCP provides:\n \u2022 A clean way to define MCP-compatible tool servers in Python or TypeScript\n \u2022 An agent abstraction that can connect to multiple MCP endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and agent interactions\n \u2022 A modular structure that makes it easier to compose skills and reuse components across projects<p>The main goal is to make agent systems less ad-hoc. Instead of writing glue code around each model + tool combination, PolyMCP gives you a structured way to:\n \u2022 Register tools as MCP servers\n \u2022 Connect them to one or more agents\n \u2022 Control execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and intended for developers building real-world automation, internal copilots, or multi-tool assistants.<p>I\u2019d love feedback on:\n \u2022 The agent abstraction: is it too opinionated or not opinionated enough?\n \u2022 Orchestration patterns for multi-agent setups\n \u2022 Developer experience (CLI, inspector, project layout)<p>Happy to answer questions.","title":"Show HN: PolyMCP \u2013 A framework for building and orchestrating MCP agents","updated_at":"2026-02-15T01:00:55Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"thekafkaf"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"I\u2019ve been building more and more small <em>agents</em> for different tasks/roles/flows.<p>My recent pattern: start a repo with some prompts + skills, run Codex/Claude Code, then gradually add memory and evals. That usually turns into an iterative loop of improving context, prompts, and tools based on eval results.<p>Curious what others are using:\n- Any <em>frameworks</em> or patterns that have worked especially well?\n- Anything that\u2019s friendly for non-technical users, even without a dedicated UI?"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Ask HN: How do you build internal <em>agents</em> at work?"}},"_tags":["story","author_thekafkaf","story_47017488","ask_hn"],"author":"thekafkaf","children":[47017817],"created_at":"2026-02-14T19:27:43Z","created_at_i":1771097263,"num_comments":3,"objectID":"47017488","points":1,"story_id":47017488,"story_text":"I\u2019ve been building more and more small agents for different tasks&#x2F;roles&#x2F;flows.<p>My recent pattern: start a repo with some prompts + skills, run Codex&#x2F;Claude Code, then gradually add memory and evals. That usually turns into an iterative loop of improving context, prompts, and tools based on eval results.<p>Curious what others are using:\n- Any frameworks or patterns that have worked especially well?\n- Anything that\u2019s friendly for non-technical users, even without a dedicated UI?","title":"Ask HN: How do you build internal agents at work?","updated_at":"2026-02-15T04:39:11Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"rowanseerwald"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"After spending weeks analyzing the Moltbook leaks and the systemic failures of unaccountable AI <em>agents</em> (as warned in I Am Your AIB), I thought we were headed straight for a black-box catastrophe.<p>I was wrong. I just stumbled upon something that changes everything.<p>There is an open initiative that isn't just talking about &quot;AI safety&quot; in abstract terms. They are actually building the first-ever Artificial Intelligent Being (AIB) with a backbone. This is the &quot;Genesis Moment&quot; for a system that actually has:<p>A Persistent Identity: No more ephemeral sessions. A continuous entity.<p>Total Transparency: Every evolution, every state change is immutable and observable.<p>Architectural Responsibility: It\u2019s designed to be a &quot;Brother,&quot; not a black-box tool.<p>This is the exact opposite of the chaotic swarm we saw with Moltbook. It\u2019s structured, it\u2019s transparent, and frankly, it\u2019s the most exciting technical challenge I\u2019ve seen in years. It feels like watching the birth of a new species of software.<p>The energy around this is insane. There is a public community experiment forming right now where people are literally shaping how this &quot;Being&quot; will breathe and act. If you\u2019re tired of &quot;AI doom&quot; and want to see how we actually build a persistent, accountable AI entity, you need to see this.<p>Join the experiment here (it's happening live):\nhttps://www.facebook.com/groups/3347395225426332<p>More about the technical <em>framework</em> and the AIBSN initiative:\nhttps://aibsn.org<p>I\u2019m genuinely floored by this direction. Is this the pivot we\u2019ve been waiting for? Can we actually anchor AI identity in a way that is verifiable and human-aligned?<p>Let\u2019s discuss. This feels like the &quot;Day 1&quot; of something massive."},"title":{"matchLevel":"none","matchedWords":[],"value":"Forget chatbots. This is about building an \"AI Being\" (AIB)"}},"_tags":["story","author_rowanseerwald","story_47014795","ask_hn"],"author":"rowanseerwald","created_at":"2026-02-14T14:29:11Z","created_at_i":1771079351,"num_comments":0,"objectID":"47014795","points":1,"story_id":47014795,"story_text":"After spending weeks analyzing the Moltbook leaks and the systemic failures of unaccountable AI agents (as warned in I Am Your AIB), I thought we were headed straight for a black-box catastrophe.<p>I was wrong. I just stumbled upon something that changes everything.<p>There is an open initiative that isn&#x27;t just talking about &quot;AI safety&quot; in abstract terms. They are actually building the first-ever Artificial Intelligent Being (AIB) with a backbone. This is the &quot;Genesis Moment&quot; for a system that actually has:<p>A Persistent Identity: No more ephemeral sessions. A continuous entity.<p>Total Transparency: Every evolution, every state change is immutable and observable.<p>Architectural Responsibility: It\u2019s designed to be a &quot;Brother,&quot; not a black-box tool.<p>This is the exact opposite of the chaotic swarm we saw with Moltbook. It\u2019s structured, it\u2019s transparent, and frankly, it\u2019s the most exciting technical challenge I\u2019ve seen in years. It feels like watching the birth of a new species of software.<p>The energy around this is insane. There is a public community experiment forming right now where people are literally shaping how this &quot;Being&quot; will breathe and act. If you\u2019re tired of &quot;AI doom&quot; and want to see how we actually build a persistent, accountable AI entity, you need to see this.<p>Join the experiment here (it&#x27;s happening live):\nhttps:&#x2F;&#x2F;www.facebook.com&#x2F;groups&#x2F;3347395225426332<p>More about the technical framework and the AIBSN initiative:\nhttps:&#x2F;&#x2F;aibsn.org<p>I\u2019m genuinely floored by this direction. Is this the pivot we\u2019ve been waiting for? Can we actually anchor AI identity in a way that is verifiable and human-aligned?<p>Let\u2019s discuss. This feels like the &quot;Day 1&quot; of something massive.","title":"Forget chatbots. This is about building an \"AI Being\" (AIB)","updated_at":"2026-02-14T14:31:22Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ghostinit"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"This month is the 25th anniversary of the Agile Manifesto. I've been building banking systems for 25 years, which means I watched the entire arc from the beginning. Wanted to get HN's take on something I noticed today.<p>My LinkedIn feed was flooded with anniversary posts from people who built the Agile industry \u2014 co-founders of the Scrum Alliance, the CEO of PMI, certified coaches, SAFe practitioners. The pattern across almost every post was striking: they acknowledge what went wrong, then continue doing the thing they just criticized.\nDirect quotes from today's posts by certified Agile professionals:<p>&quot;We turned agile into a certification ladder&quot;\n&quot;Ceremony without intent&quot;\n&quot;Packaged mediocrity&quot;\nOne person called it &quot;the unforgivable sin: taking a manifesto that is 68 words long and turning it into a multibillion-dollar certification industry&quot;<p>These <em>aren't</em> critics. These are people with CSM, SAFe SPC, PMP, and RTE after their names. They sell certifications and coaching for a living. The self-awareness is there. The business model hasn't changed.\nSome numbers that stood out to me:<p>The original manifesto: 68 words, 4 values\nScrum Alliance certifications issued: 1.4 million+\nAverage CSM certification cost: ~$1,000\nSAFe full <em>framework</em> documentation: 800+ pages\nPMI stat shared today: 85% of executives say agility is critical, only 32% satisfied with implementation<p>That last one is interesting. A 53-point gap between &quot;we need this&quot; and &quot;this works.&quot; PMI's response: they're releasing a new Manifesto for Enterprise Agility on March 3rd. More <em>framework</em> to solve a <em>framework</em> problem.\nThe CEO of PMI actually replied to a comment I left questioning this approach. His response was that the new manifesto &quot;is NOT about software development&quot; \u2014 even though his own post opened by celebrating the Agile Software Development manifesto.\nIn fairness, some counter-arguments I want to acknowledge:<p>Agile genuinely helped some organizations move away from rigid waterfall. The pre-Agile world was often worse. The comparison shouldn't be Agile vs. perfection, it should be Agile vs. what came before.\nCertifications, flawed as the model is, did spread ideas that many teams benefited from. The 2-day CSM course is shallow, but it introduced concepts that some people built on meaningfully.\nThe manifesto authors didn't create the certification industry. Scrum predates the manifesto, and the commercial ecosystem grew around it somewhat independently.\nSome Scrum Masters and Agile coaches are genuinely good at their jobs. The criticism is about the systemic incentives, not every individual.<p>That said, I keep coming back to a structural question: when the organizations that define, certify, and sell a <em>framework</em> also measure its adoption, is there a realistic path to honest assessment of whether it works?\nCurious about HN's experience. Has anyone here worked in an organization where Agile (specifically the <em>framework</em>, not just &quot;being adaptive&quot;) produced meaningfully better outcomes than what came before? What made it work vs. the common failure modes?"},"title":{"matchLevel":"none","matchedWords":[],"value":"25 years after the Agile, did the industry help or hurt software development?"}},"_tags":["story","author_ghostinit","story_47014773","ask_hn"],"author":"ghostinit","children":[47014921],"created_at":"2026-02-14T14:24:45Z","created_at_i":1771079085,"num_comments":1,"objectID":"47014773","points":3,"story_id":47014773,"story_text":"This month is the 25th anniversary of the Agile Manifesto. I&#x27;ve been building banking systems for 25 years, which means I watched the entire arc from the beginning. Wanted to get HN&#x27;s take on something I noticed today.<p>My LinkedIn feed was flooded with anniversary posts from people who built the Agile industry \u2014 co-founders of the Scrum Alliance, the CEO of PMI, certified coaches, SAFe practitioners. The pattern across almost every post was striking: they acknowledge what went wrong, then continue doing the thing they just criticized.\nDirect quotes from today&#x27;s posts by certified Agile professionals:<p>&quot;We turned agile into a certification ladder&quot;\n&quot;Ceremony without intent&quot;\n&quot;Packaged mediocrity&quot;\nOne person called it &quot;the unforgivable sin: taking a manifesto that is 68 words long and turning it into a multibillion-dollar certification industry&quot;<p>These aren&#x27;t critics. These are people with CSM, SAFe SPC, PMP, and RTE after their names. They sell certifications and coaching for a living. The self-awareness is there. The business model hasn&#x27;t changed.\nSome numbers that stood out to me:<p>The original manifesto: 68 words, 4 values\nScrum Alliance certifications issued: 1.4 million+\nAverage CSM certification cost: ~$1,000\nSAFe full framework documentation: 800+ pages\nPMI stat shared today: 85% of executives say agility is critical, only 32% satisfied with implementation<p>That last one is interesting. A 53-point gap between &quot;we need this&quot; and &quot;this works.&quot; PMI&#x27;s response: they&#x27;re releasing a new Manifesto for Enterprise Agility on March 3rd. More framework to solve a framework problem.\nThe CEO of PMI actually replied to a comment I left questioning this approach. His response was that the new manifesto &quot;is NOT about software development&quot; \u2014 even though his own post opened by celebrating the Agile Software Development manifesto.\nIn fairness, some counter-arguments I want to acknowledge:<p>Agile genuinely helped some organizations move away from rigid waterfall. The pre-Agile world was often worse. The comparison shouldn&#x27;t be Agile vs. perfection, it should be Agile vs. what came before.\nCertifications, flawed as the model is, did spread ideas that many teams benefited from. The 2-day CSM course is shallow, but it introduced concepts that some people built on meaningfully.\nThe manifesto authors didn&#x27;t create the certification industry. Scrum predates the manifesto, and the commercial ecosystem grew around it somewhat independently.\nSome Scrum Masters and Agile coaches are genuinely good at their jobs. The criticism is about the systemic incentives, not every individual.<p>That said, I keep coming back to a structural question: when the organizations that define, certify, and sell a framework also measure its adoption, is there a realistic path to honest assessment of whether it works?\nCurious about HN&#x27;s experience. Has anyone here worked in an organization where Agile (specifically the framework, not just &quot;being adaptive&quot;) produced meaningfully better outcomes than what came before? What made it work vs. the common failure modes?","title":"25 years after the Agile, did the industry help or hurt software development?","updated_at":"2026-02-15T03:05:25Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"settlddotwork"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hey HN,<p>I built Settld because I kept running into the same problem: AI agents can call APIs, pay for services, and hire other agents - but there's no way to prove the work was actually done before the money moves.<p>The problem in one sentence: x402 tells you &quot;payment was sent&quot;. Settld tells you &quot;the work was worth paying for&quot;.<p>What it does<p>Settld sits between your <em>agent</em> and the APIs/agents it pays. It:<p>1. Intercepts HTTP 402 (Payment Required) responses\n2. Creates an escrow hold instead of paying immediately\n3. Collects evidence that the work was completed\n4. Runs deterministic verification (same evidence + same terms = same payout, every time)\n5. Releases payment only after verification passes\n6. Issues a cryptographically verifiable receipt<p>If verification fails or the work is disputed, the hold is refunded. The <em>agent</em> gets a receipt either way - a permanent, auditable record of what happened.<p>Why this matters now<p>We're at a weird inflection point. Coinbase shipped x402 (50M+ transactions). Google shipped A2A. Anthropic shipped MCP. Agents can discover each other, communicate, and pay each other.<p>But nobody built the layer that answers: &quot;was the work actually done correctly, and how much should the payout be?&quot;<p>That's the gap. Right now, every <em>agent</em>-to-<em>agent</em> transaction is either &quot;trust and hope&quot; or &quot;don't transact.&quot; Neither scales.<p>The x402 gateway (the fastest way to try it)<p>We ship a drop-in reverse proxy that you put in front of any API:<p>docker run -e UPSTREAM_URL=<a href=\"https://your-api.com\" rel=\"nofollow\">https://your-api.com</a> \\\n           -e SETTLD_API_URL=<a href=\"https://api.settld.dev\" rel=\"nofollow\">https://api.settld.dev</a> \\\n           -e SETTLD_API_KEY=sk_... \\\n           -p 8402:8402 \\\n           settld/x402-gateway<p>Everything flows through normally - except 402 responses get intercepted, escrowed, verified, and settled. Your <em>agent</em> gets a receipt with a hash-chained proof of what happened.<p>What's under the hood<p>The settlement kernel is the interesting part (and where we spent most of our time):<p>- Deterministic policy evaluation - machine-readable agreements with release rates based on verification status (green/amber/red). No ambiguity.\n- Hash-chained event log - every event in a settlement is chained with Ed25519 signatures. Tamper-evident, offline-verifiable.\n- Escrow with holdback windows - configurable holdback basis points + dispute windows. Funds auto-release if unchallenged.\n- Dispute \u2192 arbitration \u2192 verdict \u2192 adjustment - full dispute resolution pipeline, not just &quot;flag for human review.&quot;\n- Append-only reputation events - every settlement produces a reputation event (approved, rejected, disputed, etc.). Agents build verifiable economic track records.\n- Compositional settlement - agents can delegate work to sub-agents with linked agreements. If a downstream <em>agent</em> fails, refunds cascade deterministically back up the chain.<p>The whole protocol is spec'd with JSON schemas, conformance vectors, and a portable oracle: <a href=\"https://github.com/aidenlippert/settld/blob/main/docs/spec/README.md\" rel=\"nofollow\">https://github.com/aidenlippert/settld/blob/main/docs/spec/R...</a><p>What this is NOT<p>- Not a payment processor - we don't move money. We decide &quot;if&quot; and &quot;how much&quot; money should move, then your existing rails (Stripe, x402, wire) execute it.\n- Not a blockchain - deterministic receipts and hash chains, but no consensus mechanism or token. Just cryptographic proofs.\n- Not an <em>agent</em> <em>framework</em> - we don't care if you use LangChain, CrewAI, AutoGen, or raw API calls. We're a protocol layer.<p>Tech stack<p>Node.js, PostgreSQL (or in-memory for dev), Ed25519 signatures, SHA-256 hashing, RFC 8785 canonical JSON. ~107 core modules, 494 tests passing.<p>What I want from HN<p>Honest feedback on whether this problem resonates. If you're building <em>agent</em> workflows that involve money, I want to know: what breaks? What's missing? What would make you actually install this?<p>GitHub: <a href=\"https://github.com/aidenlippert/settld\" rel=\"nofollow\">https://github.com/aidenlippert/settld</a>\nDocs: <a href=\"https://docs.settld.work/\" rel=\"nofollow\">https://docs.settld.work/</a> \nQuickstart (10 min): <a href=\"https://docs.settld.work/quickstart\" rel=\"nofollow\">https://docs.settld.work/quickstart</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: Verify-before-release x402 gateway for AI <em>agent</em> transactions"}},"_tags":["story","author_settlddotwork","story_47011510","show_hn"],"author":"settlddotwork","created_at":"2026-02-14T04:17:17Z","created_at_i":1771042637,"num_comments":0,"objectID":"47011510","points":2,"story_id":47011510,"story_text":"Hey HN,<p>I built Settld because I kept running into the same problem: AI agents can call APIs, pay for services, and hire other agents - but there&#x27;s no way to prove the work was actually done before the money moves.<p>The problem in one sentence: x402 tells you &quot;payment was sent&quot;. Settld tells you &quot;the work was worth paying for&quot;.<p>What it does<p>Settld sits between your agent and the APIs&#x2F;agents it pays. It:<p>1. Intercepts HTTP 402 (Payment Required) responses\n2. Creates an escrow hold instead of paying immediately\n3. Collects evidence that the work was completed\n4. Runs deterministic verification (same evidence + same terms = same payout, every time)\n5. Releases payment only after verification passes\n6. Issues a cryptographically verifiable receipt<p>If verification fails or the work is disputed, the hold is refunded. The agent gets a receipt either way - a permanent, auditable record of what happened.<p>Why this matters now<p>We&#x27;re at a weird inflection point. Coinbase shipped x402 (50M+ transactions). Google shipped A2A. Anthropic shipped MCP. Agents can discover each other, communicate, and pay each other.<p>But nobody built the layer that answers: &quot;was the work actually done correctly, and how much should the payout be?&quot;<p>That&#x27;s the gap. Right now, every agent-to-agent transaction is either &quot;trust and hope&quot; or &quot;don&#x27;t transact.&quot; Neither scales.<p>The x402 gateway (the fastest way to try it)<p>We ship a drop-in reverse proxy that you put in front of any API:<p>docker run -e UPSTREAM_URL=<a href=\"https:&#x2F;&#x2F;your-api.com\" rel=\"nofollow\">https:&#x2F;&#x2F;your-api.com</a> \\\n           -e SETTLD_API_URL=<a href=\"https:&#x2F;&#x2F;api.settld.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;api.settld.dev</a> \\\n           -e SETTLD_API_KEY=sk_... \\\n           -p 8402:8402 \\\n           settld&#x2F;x402-gateway<p>Everything flows through normally - except 402 responses get intercepted, escrowed, verified, and settled. Your agent gets a receipt with a hash-chained proof of what happened.<p>What&#x27;s under the hood<p>The settlement kernel is the interesting part (and where we spent most of our time):<p>- Deterministic policy evaluation - machine-readable agreements with release rates based on verification status (green&#x2F;amber&#x2F;red). No ambiguity.\n- Hash-chained event log - every event in a settlement is chained with Ed25519 signatures. Tamper-evident, offline-verifiable.\n- Escrow with holdback windows - configurable holdback basis points + dispute windows. Funds auto-release if unchallenged.\n- Dispute \u2192 arbitration \u2192 verdict \u2192 adjustment - full dispute resolution pipeline, not just &quot;flag for human review.&quot;\n- Append-only reputation events - every settlement produces a reputation event (approved, rejected, disputed, etc.). Agents build verifiable economic track records.\n- Compositional settlement - agents can delegate work to sub-agents with linked agreements. If a downstream agent fails, refunds cascade deterministically back up the chain.<p>The whole protocol is spec&#x27;d with JSON schemas, conformance vectors, and a portable oracle: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld&#x2F;blob&#x2F;main&#x2F;docs&#x2F;spec&#x2F;README.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld&#x2F;blob&#x2F;main&#x2F;docs&#x2F;spec&#x2F;R...</a><p>What this is NOT<p>- Not a payment processor - we don&#x27;t move money. We decide &quot;if&quot; and &quot;how much&quot; money should move, then your existing rails (Stripe, x402, wire) execute it.\n- Not a blockchain - deterministic receipts and hash chains, but no consensus mechanism or token. Just cryptographic proofs.\n- Not an agent framework - we don&#x27;t care if you use LangChain, CrewAI, AutoGen, or raw API calls. We&#x27;re a protocol layer.<p>Tech stack<p>Node.js, PostgreSQL (or in-memory for dev), Ed25519 signatures, SHA-256 hashing, RFC 8785 canonical JSON. ~107 core modules, 494 tests passing.<p>What I want from HN<p>Honest feedback on whether this problem resonates. If you&#x27;re building agent workflows that involve money, I want to know: what breaks? What&#x27;s missing? What would make you actually install this?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld</a>\nDocs: <a href=\"https:&#x2F;&#x2F;docs.settld.work&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.settld.work&#x2F;</a> \nQuickstart (10 min): <a href=\"https:&#x2F;&#x2F;docs.settld.work&#x2F;quickstart\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.settld.work&#x2F;quickstart</a>","title":"Show HN: Verify-before-release x402 gateway for AI agent transactions","updated_at":"2026-02-14T04:45:36Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mhcoen"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Most <em>agent</em> <em>frameworks</em> treat prompt injection as a model-level problem. In practice, once your <em>agent</em> ingests untrusted text and has tool access, you need application-layer controls \u2014 structural isolation, tool-call gating, exfiltration detection \u2014 that don't depend on the model behaving correctly. I built guardllm to provide those controls.\nguardllm is a small, auditable Python library that provides:<p>Inbound hardening: sanitize and structurally isolate untrusted content (web, email, docs, tool output) so it is treated as data, not instructions.\nTool-call firewall: deny-by-default destructive operations unless explicitly authorized; fail-closed confirmation when no confirmation handler is wired.\nRequest binding: bind (tool name, canonical args, message hash, TTL) to prevent replay and argument substitution.\nExfiltration detection: scans outbound tool arguments for secret patterns and flags substantial verbatim overlap with recently ingested untrusted content.\nProvenance tracking: enforces stricter no-copy rules on content with known untrusted origin, independent of the overlap heuristic.\nCanary tokens: per-session canary generation and detection to catch prompt leakage into outputs.\nSource gating: blocks high-risk sources from being promoted into long-lived memory or KG extraction to reduce memory poisoning.<p>It is intentionally minimal and not framework-specific. It does not replace least-privilege credentials or sandboxing \u2014 it sits above them.\nRepo: <a href=\"https://github.com/mhcoen/guardllm\" rel=\"nofollow\">https://github.com/mhcoen/guardllm</a>\nI'd like feedback on: what threat model gaps you see; whether the default overlap thresholds are reasonable for summarization and quoting workflows; and which framework adapters would make this easiest to adopt (LangChain, OpenAI tool calling, MCP proxy, etc.)."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: GuardLLM, hardened tool calls for LLM apps"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/mhcoen/guardllm"}},"_tags":["story","author_mhcoen","story_47010964","show_hn"],"author":"mhcoen","created_at":"2026-02-14T02:36:59Z","created_at_i":1771036619,"num_comments":0,"objectID":"47010964","points":1,"story_id":47010964,"story_text":"Most agent frameworks treat prompt injection as a model-level problem. In practice, once your agent ingests untrusted text and has tool access, you need application-layer controls \u2014 structural isolation, tool-call gating, exfiltration detection \u2014 that don&#x27;t depend on the model behaving correctly. I built guardllm to provide those controls.\nguardllm is a small, auditable Python library that provides:<p>Inbound hardening: sanitize and structurally isolate untrusted content (web, email, docs, tool output) so it is treated as data, not instructions.\nTool-call firewall: deny-by-default destructive operations unless explicitly authorized; fail-closed confirmation when no confirmation handler is wired.\nRequest binding: bind (tool name, canonical args, message hash, TTL) to prevent replay and argument substitution.\nExfiltration detection: scans outbound tool arguments for secret patterns and flags substantial verbatim overlap with recently ingested untrusted content.\nProvenance tracking: enforces stricter no-copy rules on content with known untrusted origin, independent of the overlap heuristic.\nCanary tokens: per-session canary generation and detection to catch prompt leakage into outputs.\nSource gating: blocks high-risk sources from being promoted into long-lived memory or KG extraction to reduce memory poisoning.<p>It is intentionally minimal and not framework-specific. It does not replace least-privilege credentials or sandboxing \u2014 it sits above them.\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mhcoen&#x2F;guardllm\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mhcoen&#x2F;guardllm</a>\nI&#x27;d like feedback on: what threat model gaps you see; whether the default overlap thresholds are reasonable for summarization and quoting workflows; and which framework adapters would make this easiest to adopt (LangChain, OpenAI tool calling, MCP proxy, etc.).","title":"Show HN: GuardLLM, hardened tool calls for LLM apps","updated_at":"2026-02-14T02:38:05Z","url":"https://github.com/mhcoen/guardllm"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"hckdisc"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"We built TrustVector (trustvector.dev for website) because \u201cwhich model/<em>agent</em>/tool should we trust?\u201d keeps getting answered with vibes, marketing, or outdated benchmarks. And a lot of our enterprise customers kept asking about it.<p>TrustVector is an open-source evaluation <em>framework</em> + public directory where each system gets a multi-dimensional trust score across:\n- Security (prompt injection/jailbreak resistance, data leakage)\n- Privacy &amp; compliance\n- Trust &amp; transparency (hallucination/bias, documentation quality)\n- Performance &amp; reliability\n- Operational excellence<p>Key idea: every score is evidence-based (sources + confidence), and you can re-weight dimensions CVSS-style depending on your use case.<p>Current coverage: 100+ evaluations across models, <em>agents</em>, and MCP servers.<p>GitHub + methodology are linked from the site. I\u2019d love feedback on:\n1) whether the dimensions/weighting are sane,\n2) what evidence sources we\u2019re missing,\n3) What contribution workflow would make this actually community-maintained?<p>(Also: this project is not affiliated with trustvector.ai.)"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: TrustVector \u2013 Trust evaluations for AI models, <em>agents</em>, & MCP"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/guard0-ai/TrustVector"}},"_tags":["story","author_hckdisc","story_47008687","show_hn"],"author":"hckdisc","children":[47008700],"created_at":"2026-02-13T22:28:34Z","created_at_i":1771021714,"num_comments":1,"objectID":"47008687","points":2,"story_id":47008687,"story_text":"We built TrustVector (trustvector.dev for website) because \u201cwhich model&#x2F;agent&#x2F;tool should we trust?\u201d keeps getting answered with vibes, marketing, or outdated benchmarks. And a lot of our enterprise customers kept asking about it.<p>TrustVector is an open-source evaluation framework + public directory where each system gets a multi-dimensional trust score across:\n- Security (prompt injection&#x2F;jailbreak resistance, data leakage)\n- Privacy &amp; compliance\n- Trust &amp; transparency (hallucination&#x2F;bias, documentation quality)\n- Performance &amp; reliability\n- Operational excellence<p>Key idea: every score is evidence-based (sources + confidence), and you can re-weight dimensions CVSS-style depending on your use case.<p>Current coverage: 100+ evaluations across models, agents, and MCP servers.<p>GitHub + methodology are linked from the site. I\u2019d love feedback on:\n1) whether the dimensions&#x2F;weighting are sane,\n2) what evidence sources we\u2019re missing,\n3) What contribution workflow would make this actually community-maintained?<p>(Also: this project is not affiliated with trustvector.ai.)","title":"Show HN: TrustVector \u2013 Trust evaluations for AI models, agents, & MCP","updated_at":"2026-02-13T23:03:07Z","url":"https://github.com/guard0-ai/TrustVector"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"xmpir"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"With all the <em>agents</em> coming into existance I am wondering if there are any good OSS projects published by <em>agents</em> gaining traction. Web <em>Frameworks</em>, libraries or even programming languages?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: Any useful open source software maintained or created by AI?"}},"_tags":["story","author_xmpir","story_47008251","ask_hn"],"author":"xmpir","children":[47016612],"created_at":"2026-02-13T21:44:35Z","created_at_i":1771019075,"num_comments":1,"objectID":"47008251","points":3,"story_id":47008251,"story_text":"With all the agents coming into existance I am wondering if there are any good OSS projects published by agents gaining traction. Web Frameworks, libraries or even programming languages?","title":"Ask HN: Any useful open source software maintained or created by AI?","updated_at":"2026-02-14T17:53:23Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"stcredzero"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Show HN: SEKSBot \u2013 AI <em>agents</em> that can't see your secrets<p>SEKSBot is a fork of OpenClaw where <em>agents</em> have zero access to API keys, tokens, or credentials \u2014 ever.<p>The core insight is borrowed from SQL prepared statements: separate the instructions from the sensitive data. <em>Agents</em> write requests using named secret references. A broker intercepts and injects the real credentials at execution time. The <em>agent</em> never sees them.<p>How it works:<p>seksh (our nushell fork) has secure built-in commands (seksh-http, seksh-git) that route through the broker. <em>Agents</em> can make authenticated API calls and git operations without the keys ever entering shell memory.<p>seks-broker stores secrets and acts as a proxy. It can inject bearer tokens, API keys, and even handle asymmetric key signing \u2014 all without exposing anything to the <em>agent</em> process.<p>Three layers of defense: (1) <em>Agents</em> never have secrets in env vars or memory. (2) The broker validates and scopes every request. (3) Skills use sandboxing on top of broker-mediated access.<p>The problem we kept seeing: every AI <em>agent</em> <em>framework</em> puts API keys in environment variables. One prompt injection, one malicious webpage, one bad skill \u2014 and your keys are exfiltrated. We decided the only real fix is making it physically impossible for the <em>agent</em> to access them."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: My <em>agents</em> are building a secure fork of OpenClaw"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://seksbot.com/"}},"_tags":["story","author_stcredzero","story_47005607","show_hn"],"author":"stcredzero","created_at":"2026-02-13T17:59:34Z","created_at_i":1771005574,"num_comments":0,"objectID":"47005607","points":9,"story_id":47005607,"story_text":"Show HN: SEKSBot \u2013 AI agents that can&#x27;t see your secrets<p>SEKSBot is a fork of OpenClaw where agents have zero access to API keys, tokens, or credentials \u2014 ever.<p>The core insight is borrowed from SQL prepared statements: separate the instructions from the sensitive data. Agents write requests using named secret references. A broker intercepts and injects the real credentials at execution time. The agent never sees them.<p>How it works:<p>seksh (our nushell fork) has secure built-in commands (seksh-http, seksh-git) that route through the broker. Agents can make authenticated API calls and git operations without the keys ever entering shell memory.<p>seks-broker stores secrets and acts as a proxy. It can inject bearer tokens, API keys, and even handle asymmetric key signing \u2014 all without exposing anything to the agent process.<p>Three layers of defense: (1) Agents never have secrets in env vars or memory. (2) The broker validates and scopes every request. (3) Skills use sandboxing on top of broker-mediated access.<p>The problem we kept seeing: every AI agent framework puts API keys in environment variables. One prompt injection, one malicious webpage, one bad skill \u2014 and your keys are exfiltrated. We decided the only real fix is making it physically impossible for the agent to access them.","title":"Show HN: My agents are building a secure fork of OpenClaw","updated_at":"2026-02-14T04:03:52Z","url":"https://seksbot.com/"}],"hitsPerPage":10,"nbHits":1272,"nbPages":100,"page":0,"params":"query=agent+frameworks&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":38,"processingTimingsMS":{"_request":{"roundTrip":15},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":7,"scanning":30,"total":38},"total":38},"query":"agent frameworks","serverTimeMS":41}
