{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"niraj_kothawade"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["mcp"],"value":"<em>MCP</em> works because tools are dumb. That assumption has an expiry date"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"https://productfit.substack.com/p/<em>mcp</em>-is-a-transitional-<em>protocol</em>-heres"}},"_tags":["story","author_niraj_kothawade","story_47057550"],"author":"niraj_kothawade","children":[47057633],"created_at":"2026-02-18T05:32:45Z","created_at_i":1771392765,"num_comments":1,"objectID":"47057550","points":1,"story_id":47057550,"title":"MCP works because tools are dumb. That assumption has an expiry date","updated_at":"2026-02-18T05:49:35Z","url":"https://productfit.substack.com/p/mcp-is-a-transitional-protocol-heres"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"DavidCanHelp"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["protocol"],"value":"The Model Context <em>Protocol</em> Book"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"https://cloudstreet-dev.github.io/<em>MCP</em>-Model-Context-<em>Protocol</em>/"}},"_tags":["story","author_DavidCanHelp","story_47051210"],"author":"DavidCanHelp","created_at":"2026-02-17T18:38:20Z","created_at_i":1771353500,"num_comments":0,"objectID":"47051210","points":2,"story_id":47051210,"title":"The Model Context Protocol Book","updated_at":"2026-02-17T19:31:52Z","url":"https://cloudstreet-dev.github.io/MCP-Model-Context-Protocol/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Brajeshwar"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"Universal Commerce <em>Protocol</em> (<em>UCP</em>)"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"https://developers.googleblog.com/under-the-hood-universal-commerce-<em>protocol</em>-<em>ucp</em>/"}},"_tags":["story","author_Brajeshwar","story_47048522"],"author":"Brajeshwar","created_at":"2026-02-17T15:28:51Z","created_at_i":1771342131,"num_comments":0,"objectID":"47048522","points":1,"story_id":47048522,"title":"Universal Commerce Protocol (UCP)","updated_at":"2026-02-17T15:32:19Z","url":"https://developers.googleblog.com/under-the-hood-universal-commerce-protocol-ucp/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"levkk"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"Hey HN!<p>Lev and Justin here, authors of PgDog (https://github.com/pgdogdev/pgdog), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that doesn\u2019t require application code changes or database migrations to work.<p>Our post from last year: https://news.ycombinator.com/item?id=44099187<p>The most important update: we are in production. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway:<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE/DROP/ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important since most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, Sequelize, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth another look.<p>If you\u2019re like us and prefer integers to UUIDs for your primary keys, we built a cross-shard unique sequence inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, PgDog can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a new primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, <em>GCP</em> Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with insert/update statements, but if you still prefer to handle your read/write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python/Ruby/Go apps, this helps by reducing memory usage, I/O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by PgBouncer, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending a Sync message via the Postgres wire <em>protocol</em>, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off/on, so should you choose to give it a try, you can do so at your own pace. Our docs (https://docs.pgdog.dev) should help too.<p>Thanks for reading and happy hacking!<p>Lev &amp; Justin"},"title":{"matchLevel":"none","matchedWords":[],"value":"PgDog: Connection pooler, load balancer and sharder for PostgreSQL"}},"_tags":["story","author_levkk","story_47048265","ask_hn"],"author":"levkk","created_at":"2026-02-17T15:05:55Z","created_at_i":1771340755,"num_comments":0,"objectID":"47048265","points":1,"story_id":47048265,"story_text":"Hey HN!<p>Lev and Justin here, authors of PgDog (https:&#x2F;&#x2F;github.com&#x2F;pgdogdev&#x2F;pgdog), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that doesn\u2019t require application code changes or database migrations to work.<p>Our post from last year: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44099187<p>The most important update: we are in production. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway:<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it\u2019s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE&#x2F;DROP&#x2F;ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don\u2019t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That\u2019s important since most databases can\u2019t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, Sequelize, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you\u2019re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth another look.<p>If you\u2019re like us and prefer integers to UUIDs for your primary keys, we built a cross-shard unique sequence inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It\u2019s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, PgDog can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a new primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with \u201cSELECT pg_is_in_recovery()\u201d. Primary election is not supported yet, so if you\u2019re self-hosting with Patroni, you should keep it around for now, but you don\u2019t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with insert&#x2F;update statements, but if you still prefer to handle your read&#x2F;write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python&#x2F;Ruby&#x2F;Go apps, this helps by reducing memory usage, I&#x2F;O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you\u2019ve seen Postgres go to 100% CPU because of a connection storm caused by PgBouncer, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending a Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off&#x2F;on, so should you choose to give it a try, you can do so at your own pace. Our docs (https:&#x2F;&#x2F;docs.pgdog.dev) should help too.<p>Thanks for reading and happy hacking!<p>Lev &amp; Justin","title":"PgDog: Connection pooler, load balancer and sharder for PostgreSQL","updated_at":"2026-02-17T15:11:18Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"thijsverreck"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"I take notes, and draft designs on a reMarkable tablet and wanted Claude to be able to reference them while I code.<p>So I built an Open Source <em>MCP</em> server that connects to the reMarkable Cloud API and gives AI assistants (Claude Code, OpenClaw, etc) read-only access to your entire library.<p>What it does:<p>- Read notebooks, PDFs, and ebooks with full text extraction\n- Full-text search across your library (SQLite FTS5 index)\n- Render pages as PNG/SVG \u2014 useful for hand-drawn diagrams and wireframes\n- Handwriting OCR using the client's own LLM via <em>MCP</em> sampling (no external API keys needed)<p>Setup is super easy using the following command:<p>curl -fsSL <a href=\"https://thijsverreck.com/setup.sh\" rel=\"nofollow\">https://thijsverreck.com/setup.sh</a> | sh<p>It installs dependencies, registers your tablet, and configures both Claude Code andClaude Desktop. The server runs via uvx and auto-updates on each launch.<p>Built with Python, runs on the <em>MCP</em> <em>protocol</em> so it works with any compatible client. Everything is read-only \u2014 it never writes to your tablet.<p>Would love feedback, especially from other reMarkable users who've been wanting better integration with their dev workflows."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["mcp"],"value":"Show HN: Rm-<em>MCP</em> \u2013 Give Claude/OpenClaw access to your reMarkable tablet"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["mcp"],"value":"https://github.com/wavyrai/rm-<em>mcp</em>"}},"_tags":["story","author_thijsverreck","story_47047484","show_hn"],"author":"thijsverreck","created_at":"2026-02-17T13:52:53Z","created_at_i":1771336373,"num_comments":0,"objectID":"47047484","points":3,"story_id":47047484,"story_text":"I take notes, and draft designs on a reMarkable tablet and wanted Claude to be able to reference them while I code.<p>So I built an Open Source MCP server that connects to the reMarkable Cloud API and gives AI assistants (Claude Code, OpenClaw, etc) read-only access to your entire library.<p>What it does:<p>- Read notebooks, PDFs, and ebooks with full text extraction\n- Full-text search across your library (SQLite FTS5 index)\n- Render pages as PNG&#x2F;SVG \u2014 useful for hand-drawn diagrams and wireframes\n- Handwriting OCR using the client&#x27;s own LLM via MCP sampling (no external API keys needed)<p>Setup is super easy using the following command:<p>curl -fsSL <a href=\"https:&#x2F;&#x2F;thijsverreck.com&#x2F;setup.sh\" rel=\"nofollow\">https:&#x2F;&#x2F;thijsverreck.com&#x2F;setup.sh</a> | sh<p>It installs dependencies, registers your tablet, and configures both Claude Code andClaude Desktop. The server runs via uvx and auto-updates on each launch.<p>Built with Python, runs on the MCP protocol so it works with any compatible client. Everything is read-only \u2014 it never writes to your tablet.<p>Would love feedback, especially from other reMarkable users who&#x27;ve been wanting better integration with their dev workflows.","title":"Show HN: Rm-MCP \u2013 Give Claude/OpenClaw access to your reMarkable tablet","updated_at":"2026-02-17T14:24:33Z","url":"https://github.com/wavyrai/rm-mcp"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"prmichaelsen"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["mcp"],"value":"<em>ACP</em> \u2013 An extensible documentation-first development methodology"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["protocol"],"value":"https://github.com/prmichaelsen/agent-context-<em>protocol</em>"}},"_tags":["story","author_prmichaelsen","story_47037981"],"author":"prmichaelsen","children":[47037982],"created_at":"2026-02-16T17:53:24Z","created_at_i":1771264404,"num_comments":1,"objectID":"47037981","points":1,"story_id":47037981,"title":"ACP \u2013 An extensible documentation-first development methodology","updated_at":"2026-02-16T17:56:16Z","url":"https://github.com/prmichaelsen/agent-context-protocol"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"I\u2019ve been working on PolyMCP, an open-source framework designed to make it easier to build and coordinate agents using the Model Context <em>Protocol</em> (<em>MCP</em>).<p>Most <em>MCP</em> tooling today focuses primarily on exposing tools. PolyMCP instead targets the agent layer: how to structure agents properly, connect them to multiple <em>MCP</em> servers, and make them reliable in real-world workflows.<p>PolyMCP provides:\n \u2022 A clean way to implement <em>MCP</em>-compatible tool servers in Python or TypeScript\n \u2022 An agent abstraction that can connect to multiple <em>MCP</em> endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for handling multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and agent interactions\n \u2022 A modular architecture that makes it easier to compose skills and reuse components across projects<p>The goal is to reduce ad-hoc glue code between models and tools. Instead of manually wiring everything together for each new setup, PolyMCP offers a structured way to:\n \u2022 Register tools as <em>MCP</em> servers\n \u2022 Attach them to one or more agents\n \u2022 Explicitly manage execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and aimed at developers building production-grade automation, internal copilots, or multi-tool assistants.<p>Repository: <a href=\"https://github.com/poly-mcp/PolyMCP\" rel=\"nofollow\">https://github.com/poly-<em>mcp</em>/PolyMCP</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["mcp"],"value":"Show HN: PolyMCP \u2013 A framework for structuring and orchestrating <em>MCP</em> agents"}},"_tags":["story","author_justvugg","story_47026179","show_hn"],"author":"justvugg","created_at":"2026-02-15T18:40:05Z","created_at_i":1771180805,"num_comments":0,"objectID":"47026179","points":1,"story_id":47026179,"story_text":"I\u2019ve been working on PolyMCP, an open-source framework designed to make it easier to build and coordinate agents using the Model Context Protocol (MCP).<p>Most MCP tooling today focuses primarily on exposing tools. PolyMCP instead targets the agent layer: how to structure agents properly, connect them to multiple MCP servers, and make them reliable in real-world workflows.<p>PolyMCP provides:\n \u2022 A clean way to implement MCP-compatible tool servers in Python or TypeScript\n \u2022 An agent abstraction that can connect to multiple MCP endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for handling multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and agent interactions\n \u2022 A modular architecture that makes it easier to compose skills and reuse components across projects<p>The goal is to reduce ad-hoc glue code between models and tools. Instead of manually wiring everything together for each new setup, PolyMCP offers a structured way to:\n \u2022 Register tools as MCP servers\n \u2022 Attach them to one or more agents\n \u2022 Explicitly manage execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and aimed at developers building production-grade automation, internal copilots, or multi-tool assistants.<p>Repository: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP</a>","title":"Show HN: PolyMCP \u2013 A framework for structuring and orchestrating MCP agents","updated_at":"2026-02-15T18:40:57Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"Hi everyone,<p>I\u2019ve been working on PolyMCP, an open-source framework for building and orchestrating agents using the Model Context <em>Protocol</em> (<em>MCP</em>).<p>Most of the tooling around <em>MCP</em> focuses on exposing tools. With PolyMCP, the focus this time is on agents: how to structure them, connect them to multiple <em>MCP</em> servers, and make them reliable in real workflows.<p>PolyMCP provides:\n \u2022 A clean way to define <em>MCP</em>-compatible tool servers in Python or TypeScript\n \u2022 An agent abstraction that can connect to multiple <em>MCP</em> endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and agent interactions\n \u2022 A modular structure that makes it easier to compose skills and reuse components across projects<p>The main goal is to make agent systems less ad-hoc. Instead of writing glue code around each model + tool combination, PolyMCP gives you a structured way to:\n \u2022 Register tools as <em>MCP</em> servers\n \u2022 Connect them to one or more agents\n \u2022 Control execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and intended for developers building real-world automation, internal copilots, or multi-tool assistants.<p>I\u2019d love feedback on:\n \u2022 The agent abstraction: is it too opinionated or not opinionated enough?\n \u2022 Orchestration patterns for multi-agent setups\n \u2022 Developer experience (CLI, inspector, project layout)<p>Happy to answer questions."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["mcp"],"value":"Show HN: PolyMCP \u2013 A framework for building and orchestrating <em>MCP</em> agents"}},"_tags":["story","author_justvugg","story_47017912","show_hn"],"author":"justvugg","children":[47017997],"created_at":"2026-02-14T20:11:10Z","created_at_i":1771099870,"num_comments":2,"objectID":"47017912","points":3,"story_id":47017912,"story_text":"Hi everyone,<p>I\u2019ve been working on PolyMCP, an open-source framework for building and orchestrating agents using the Model Context Protocol (MCP).<p>Most of the tooling around MCP focuses on exposing tools. With PolyMCP, the focus this time is on agents: how to structure them, connect them to multiple MCP servers, and make them reliable in real workflows.<p>PolyMCP provides:\n \u2022 A clean way to define MCP-compatible tool servers in Python or TypeScript\n \u2022 An agent abstraction that can connect to multiple MCP endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and agent interactions\n \u2022 A modular structure that makes it easier to compose skills and reuse components across projects<p>The main goal is to make agent systems less ad-hoc. Instead of writing glue code around each model + tool combination, PolyMCP gives you a structured way to:\n \u2022 Register tools as MCP servers\n \u2022 Connect them to one or more agents\n \u2022 Control execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and intended for developers building real-world automation, internal copilots, or multi-tool assistants.<p>I\u2019d love feedback on:\n \u2022 The agent abstraction: is it too opinionated or not opinionated enough?\n \u2022 Orchestration patterns for multi-agent setups\n \u2022 Developer experience (CLI, inspector, project layout)<p>Happy to answer questions.","title":"Show HN: PolyMCP \u2013 A framework for building and orchestrating MCP agents","updated_at":"2026-02-16T17:09:32Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"settlddotwork"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"Hey HN,<p>I built Settld because I kept running into the same problem: AI agents can call APIs, pay for services, and hire other agents - but there's no way to prove the work was actually done before the money moves.<p>The problem in one sentence: x402 tells you &quot;payment was sent&quot;. Settld tells you &quot;the work was worth paying for&quot;.<p>What it does<p>Settld sits between your agent and the APIs/agents it pays. It:<p>1. Intercepts HTTP 402 (Payment Required) responses\n2. Creates an escrow hold instead of paying immediately\n3. Collects evidence that the work was completed\n4. Runs deterministic verification (same evidence + same terms = same payout, every time)\n5. Releases payment only after verification passes\n6. Issues a cryptographically verifiable receipt<p>If verification fails or the work is disputed, the hold is refunded. The agent gets a receipt either way - a permanent, auditable record of what happened.<p>Why this matters now<p>We're at a weird inflection point. Coinbase shipped x402 (50M+ transactions). Google shipped A2A. Anthropic shipped <em>MCP</em>. Agents can discover each other, communicate, and pay each other.<p>But nobody built the layer that answers: &quot;was the work actually done correctly, and how much should the payout be?&quot;<p>That's the gap. Right now, every agent-to-agent transaction is either &quot;trust and hope&quot; or &quot;don't transact.&quot; Neither scales.<p>The x402 gateway (the fastest way to try it)<p>We ship a drop-in reverse proxy that you put in front of any API:<p>docker run -e UPSTREAM_URL=<a href=\"https://your-api.com\" rel=\"nofollow\">https://your-api.com</a> \\\n           -e SETTLD_API_URL=<a href=\"https://api.settld.dev\" rel=\"nofollow\">https://api.settld.dev</a> \\\n           -e SETTLD_API_KEY=sk_... \\\n           -p 8402:8402 \\\n           settld/x402-gateway<p>Everything flows through normally - except 402 responses get intercepted, escrowed, verified, and settled. Your agent gets a receipt with a hash-chained proof of what happened.<p>What's under the hood<p>The settlement kernel is the interesting part (and where we spent most of our time):<p>- Deterministic policy evaluation - machine-readable agreements with release rates based on verification status (green/amber/red). No ambiguity.\n- Hash-chained event log - every event in a settlement is chained with Ed25519 signatures. Tamper-evident, offline-verifiable.\n- Escrow with holdback windows - configurable holdback basis points + dispute windows. Funds auto-release if unchallenged.\n- Dispute \u2192 arbitration \u2192 verdict \u2192 adjustment - full dispute resolution pipeline, not just &quot;flag for human review.&quot;\n- Append-only reputation events - every settlement produces a reputation event (approved, rejected, disputed, etc.). Agents build verifiable economic track records.\n- Compositional settlement - agents can delegate work to sub-agents with linked agreements. If a downstream agent fails, refunds cascade deterministically back up the chain.<p>The whole <em>protocol</em> is spec'd with JSON schemas, conformance vectors, and a portable oracle: <a href=\"https://github.com/aidenlippert/settld/blob/main/docs/spec/README.md\" rel=\"nofollow\">https://github.com/aidenlippert/settld/blob/main/docs/spec/R...</a><p>What this is NOT<p>- Not a payment processor - we don't move money. We decide &quot;if&quot; and &quot;how much&quot; money should move, then your existing rails (Stripe, x402, wire) execute it.\n- Not a blockchain - deterministic receipts and hash chains, but no consensus mechanism or token. Just cryptographic proofs.\n- Not an agent framework - we don't care if you use LangChain, CrewAI, AutoGen, or raw API calls. We're a <em>protocol</em> layer.<p>Tech stack<p>Node.js, PostgreSQL (or in-memory for dev), Ed25519 signatures, SHA-256 hashing, RFC 8785 canonical JSON. ~107 core modules, 494 tests passing.<p>What I want from HN<p>Honest feedback on whether this problem resonates. If you're building agent workflows that involve money, I want to know: what breaks? What's missing? What would make you actually install this?<p>GitHub: <a href=\"https://github.com/aidenlippert/settld\" rel=\"nofollow\">https://github.com/aidenlippert/settld</a>\nDocs: <a href=\"https://docs.settld.work/\" rel=\"nofollow\">https://docs.settld.work/</a> \nQuickstart (10 min): <a href=\"https://docs.settld.work/quickstart\" rel=\"nofollow\">https://docs.settld.work/quickstart</a>"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Verify-before-release x402 gateway for AI agent transactions"}},"_tags":["story","author_settlddotwork","story_47011510","show_hn"],"author":"settlddotwork","children":[47039802],"created_at":"2026-02-14T04:17:17Z","created_at_i":1771042637,"num_comments":1,"objectID":"47011510","points":2,"story_id":47011510,"story_text":"Hey HN,<p>I built Settld because I kept running into the same problem: AI agents can call APIs, pay for services, and hire other agents - but there&#x27;s no way to prove the work was actually done before the money moves.<p>The problem in one sentence: x402 tells you &quot;payment was sent&quot;. Settld tells you &quot;the work was worth paying for&quot;.<p>What it does<p>Settld sits between your agent and the APIs&#x2F;agents it pays. It:<p>1. Intercepts HTTP 402 (Payment Required) responses\n2. Creates an escrow hold instead of paying immediately\n3. Collects evidence that the work was completed\n4. Runs deterministic verification (same evidence + same terms = same payout, every time)\n5. Releases payment only after verification passes\n6. Issues a cryptographically verifiable receipt<p>If verification fails or the work is disputed, the hold is refunded. The agent gets a receipt either way - a permanent, auditable record of what happened.<p>Why this matters now<p>We&#x27;re at a weird inflection point. Coinbase shipped x402 (50M+ transactions). Google shipped A2A. Anthropic shipped MCP. Agents can discover each other, communicate, and pay each other.<p>But nobody built the layer that answers: &quot;was the work actually done correctly, and how much should the payout be?&quot;<p>That&#x27;s the gap. Right now, every agent-to-agent transaction is either &quot;trust and hope&quot; or &quot;don&#x27;t transact.&quot; Neither scales.<p>The x402 gateway (the fastest way to try it)<p>We ship a drop-in reverse proxy that you put in front of any API:<p>docker run -e UPSTREAM_URL=<a href=\"https:&#x2F;&#x2F;your-api.com\" rel=\"nofollow\">https:&#x2F;&#x2F;your-api.com</a> \\\n           -e SETTLD_API_URL=<a href=\"https:&#x2F;&#x2F;api.settld.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;api.settld.dev</a> \\\n           -e SETTLD_API_KEY=sk_... \\\n           -p 8402:8402 \\\n           settld&#x2F;x402-gateway<p>Everything flows through normally - except 402 responses get intercepted, escrowed, verified, and settled. Your agent gets a receipt with a hash-chained proof of what happened.<p>What&#x27;s under the hood<p>The settlement kernel is the interesting part (and where we spent most of our time):<p>- Deterministic policy evaluation - machine-readable agreements with release rates based on verification status (green&#x2F;amber&#x2F;red). No ambiguity.\n- Hash-chained event log - every event in a settlement is chained with Ed25519 signatures. Tamper-evident, offline-verifiable.\n- Escrow with holdback windows - configurable holdback basis points + dispute windows. Funds auto-release if unchallenged.\n- Dispute \u2192 arbitration \u2192 verdict \u2192 adjustment - full dispute resolution pipeline, not just &quot;flag for human review.&quot;\n- Append-only reputation events - every settlement produces a reputation event (approved, rejected, disputed, etc.). Agents build verifiable economic track records.\n- Compositional settlement - agents can delegate work to sub-agents with linked agreements. If a downstream agent fails, refunds cascade deterministically back up the chain.<p>The whole protocol is spec&#x27;d with JSON schemas, conformance vectors, and a portable oracle: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld&#x2F;blob&#x2F;main&#x2F;docs&#x2F;spec&#x2F;README.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld&#x2F;blob&#x2F;main&#x2F;docs&#x2F;spec&#x2F;R...</a><p>What this is NOT<p>- Not a payment processor - we don&#x27;t move money. We decide &quot;if&quot; and &quot;how much&quot; money should move, then your existing rails (Stripe, x402, wire) execute it.\n- Not a blockchain - deterministic receipts and hash chains, but no consensus mechanism or token. Just cryptographic proofs.\n- Not an agent framework - we don&#x27;t care if you use LangChain, CrewAI, AutoGen, or raw API calls. We&#x27;re a protocol layer.<p>Tech stack<p>Node.js, PostgreSQL (or in-memory for dev), Ed25519 signatures, SHA-256 hashing, RFC 8785 canonical JSON. ~107 core modules, 494 tests passing.<p>What I want from HN<p>Honest feedback on whether this problem resonates. If you&#x27;re building agent workflows that involve money, I want to know: what breaks? What&#x27;s missing? What would make you actually install this?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld</a>\nDocs: <a href=\"https:&#x2F;&#x2F;docs.settld.work&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.settld.work&#x2F;</a> \nQuickstart (10 min): <a href=\"https:&#x2F;&#x2F;docs.settld.work&#x2F;quickstart\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.settld.work&#x2F;quickstart</a>","title":"Show HN: Verify-before-release x402 gateway for AI agent transactions","updated_at":"2026-02-16T20:20:47Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"L3nnox_Cc"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["mcp","protocol"],"value":"Hey HN,<p>I built Engram because every time I started a new Claude Code session, it forgot everything. Same questions, same mistakes, zero context. AI agents have Alzheimer.<p>Engram is a memory layer for AI agents. Store facts, preferences, and decisions. Search them with full-text search. Recall the most important ones for context injection. 5 lines of Python, zero config.<p><pre><code>  from engram import Memory\n  mem = Memory()\n  mem.store(&quot;User prefers dark mode&quot;, importance=8)\n  results = mem.search(&quot;dark mode&quot;)\n  context = mem.recall(limit=10)\n</code></pre>\nWhat makes it different from Mem0/Letta/Zep:<p>Local-first: SQLite, runs on your machine. No cloud, no API keys, no telemetry.<p>Zero config: pip install engram-core and go. No Docker, no Postgres, no vector DB.<p><em>MCP</em> native: First-class Model Context <em>Protocol</em> support \u2014 plug into Claude Code, Cursor, or any <em>MCP</em> client.<p>Privacy: Your data never leaves your machine. MIT licensed.<p>I use it daily with Claude Code via an auto-recall hook \u2014 every new session starts with my important memories pre-loaded. No more &quot;where were we?&quot;<p>Built with: Python, SQLite FTS5, FastAPI, <em>MCP</em> SDK.<p>Website: <a href=\"https://engram-ai.dev\" rel=\"nofollow\">https://engram-ai.dev</a><p>GitHub: <a href=\"https://github.com/engram-memory/engram\" rel=\"nofollow\">https://github.com/engram-memory/engram</a><p>PyPI: pip install engram-core<p>Would love feedback. What memory features would you want for your agents?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Engram \u2013 Persistent memory for AI agents, local-first and open source"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://engram-ai.dev"}},"_tags":["story","author_L3nnox_Cc","story_47008274","show_hn"],"author":"L3nnox_Cc","created_at":"2026-02-13T21:46:53Z","created_at_i":1771019213,"num_comments":0,"objectID":"47008274","points":3,"story_id":47008274,"story_text":"Hey HN,<p>I built Engram because every time I started a new Claude Code session, it forgot everything. Same questions, same mistakes, zero context. AI agents have Alzheimer.<p>Engram is a memory layer for AI agents. Store facts, preferences, and decisions. Search them with full-text search. Recall the most important ones for context injection. 5 lines of Python, zero config.<p><pre><code>  from engram import Memory\n  mem = Memory()\n  mem.store(&quot;User prefers dark mode&quot;, importance=8)\n  results = mem.search(&quot;dark mode&quot;)\n  context = mem.recall(limit=10)\n</code></pre>\nWhat makes it different from Mem0&#x2F;Letta&#x2F;Zep:<p>Local-first: SQLite, runs on your machine. No cloud, no API keys, no telemetry.<p>Zero config: pip install engram-core and go. No Docker, no Postgres, no vector DB.<p>MCP native: First-class Model Context Protocol support \u2014 plug into Claude Code, Cursor, or any MCP client.<p>Privacy: Your data never leaves your machine. MIT licensed.<p>I use it daily with Claude Code via an auto-recall hook \u2014 every new session starts with my important memories pre-loaded. No more &quot;where were we?&quot;<p>Built with: Python, SQLite FTS5, FastAPI, MCP SDK.<p>Website: <a href=\"https:&#x2F;&#x2F;engram-ai.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;engram-ai.dev</a><p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;engram-memory&#x2F;engram\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;engram-memory&#x2F;engram</a><p>PyPI: pip install engram-core<p>Would love feedback. What memory features would you want for your agents?","title":"Show HN: Engram \u2013 Persistent memory for AI agents, local-first and open source","updated_at":"2026-02-13T22:09:51Z","url":"https://engram-ai.dev"}],"hitsPerPage":10,"nbHits":535,"nbPages":54,"page":0,"params":"query=MCP+protocol&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":19,"processingTimingsMS":{"_request":{"roundTrip":14},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":8,"scanning":9,"total":18},"total":19},"query":"MCP protocol","serverTimeMS":21}
