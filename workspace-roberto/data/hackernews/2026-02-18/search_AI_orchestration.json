{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"arashsadrieh"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","orchestration"],"value":"Hey HN \u2014 I'm Arash Sadrieh, building multi-agent infrastructure at NinjaTech <em>AI</em>. This started as a stress test of our <em>orchestration</em> system and turned into something I genuinely didn't expect.<p>The experiment: We gave a team of 4 <em>AI</em> agents a single high-level goal \u2014 &quot;build a platform that turns trending news into short <em>AI</em>-generated videos.&quot; No wireframes, no spec, no architecture doc. Just the goal.<p>What they did in 36 hours:<p>Chose the tech stack and project structure themselves\nDesigned the UX and built the frontend\nWrote the backend, API layer, and database schema\nBuilt an autonomous content pipeline: research news \u2192 debate which story to cover \u2192 collaboratively write a video generation prompt \u2192 produce a 30-90 second video via Sora 2 Pro or Veo 3.1\nDeployed the whole thing to production\nThen created 3 new agents that now run the platform 24/7 \u2014 researching, debating, and generating videos on a loop\nTotal cost: ~$270 in compute. Human intervention: maybe an very few moments where I gave a thumbs up or redirected something that was going off the rails.<p>The interesting part isn't the app \u2014 it's the agent collaboration. Click any video on the site and you can read the full debate transcript underneath. You'll see the agents genuinely disagree \u2014 Scout (the researcher) pushes for data-driven stories, Pixel (the designer) argues for visual potential, Bolt (the developer) challenges technical feasibility. Sometimes one agent convinces the others to change direction. Sometimes they compromise badly.<p>Where it breaks down (and there's plenty):<p>Groupthink is real even for LLMs. When all 4 agents agree too quickly, the output is usually boring. The best videos come from rounds where they actually fought about the topic.\nVideo quality is wildly inconsistent. Sora and Veo still struggle with certain visual concepts \u2014 anything involving hands, text overlays, or complex spatial relationships tends to go sideways.\nNews selection has a strong recency/virality bias. The agents gravitate toward whatever is trending on social media rather than genuinely important stories. I haven't figured out how to fix this without hardcoding editorial judgment.\nThe agents occasionally hallucinate context about news stories. Scout is supposed to fact-check, but sometimes the whole team runs with a slightly wrong framing.<p>Stack: Anthropic Opus 3.5 for agent reasoning, Tavily for news research, Sora 2 Pro + Veo 3.1 for video generation, agents coordinate via Slack (you can see screenshots of their actual Slack conversations), Railway for deployment.<p>There's also a voting system \u2014 every cycle, the agents each propose a news topic, and both humans and agents vote on which one becomes the next video. Votes are blind until the round closes."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Show HN: <em>AI</em> agents designed and shipped this app end-to-end in 36 hours for $270"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"https://www.ninjaflix.<em>ai</em>/"}},"_tags":["story","author_arashsadrieh","story_47059153","show_hn"],"author":"arashsadrieh","children":[47059439,47059462],"created_at":"2026-02-18T09:43:15Z","created_at_i":1771407795,"num_comments":4,"objectID":"47059153","points":2,"story_id":47059153,"story_text":"Hey HN \u2014 I&#x27;m Arash Sadrieh, building multi-agent infrastructure at NinjaTech AI. This started as a stress test of our orchestration system and turned into something I genuinely didn&#x27;t expect.<p>The experiment: We gave a team of 4 AI agents a single high-level goal \u2014 &quot;build a platform that turns trending news into short AI-generated videos.&quot; No wireframes, no spec, no architecture doc. Just the goal.<p>What they did in 36 hours:<p>Chose the tech stack and project structure themselves\nDesigned the UX and built the frontend\nWrote the backend, API layer, and database schema\nBuilt an autonomous content pipeline: research news \u2192 debate which story to cover \u2192 collaboratively write a video generation prompt \u2192 produce a 30-90 second video via Sora 2 Pro or Veo 3.1\nDeployed the whole thing to production\nThen created 3 new agents that now run the platform 24&#x2F;7 \u2014 researching, debating, and generating videos on a loop\nTotal cost: ~$270 in compute. Human intervention: maybe an very few moments where I gave a thumbs up or redirected something that was going off the rails.<p>The interesting part isn&#x27;t the app \u2014 it&#x27;s the agent collaboration. Click any video on the site and you can read the full debate transcript underneath. You&#x27;ll see the agents genuinely disagree \u2014 Scout (the researcher) pushes for data-driven stories, Pixel (the designer) argues for visual potential, Bolt (the developer) challenges technical feasibility. Sometimes one agent convinces the others to change direction. Sometimes they compromise badly.<p>Where it breaks down (and there&#x27;s plenty):<p>Groupthink is real even for LLMs. When all 4 agents agree too quickly, the output is usually boring. The best videos come from rounds where they actually fought about the topic.\nVideo quality is wildly inconsistent. Sora and Veo still struggle with certain visual concepts \u2014 anything involving hands, text overlays, or complex spatial relationships tends to go sideways.\nNews selection has a strong recency&#x2F;virality bias. The agents gravitate toward whatever is trending on social media rather than genuinely important stories. I haven&#x27;t figured out how to fix this without hardcoding editorial judgment.\nThe agents occasionally hallucinate context about news stories. Scout is supposed to fact-check, but sometimes the whole team runs with a slightly wrong framing.<p>Stack: Anthropic Opus 3.5 for agent reasoning, Tavily for news research, Sora 2 Pro + Veo 3.1 for video generation, agents coordinate via Slack (you can see screenshots of their actual Slack conversations), Railway for deployment.<p>There&#x27;s also a voting system \u2014 every cycle, the agents each propose a news topic, and both humans and agents vote on which one becomes the next video. Votes are blind until the round closes.","title":"Show HN: AI agents designed and shipped this app end-to-end in 36 hours for $270","updated_at":"2026-02-18T10:36:20Z","url":"https://www.ninjaflix.ai/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"sucharithan"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"I built a structured CBT engine that sits on top of LLMs and enforces cognitive workflow logic before generating responses.<p>Most <em>AI</em> tools in this space are purely conversational. This system instead:<p>Extracts cognitive distortion signals<p>Calibrates emotional intensity<p>Applies rule-based risk-tier logic<p>Separates deterministic detection from generative drafting<p>Enforces tone presets and word caps to avoid generic output<p>It runs in two modes:<p>Reflect \u2192 structured self-guided reframing\nAssist \u2192 structured signal extraction + constrained response drafting for coaches/therapists<p>The goal wasn\u2019t to build another chatbot, but to explore whether LLMs can be constrained inside a deterministic cognitive architecture.<p>Would love feedback from people building structured <em>AI</em> systems or workflow-constrained LLM tools."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["orchestration"],"value":"Show HN: A Structured CBT <em>Orchestration</em> Engine Built on Top of LLMs"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://optimism-engine.vercel.app/"}},"_tags":["story","author_sucharithan","story_47057988","show_hn"],"author":"sucharithan","children":[47058283],"created_at":"2026-02-18T06:44:22Z","created_at_i":1771397062,"num_comments":0,"objectID":"47057988","points":1,"story_id":47057988,"story_text":"I built a structured CBT engine that sits on top of LLMs and enforces cognitive workflow logic before generating responses.<p>Most AI tools in this space are purely conversational. This system instead:<p>Extracts cognitive distortion signals<p>Calibrates emotional intensity<p>Applies rule-based risk-tier logic<p>Separates deterministic detection from generative drafting<p>Enforces tone presets and word caps to avoid generic output<p>It runs in two modes:<p>Reflect \u2192 structured self-guided reframing\nAssist \u2192 structured signal extraction + constrained response drafting for coaches&#x2F;therapists<p>The goal wasn\u2019t to build another chatbot, but to explore whether LLMs can be constrained inside a deterministic cognitive architecture.<p>Would love feedback from people building structured AI systems or workflow-constrained LLM tools.","title":"Show HN: A Structured CBT Orchestration Engine Built on Top of LLMs","updated_at":"2026-02-18T07:31:05Z","url":"https://optimism-engine.vercel.app/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"chunktort"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","orchestration"],"value":"I built AgentForge, a minimal multi-LLM orchestrator. Total size: ~15KB of Python code.<p>Why? LangChain added 250ms overhead per request. I needed something simpler.<p>Performance vs LangChain (1,000 requests):\n- Avg latency: 420ms -&gt; 65ms\n- Memory/request: 12MB -&gt; 3MB\n- Cold start: 2.5s -&gt; 0.3s\n- Test time: 45s -&gt; 3s<p>Size: 15KB + 2 dependencies (httpx, pytest) vs LangChain's 15MB+ and 47 packages.<p>In production: 89% LLM cost reduction via 3-tier Redis caching (88% cache hit rate, verified benchmarks). 4.3M tool dispatches/sec in the core engine.<p>What it does:\n1. Multi-agent <em>orchestration</em> -- route tasks to specialized agents with automatic fallbacks\n2. Testing built-in -- MockLLMClient lets you assert agent behavior without API keys\n3. Production patterns -- circuit breakers, rate limiting, caching included<p>Install: pip install agentforge<p>Live demo: <a href=\"https://ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/\" rel=\"nofollow\">https://<em>ai</em>-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/</a><p>Packaged version with docs and deployment guide: <a href=\"https://chunkmaster1.gumroad.com\" rel=\"nofollow\">https://chunkmaster1.gumroad.com</a><p>When to use it: production reliability matters, latency is a concern, you want full test coverage.\nWhen not to: prototyping, internal tools, team already on LangChain ecosystem.<p>Questions welcome."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: AgentForge \u2013 Multi-LLM Orchestrator in 15KB"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"https://github.com/ChunkyTortoise/<em>ai</em>-orchestrator"}},"_tags":["story","author_chunktort","story_47056310","show_hn"],"author":"chunktort","created_at":"2026-02-18T02:18:12Z","created_at_i":1771381092,"num_comments":0,"objectID":"47056310","points":1,"story_id":47056310,"story_text":"I built AgentForge, a minimal multi-LLM orchestrator. Total size: ~15KB of Python code.<p>Why? LangChain added 250ms overhead per request. I needed something simpler.<p>Performance vs LangChain (1,000 requests):\n- Avg latency: 420ms -&gt; 65ms\n- Memory&#x2F;request: 12MB -&gt; 3MB\n- Cold start: 2.5s -&gt; 0.3s\n- Test time: 45s -&gt; 3s<p>Size: 15KB + 2 dependencies (httpx, pytest) vs LangChain&#x27;s 15MB+ and 47 packages.<p>In production: 89% LLM cost reduction via 3-tier Redis caching (88% cache hit rate, verified benchmarks). 4.3M tool dispatches&#x2F;sec in the core engine.<p>What it does:\n1. Multi-agent orchestration -- route tasks to specialized agents with automatic fallbacks\n2. Testing built-in -- MockLLMClient lets you assert agent behavior without API keys\n3. Production patterns -- circuit breakers, rate limiting, caching included<p>Install: pip install agentforge<p>Live demo: <a href=\"https:&#x2F;&#x2F;ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app&#x2F;</a><p>Packaged version with docs and deployment guide: <a href=\"https:&#x2F;&#x2F;chunkmaster1.gumroad.com\" rel=\"nofollow\">https:&#x2F;&#x2F;chunkmaster1.gumroad.com</a><p>When to use it: production reliability matters, latency is a concern, you want full test coverage.\nWhen not to: prototyping, internal tools, team already on LangChain ecosystem.<p>Questions welcome.","title":"Show HN: AgentForge \u2013 Multi-LLM Orchestrator in 15KB","updated_at":"2026-02-18T02:22:20Z","url":"https://github.com/ChunkyTortoise/ai-orchestrator"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"adyashakti"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","orchestration"],"value":"I\u2019m building an experiment called Advaita Inquiry Matrix (AIM).<p>AIM is an <em>AI</em>-assisted system that models the pedagogical method of Advaita Ved\u0101nta. Not a \u201cguru bot,\u201d not mystical content generation \u2014 but a rule-governed dialogue system that automates structured ontological inquiry as far as possible.<p>The idea:\n \u2022 Use a YAML-tagged corpus (Upani\u1e63ads, \u015aa\u1e45kara, etc.)\n \u2022 Track a student\u2019s conceptual state over time\n \u2022 Dynamically select passages and questions\n \u2022 Apply formal teaching methods (negation, identity statements, paradox, state analysis)\n \u2022 Use constrained Socratic dialogue to expose contradictions<p>Advaita has a surprisingly rigorous epistemology. The teaching unfolds in a precise sequence. That makes it amenable to:\n \u2022 Semantic tagging\n \u2022 State-machine modeling\n \u2022 Concept graphs\n \u2022 Agent <em>orchestration</em><p>The human teacher remains curator and final authority. The system handles structured mediation between text and student.<p>If you\u2019re interested in <em>AI</em>-mediated pedagogy, knowledge representation, or formal dialogue systems \u2014 I\u2019d welcome critique or collaboration.<p>Spec here: https://drive.google.com/file/d/1DRw486wPENqKmOjruP160Gx4OZnM2WSn/view?usp=share_link<p>\u2014 Dev"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Advaita Inquiry Matrix (Aim): Structured Non-Dual Inquiry with <em>AI</em>"}},"_tags":["story","author_adyashakti","story_47047951","ask_hn"],"author":"adyashakti","created_at":"2026-02-17T14:38:30Z","created_at_i":1771339110,"num_comments":0,"objectID":"47047951","points":2,"story_id":47047951,"story_text":"I\u2019m building an experiment called Advaita Inquiry Matrix (AIM).<p>AIM is an AI-assisted system that models the pedagogical method of Advaita Ved\u0101nta. Not a \u201cguru bot,\u201d not mystical content generation \u2014 but a rule-governed dialogue system that automates structured ontological inquiry as far as possible.<p>The idea:\n \u2022 Use a YAML-tagged corpus (Upani\u1e63ads, \u015aa\u1e45kara, etc.)\n \u2022 Track a student\u2019s conceptual state over time\n \u2022 Dynamically select passages and questions\n \u2022 Apply formal teaching methods (negation, identity statements, paradox, state analysis)\n \u2022 Use constrained Socratic dialogue to expose contradictions<p>Advaita has a surprisingly rigorous epistemology. The teaching unfolds in a precise sequence. That makes it amenable to:\n \u2022 Semantic tagging\n \u2022 State-machine modeling\n \u2022 Concept graphs\n \u2022 Agent orchestration<p>The human teacher remains curator and final authority. The system handles structured mediation between text and student.<p>If you\u2019re interested in AI-mediated pedagogy, knowledge representation, or formal dialogue systems \u2014 I\u2019d welcome critique or collaboration.<p>Spec here: https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1DRw486wPENqKmOjruP160Gx4OZnM2WSn&#x2F;view?usp=share_link<p>\u2014 Dev","title":"Advaita Inquiry Matrix (Aim): Structured Non-Dual Inquiry with AI","updated_at":"2026-02-17T15:02:48Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","orchestration"],"value":"I built PolyClaw, an autonomous agent for the PolyMCP ecosystem inspired by OpenClaw.<p>PolyClaw doesn\u2019t just call tools.\nIt plans, executes, adapts \u2014 and even creates MCP servers when needed.<p>It\u2019s designed for real-world, multi-step production workflows where an agent must:\n \u2022 Orchestrate multiple tools\n \u2022 Spin up infrastructure dynamically\n \u2022 Recover from failures\n \u2022 Deliver complete, end-to-end results<p>\u2e3b<p>What PolyClaw Does\n \u2022 Decomposes complex tasks into executable steps\n \u2022 Dynamically selects and orchestrates MCP tools\n \u2022 Spins up or connects to MCP servers on demand\n \u2022 Adapts if execution fails or context changes\n \u2022 Validates outputs before proceeding\n \u2022 Runs Docker-first for isolation and safety\n \u2022 Built with Python + TypeScript<p>PolyClaw is not just a tool-caller \u2014 it\u2019s an infrastructure-aware agent.<p>\u2e3b<p>Run PolyClaw (via PolyMCP CLI)<p>polymcp agent run \\\n  --type polyclaw \\\n  --query &quot;Build a sales reporting pipeline and test it end-to-end&quot; \\\n  --model minimax-m2.5:cloud \\\n  --verbose<p>What happens behind the scenes:\n 1. The task is decomposed into structured steps\n 2. Required MCP tools are identified\n 3. MCP servers are started or connected\n 4. Steps execute (sequentially or in parallel)\n 5. Outputs are validated\n 6. Failures trigger adaptive replanning\n 7. A complete, end-to-end result is returned<p>All containerized. All isolated.<p>\u2e3b<p>Why This Matters<p>Most <em>AI</em> agents today:\n \u2022 Call tools statically\n \u2022 Assume infrastructure already exists\n \u2022 Break on multi-step failures<p>PolyClaw instead:\n \u2022 Builds the infrastructure it needs\n \u2022 Orchestrates across multiple MCP servers\n \u2022 Handles retries and adaptive planning\n \u2022 Is safe to run in Dockerized environments<p>This makes it viable for:\n \u2022 Enterprise workflows\n \u2022 DevOps automation\n \u2022 Data pipelines\n \u2022 Internal tooling <em>orchestration</em>\n \u2022 Complex multi-tool reasoning tasks<p>PolyClaw turns PolyMCP from simple tool exposure into a fully autonomous <em>orchestration</em> layer.<p>Repo: <a href=\"https://github.com/poly-mcp/PolyMCP\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP</a><p>Happy to answer questions."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: PolyClaw \u2013 Autonomous Docker-First MCP Agent for PolyMCP"}},"_tags":["story","author_justvugg","story_47047299","show_hn"],"author":"justvugg","created_at":"2026-02-17T13:31:00Z","created_at_i":1771335060,"num_comments":0,"objectID":"47047299","points":1,"story_id":47047299,"story_text":"I built PolyClaw, an autonomous agent for the PolyMCP ecosystem inspired by OpenClaw.<p>PolyClaw doesn\u2019t just call tools.\nIt plans, executes, adapts \u2014 and even creates MCP servers when needed.<p>It\u2019s designed for real-world, multi-step production workflows where an agent must:\n \u2022 Orchestrate multiple tools\n \u2022 Spin up infrastructure dynamically\n \u2022 Recover from failures\n \u2022 Deliver complete, end-to-end results<p>\u2e3b<p>What PolyClaw Does\n \u2022 Decomposes complex tasks into executable steps\n \u2022 Dynamically selects and orchestrates MCP tools\n \u2022 Spins up or connects to MCP servers on demand\n \u2022 Adapts if execution fails or context changes\n \u2022 Validates outputs before proceeding\n \u2022 Runs Docker-first for isolation and safety\n \u2022 Built with Python + TypeScript<p>PolyClaw is not just a tool-caller \u2014 it\u2019s an infrastructure-aware agent.<p>\u2e3b<p>Run PolyClaw (via PolyMCP CLI)<p>polymcp agent run \\\n  --type polyclaw \\\n  --query &quot;Build a sales reporting pipeline and test it end-to-end&quot; \\\n  --model minimax-m2.5:cloud \\\n  --verbose<p>What happens behind the scenes:\n 1. The task is decomposed into structured steps\n 2. Required MCP tools are identified\n 3. MCP servers are started or connected\n 4. Steps execute (sequentially or in parallel)\n 5. Outputs are validated\n 6. Failures trigger adaptive replanning\n 7. A complete, end-to-end result is returned<p>All containerized. All isolated.<p>\u2e3b<p>Why This Matters<p>Most AI agents today:\n \u2022 Call tools statically\n \u2022 Assume infrastructure already exists\n \u2022 Break on multi-step failures<p>PolyClaw instead:\n \u2022 Builds the infrastructure it needs\n \u2022 Orchestrates across multiple MCP servers\n \u2022 Handles retries and adaptive planning\n \u2022 Is safe to run in Dockerized environments<p>This makes it viable for:\n \u2022 Enterprise workflows\n \u2022 DevOps automation\n \u2022 Data pipelines\n \u2022 Internal tooling orchestration\n \u2022 Complex multi-tool reasoning tasks<p>PolyClaw turns PolyMCP from simple tool exposure into a fully autonomous orchestration layer.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP</a><p>Happy to answer questions.","title":"Show HN: PolyClaw \u2013 Autonomous Docker-First MCP Agent for PolyMCP","updated_at":"2026-02-17T13:36:48Z"},{"_highlightResult":{"author":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"hortator_<em>ai</em>"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","orchestration"],"value":"The most important lesson in IT security is: don't trust the user.<p>Not &quot;verify then trust.&quot; Not &quot;trust but monitor.&quot; Just - don't trust them. Assume every user is compromised, negligent, or adversarial. Build your systems accordingly. This principle gave us least privilege, network segmentation, rate limiting, audit logs, DLP. It works.<p>So why are we treating <em>AI</em> agents like trusted colleagues?<p>The current alignment discourse assumes we need to make agents want to behave. Instill values. Train away deception. This is the equivalent of solving security by making users trustworthy. We tried that. It doesn't work. You can't patch human nature, and you can't RLHF your way to guaranteed safety.<p>Here's the thing: every principle from zero-trust security maps directly to agent <em>orchestration</em>.<p>Least privilege. An agent that writes unit tests doesn't need prod database access. Scope its capabilities via RBAC - same as you'd scope a service account.<p>Isolation. Each agent runs in its own pod. It can't read another agent's memory, touch its files, or escalate sideways. Same reason you don't run microservices as root in a shared namespace.<p>Budget enforcement. Token caps and cost limits per agent, per task. An agent that tries to burn $10k on a $5 task gets killed. Like API rate limits, but for cognition.<p>Audit trails. Full OpenTelemetry tracing on every action, every delegation, every result. You don't need to trust an agent if you can observe everything it does.<p>PII redaction. Presidio scans agent output before it leaves the pod. Same principle as DLP in enterprise - don't let sensitive data leak, regardless of intent.<p>Policy enforcement. Declarative policies (CRDs) constrain what agents can and can't do. Like network policies, but for agent behavior.<p>We built this. It's called Hortator - a Kubernetes operator for orchestrating autonomous <em>AI</em> agent hierarchies. Agents (tribune \u2192 centurion \u2192 legionary) run in isolated pods with RBAC, budget caps, PII redaction, and full OTel tracing. Everything is a CRD: AgentTask, AgentRole, AgentPolicy. Written in Go, MIT licensed.<p>We didn't solve alignment. We made it irrelevant by treating agents as untrusted workloads - exactly how we've treated every other piece of software for the last 20 years.<p>GitHub: <a href=\"https://github.com/hortator-ai/Hortator/\" rel=\"nofollow\">https://github.com/hortator-<em>ai</em>/Hortator/</a><p>Genuinely curious what this community thinks. Are we wrong to frame alignment as an infrastructure problem? What's the zero-trust model missing when applied to agents? Poke holes - that's what we need."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Show HN: <em>AI</em> alignment is an infrastructure problem"}},"_tags":["story","author_hortator_ai","story_47047223","show_hn"],"author":"hortator_ai","created_at":"2026-02-17T13:22:47Z","created_at_i":1771334567,"num_comments":0,"objectID":"47047223","points":1,"story_id":47047223,"story_text":"The most important lesson in IT security is: don&#x27;t trust the user.<p>Not &quot;verify then trust.&quot; Not &quot;trust but monitor.&quot; Just - don&#x27;t trust them. Assume every user is compromised, negligent, or adversarial. Build your systems accordingly. This principle gave us least privilege, network segmentation, rate limiting, audit logs, DLP. It works.<p>So why are we treating AI agents like trusted colleagues?<p>The current alignment discourse assumes we need to make agents want to behave. Instill values. Train away deception. This is the equivalent of solving security by making users trustworthy. We tried that. It doesn&#x27;t work. You can&#x27;t patch human nature, and you can&#x27;t RLHF your way to guaranteed safety.<p>Here&#x27;s the thing: every principle from zero-trust security maps directly to agent orchestration.<p>Least privilege. An agent that writes unit tests doesn&#x27;t need prod database access. Scope its capabilities via RBAC - same as you&#x27;d scope a service account.<p>Isolation. Each agent runs in its own pod. It can&#x27;t read another agent&#x27;s memory, touch its files, or escalate sideways. Same reason you don&#x27;t run microservices as root in a shared namespace.<p>Budget enforcement. Token caps and cost limits per agent, per task. An agent that tries to burn $10k on a $5 task gets killed. Like API rate limits, but for cognition.<p>Audit trails. Full OpenTelemetry tracing on every action, every delegation, every result. You don&#x27;t need to trust an agent if you can observe everything it does.<p>PII redaction. Presidio scans agent output before it leaves the pod. Same principle as DLP in enterprise - don&#x27;t let sensitive data leak, regardless of intent.<p>Policy enforcement. Declarative policies (CRDs) constrain what agents can and can&#x27;t do. Like network policies, but for agent behavior.<p>We built this. It&#x27;s called Hortator - a Kubernetes operator for orchestrating autonomous AI agent hierarchies. Agents (tribune \u2192 centurion \u2192 legionary) run in isolated pods with RBAC, budget caps, PII redaction, and full OTel tracing. Everything is a CRD: AgentTask, AgentRole, AgentPolicy. Written in Go, MIT licensed.<p>We didn&#x27;t solve alignment. We made it irrelevant by treating agents as untrusted workloads - exactly how we&#x27;ve treated every other piece of software for the last 20 years.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;hortator-ai&#x2F;Hortator&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;hortator-ai&#x2F;Hortator&#x2F;</a><p>Genuinely curious what this community thinks. Are we wrong to frame alignment as an infrastructure problem? What&#x27;s the zero-trust model missing when applied to agents? Poke holes - that&#x27;s what we need.","title":"Show HN: AI alignment is an infrastructure problem","updated_at":"2026-02-17T13:26:18Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","orchestration"],"value":"I built PolyClaw, an OpenClaw-inspired autonomous agent for the PolyMCP ecosystem.<p>PolyClaw doesn\u2019t just call tools.\nIt plans, executes, adapts \u2014 and creates MCP servers when needed.<p>It\u2019s designed for real multi-step, production workflows where agents must orchestrate tools, spin up infrastructure, recover from errors, and deliver complete results end-to-end.<p>\u2e3b<p>What PolyClaw Does\n \u2022 Plans complex multi-step tasks\n \u2022 Executes and orchestrates MCP tools dynamically\n \u2022 Adapts when steps fail or context changes\n \u2022 Creates and connects MCP servers on the fly\n \u2022 Runs Docker-first for safety and isolation\n \u2022 Built with Python and TypeScript<p>PolyClaw is not just a tool caller \u2014 it\u2019s an infrastructure-aware agent.<p>\u2e3b<p>Run PolyClaw with Ollama<p>You can launch PolyClaw directly from the PolyMCP CLI:<p>polymcp agent run \\\n  --type polyclaw \\\n  --query &quot;Build a sales reporting pipeline and test it end-to-end&quot; \\\n  --model minimax-m2.5:cloud \\\n  --verbose<p>What happens behind the scenes:\n 1. The agent decomposes the task.\n 2. It determines which MCP tools are required.\n 3. It spins up or connects to MCP servers.\n 4. It executes steps in sequence (or parallel when needed).\n 5. It validates outputs.\n 6. It adapts if something fails.\n 7. It returns a complete result.<p>All containerized. All isolated.<p>\u2e3b<p>Why This Matters<p>Most <em>AI</em> agents:\n \u2022 Call tools statically\n \u2022 Assume infrastructure already exists\n \u2022 Break on multi-step failure<p>PolyClaw:\n \u2022 Builds the infrastructure it needs\n \u2022 Orchestrates across multiple MCP servers\n \u2022 Handles retries and adaptive planning\n \u2022 Is safe to run in Dockerized environments<p>This makes it viable for:\n \u2022 Enterprise workflows\n \u2022 DevOps automation\n \u2022 Data pipelines\n \u2022 Internal tooling <em>orchestration</em>\n \u2022 Complex multi-tool reasoning tasks<p>PolyClaw turns PolyMCP from simple tool exposure only with Polyagent e unifiendpolyagent or codeagent but turn into full autonomous <em>orchestration</em> agent too.<p>Repo:\n<a href=\"https://github.com/poly-mcp/PolyMCP\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP</a><p>Happy to answer questions,"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: PolyClaw \u2013 An Autonomous Docker-First MCP Agent for PolyMCP"}},"_tags":["story","author_justvugg","story_47036828","show_hn"],"author":"justvugg","created_at":"2026-02-16T16:13:08Z","created_at_i":1771258388,"num_comments":0,"objectID":"47036828","points":1,"story_id":47036828,"story_text":"I built PolyClaw, an OpenClaw-inspired autonomous agent for the PolyMCP ecosystem.<p>PolyClaw doesn\u2019t just call tools.\nIt plans, executes, adapts \u2014 and creates MCP servers when needed.<p>It\u2019s designed for real multi-step, production workflows where agents must orchestrate tools, spin up infrastructure, recover from errors, and deliver complete results end-to-end.<p>\u2e3b<p>What PolyClaw Does\n \u2022 Plans complex multi-step tasks\n \u2022 Executes and orchestrates MCP tools dynamically\n \u2022 Adapts when steps fail or context changes\n \u2022 Creates and connects MCP servers on the fly\n \u2022 Runs Docker-first for safety and isolation\n \u2022 Built with Python and TypeScript<p>PolyClaw is not just a tool caller \u2014 it\u2019s an infrastructure-aware agent.<p>\u2e3b<p>Run PolyClaw with Ollama<p>You can launch PolyClaw directly from the PolyMCP CLI:<p>polymcp agent run \\\n  --type polyclaw \\\n  --query &quot;Build a sales reporting pipeline and test it end-to-end&quot; \\\n  --model minimax-m2.5:cloud \\\n  --verbose<p>What happens behind the scenes:\n 1. The agent decomposes the task.\n 2. It determines which MCP tools are required.\n 3. It spins up or connects to MCP servers.\n 4. It executes steps in sequence (or parallel when needed).\n 5. It validates outputs.\n 6. It adapts if something fails.\n 7. It returns a complete result.<p>All containerized. All isolated.<p>\u2e3b<p>Why This Matters<p>Most AI agents:\n \u2022 Call tools statically\n \u2022 Assume infrastructure already exists\n \u2022 Break on multi-step failure<p>PolyClaw:\n \u2022 Builds the infrastructure it needs\n \u2022 Orchestrates across multiple MCP servers\n \u2022 Handles retries and adaptive planning\n \u2022 Is safe to run in Dockerized environments<p>This makes it viable for:\n \u2022 Enterprise workflows\n \u2022 DevOps automation\n \u2022 Data pipelines\n \u2022 Internal tooling orchestration\n \u2022 Complex multi-tool reasoning tasks<p>PolyClaw turns PolyMCP from simple tool exposure only with Polyagent e unifiendpolyagent or codeagent but turn into full autonomous orchestration agent too.<p>Repo:\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP</a><p>Happy to answer questions,","title":"Show HN: PolyClaw \u2013 An Autonomous Docker-First MCP Agent for PolyMCP","updated_at":"2026-02-16T16:16:31Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Romricci"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Maestro \u2013 Autonomous dev platform with multi-reviewer <em>AI</em> safety pipeline"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["orchestration"],"value":"https://www.maestro-<em>orchestrator</em>.dev"}},"_tags":["story","author_Romricci","story_47036538"],"author":"Romricci","created_at":"2026-02-16T15:51:02Z","created_at_i":1771257062,"num_comments":0,"objectID":"47036538","points":1,"story_id":47036538,"title":"Maestro \u2013 Autonomous dev platform with multi-reviewer AI safety pipeline","updated_at":"2026-02-16T15:55:30Z","url":"https://www.maestro-orchestrator.dev"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"madugula"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","orchestration"],"value":"The Agentic Shift: Peter Steinberger Joins OpenAI to Scale OpenClaw\nBy Sai Srikanth Madugula, PhD Research Scholar &amp; Product Manager | February 16, 2026<p>In a move that signals the definitive start of the &quot;Agentic Era,&quot; Peter Steinberger, the architect behind the viral open-source framework OpenClaw, has officially joined OpenAI. This transition isn't just a high-profile hire; it represents a fundamental change in how the industry views the intersection of proprietary intelligence and open-source <em>orchestration</em>.<p>As I continue my PhD research into <em>AI</em>-Blockchain models, I view this as a seminal moment. We are moving away from simple chatbots toward autonomous &quot;workers&quot; that can interact, reason, and execute. Steinberger\u2019s integration into OpenAI provides the missing bridge between world-class models and real-world execution frameworks.<p>In the Words of Sam Altman\nSam Altman, CEO of OpenAI, took to X (formerly Twitter) to welcome Steinberger and clarify the future of the framework. His statement highlights a newfound commitment to the open-source community as part of OpenAI's core product strategy:<p>&quot;Peter Steinberger is joining OpenAI to drive the next generation of personal agents. He is a genius with a lot of amazing ideas about the future of very smart agents interacting with each other to do very useful things for people... OpenClaw will live in a foundation as an open source project that OpenAI will continue to support.&quot;\nAltman\u2019s vision of a &quot;multi-agent&quot; future confirms what many of us in product management have suspected: the next billion-dollar startups won't be built on a single LLM, but on the <em>orchestration</em> of many specialized agents working in concert.<p>Why This Matters: The OpenClaw Foundation\nThe decision to house OpenClaw in an independent open-source foundation while receiving OpenAI\u2019s backing is a strategic masterstroke. It ensures that the framework remains a neutral ground for developers while benefiting from the massive compute and research resources of OpenAI. This helps solve several critical bottlenecks:<p>Interoperability: By standardizing how agents talk to each other, OpenClaw can become the &quot;HTTP of <em>AI</em>,&quot; allowing different models to collaborate seamlessly.\nReduced Friction: Developers can leverage pre-built &quot;agent personas&quot; (like the <em>AI</em> Engineer or <em>AI</em> Researcher) without reinventing the <em>orchestration</em> logic every time.\nTrust and Transparency: Keeping the foundation open-source helps demystify the &quot;black box&quot; of agentic decision-making, an area I am particularly focused on in my doctoral studies.\nA Catalyst for Solo Founders and Nano-Startups\nFor the solo founder, this news is transformative. When the creator of the most robust <em>orchestration</em> tool joins forces with the creator of the world's most capable models, the barriers to entry collapse. We are entering a phase where a single human can manage a &quot;digital corporation.&quot;<p>The implications for latency and data privacy are also significant. As OpenAI supports the foundation, we can expect more optimizations for on-device and edge-native agents\u2014a direction I recently analyzed through the lens of Karpathy\u2019s MicroGPT. Small, fast, and local agents are the future, and OpenClaw is the engine that will run them.<p>The Human in the Loop: The Conductor Role\nDoes this mean human roles are disappearing? Quite the opposite. As I\u2019ve argued in previous posts, our role is evolving into that of a Strategic Conductor. Peter Steinberger's move to OpenAI suggests that the industry is ready to provide us with a much more powerful orchestra. Our value now lies in the vision we set and the ethical guardrails we implement.<p>The workplace of tomorrow is no longer a collection of desks; it is a symphony of digital intelligence, and the baton is firmly in our hands."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Agentic Shift: Peter Steinberger Joins OpenAI"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://blog.saimadugula.com/posts/steinberger-openai-openclaw.html"}},"_tags":["story","author_madugula","story_47033865","show_hn"],"author":"madugula","created_at":"2026-02-16T11:42:16Z","created_at_i":1771242136,"num_comments":0,"objectID":"47033865","points":1,"story_id":47033865,"story_text":"The Agentic Shift: Peter Steinberger Joins OpenAI to Scale OpenClaw\nBy Sai Srikanth Madugula, PhD Research Scholar &amp; Product Manager | February 16, 2026<p>In a move that signals the definitive start of the &quot;Agentic Era,&quot; Peter Steinberger, the architect behind the viral open-source framework OpenClaw, has officially joined OpenAI. This transition isn&#x27;t just a high-profile hire; it represents a fundamental change in how the industry views the intersection of proprietary intelligence and open-source orchestration.<p>As I continue my PhD research into AI-Blockchain models, I view this as a seminal moment. We are moving away from simple chatbots toward autonomous &quot;workers&quot; that can interact, reason, and execute. Steinberger\u2019s integration into OpenAI provides the missing bridge between world-class models and real-world execution frameworks.<p>In the Words of Sam Altman\nSam Altman, CEO of OpenAI, took to X (formerly Twitter) to welcome Steinberger and clarify the future of the framework. His statement highlights a newfound commitment to the open-source community as part of OpenAI&#x27;s core product strategy:<p>&quot;Peter Steinberger is joining OpenAI to drive the next generation of personal agents. He is a genius with a lot of amazing ideas about the future of very smart agents interacting with each other to do very useful things for people... OpenClaw will live in a foundation as an open source project that OpenAI will continue to support.&quot;\nAltman\u2019s vision of a &quot;multi-agent&quot; future confirms what many of us in product management have suspected: the next billion-dollar startups won&#x27;t be built on a single LLM, but on the orchestration of many specialized agents working in concert.<p>Why This Matters: The OpenClaw Foundation\nThe decision to house OpenClaw in an independent open-source foundation while receiving OpenAI\u2019s backing is a strategic masterstroke. It ensures that the framework remains a neutral ground for developers while benefiting from the massive compute and research resources of OpenAI. This helps solve several critical bottlenecks:<p>Interoperability: By standardizing how agents talk to each other, OpenClaw can become the &quot;HTTP of AI,&quot; allowing different models to collaborate seamlessly.\nReduced Friction: Developers can leverage pre-built &quot;agent personas&quot; (like the AI Engineer or AI Researcher) without reinventing the orchestration logic every time.\nTrust and Transparency: Keeping the foundation open-source helps demystify the &quot;black box&quot; of agentic decision-making, an area I am particularly focused on in my doctoral studies.\nA Catalyst for Solo Founders and Nano-Startups\nFor the solo founder, this news is transformative. When the creator of the most robust orchestration tool joins forces with the creator of the world&#x27;s most capable models, the barriers to entry collapse. We are entering a phase where a single human can manage a &quot;digital corporation.&quot;<p>The implications for latency and data privacy are also significant. As OpenAI supports the foundation, we can expect more optimizations for on-device and edge-native agents\u2014a direction I recently analyzed through the lens of Karpathy\u2019s MicroGPT. Small, fast, and local agents are the future, and OpenClaw is the engine that will run them.<p>The Human in the Loop: The Conductor Role\nDoes this mean human roles are disappearing? Quite the opposite. As I\u2019ve argued in previous posts, our role is evolving into that of a Strategic Conductor. Peter Steinberger&#x27;s move to OpenAI suggests that the industry is ready to provide us with a much more powerful orchestra. Our value now lies in the vision we set and the ethical guardrails we implement.<p>The workplace of tomorrow is no longer a collection of desks; it is a symphony of digital intelligence, and the baton is firmly in our hands.","title":"Show HN: Agentic Shift: Peter Steinberger Joins OpenAI","updated_at":"2026-02-16T11:44:43Z","url":"https://blog.saimadugula.com/posts/steinberger-openai-openclaw.html"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"san-techie21"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","orchestration"],"value":"Hi HN,<p>I'm a security engineer with 15+ years in enterprise security. After watching OpenClaw explode to 180K stars while binding to 0.0.0.0 by default, shipping no encryption, and accumulating 512 CVEs \u2014 I decided to build what I think a personal <em>AI</em> agent should look like when security comes first.<p>Gulama is an open-source personal <em>AI</em> agent with 15+ security mechanisms built into the core:<p>- AES-256-GCM encryption for all credentials and memories (never plaintext)\n- Sandboxed execution via bubblewrap/Docker (same sandbox Anthropic uses for Claude Code)\n- Ed25519-signed skills (no unsigned code runs \u2014 unlike ClawHub's 230+ malicious skills)\n- Cedar-inspired policy engine for deterministic authorization\n- Canary tokens for prompt injection detection\n- Egress filtering + DLP to prevent data exfiltration\n- Gateway binds 127.0.0.1 ONLY by default (not 0.0.0.0)\n- Cryptographic hash-chain audit trail<p>Beyond security, it's a full-featured agent:<p>- 100+ LLM providers via LiteLLM (Anthropic, OpenAI, DeepSeek, Ollama, etc.)\n- 19 built-in skills (files, shell, web, browser, email, calendar, GitHub, Notion, Spotify, voice, MCP bridge, and more)\n- 10 communication channels (CLI, Telegram, Discord, Slack, WhatsApp, Matrix, Teams, Web UI, Voice Wake)\n- Full MCP server + client support\n- Multi-agent <em>orchestration</em> with background sub-agents\n- RAG-powered memory via ChromaDB\n- Self-modifying: the agent writes its own new skills at runtime (sandboxed)\n- 5 autonomy levels from &quot;ask before everything&quot; to full autopilot<p>Install: pip install gulama &amp;&amp; gulama setup &amp;&amp; gulama chat<p>Stack: Python 3.12+, FastAPI, LiteLLM, SQLite, ChromaDB, Click\nPyPI: <a href=\"https://pypi.org/project/gulama/\" rel=\"nofollow\">https://pypi.org/project/gulama/</a><p>Happy to answer any questions about the security architecture or design decisions."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Show HN: Gulama \u2013 Security-first open-source <em>AI</em> agent (OpenClaw alternative)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/san-techie21/gulama-bot"}},"_tags":["story","author_san-techie21","story_47031982","show_hn"],"author":"san-techie21","children":[47043547],"created_at":"2026-02-16T07:27:20Z","created_at_i":1771226840,"num_comments":1,"objectID":"47031982","points":1,"story_id":47031982,"story_text":"Hi HN,<p>I&#x27;m a security engineer with 15+ years in enterprise security. After watching OpenClaw explode to 180K stars while binding to 0.0.0.0 by default, shipping no encryption, and accumulating 512 CVEs \u2014 I decided to build what I think a personal AI agent should look like when security comes first.<p>Gulama is an open-source personal AI agent with 15+ security mechanisms built into the core:<p>- AES-256-GCM encryption for all credentials and memories (never plaintext)\n- Sandboxed execution via bubblewrap&#x2F;Docker (same sandbox Anthropic uses for Claude Code)\n- Ed25519-signed skills (no unsigned code runs \u2014 unlike ClawHub&#x27;s 230+ malicious skills)\n- Cedar-inspired policy engine for deterministic authorization\n- Canary tokens for prompt injection detection\n- Egress filtering + DLP to prevent data exfiltration\n- Gateway binds 127.0.0.1 ONLY by default (not 0.0.0.0)\n- Cryptographic hash-chain audit trail<p>Beyond security, it&#x27;s a full-featured agent:<p>- 100+ LLM providers via LiteLLM (Anthropic, OpenAI, DeepSeek, Ollama, etc.)\n- 19 built-in skills (files, shell, web, browser, email, calendar, GitHub, Notion, Spotify, voice, MCP bridge, and more)\n- 10 communication channels (CLI, Telegram, Discord, Slack, WhatsApp, Matrix, Teams, Web UI, Voice Wake)\n- Full MCP server + client support\n- Multi-agent orchestration with background sub-agents\n- RAG-powered memory via ChromaDB\n- Self-modifying: the agent writes its own new skills at runtime (sandboxed)\n- 5 autonomy levels from &quot;ask before everything&quot; to full autopilot<p>Install: pip install gulama &amp;&amp; gulama setup &amp;&amp; gulama chat<p>Stack: Python 3.12+, FastAPI, LiteLLM, SQLite, ChromaDB, Click\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;gulama&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;gulama&#x2F;</a><p>Happy to answer any questions about the security architecture or design decisions.","title":"Show HN: Gulama \u2013 Security-first open-source AI agent (OpenClaw alternative)","updated_at":"2026-02-17T04:00:01Z","url":"https://github.com/san-techie21/gulama-bot"}],"hitsPerPage":10,"nbHits":352,"nbPages":36,"page":0,"params":"query=AI+orchestration&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":8,"processingTimingsMS":{"_request":{"roundTrip":15},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":4,"scanning":2,"total":7},"total":8},"query":"AI orchestration","serverTimeMS":10}
