{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ejcho623"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["how","teams","use","claude","code"],"value":"Even within our small <em>team</em> the different AI clients (<em>Code</em>x, <em>Claude</em> <em>Code</em>/Work, Cursor, ChatGPT) used for coding, marketing, product is exploding and getting hard to track.<p>Was thinking <em>how</em> we'd keep track of the work without much overhead, and thought a lightweight cross-platform MCP logger could help.<p>You can define your own schema for the logs and it's a simple MCP and prompt you add to whatever tool you <em>use</em> for AI.<p>It's been reliable for <em>Code</em>x, ChatGPT, <em>Claude</em> and even cron-based jobs on OpenClaw. It's also nice to see all the work log aggregated across clients.<p>Please give it a try and test it with the client you're using. Should be a 2 min setup. Would be great to get any feedback if it's useful for you."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude","code"],"value":"Show HN: Agent Breadcrumbs \u2013 Unified Work Log Across <em>Claude</em>, <em>Code</em>x, OpenClaw"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/ejcho623/agent-breadcrumbs"}},"_tags":["story","author_ejcho623","story_47049841","show_hn"],"author":"ejcho623","created_at":"2026-02-17T17:07:28Z","created_at_i":1771348048,"num_comments":0,"objectID":"47049841","points":1,"story_id":47049841,"story_text":"Even within our small team the different AI clients (Codex, Claude Code&#x2F;Work, Cursor, ChatGPT) used for coding, marketing, product is exploding and getting hard to track.<p>Was thinking how we&#x27;d keep track of the work without much overhead, and thought a lightweight cross-platform MCP logger could help.<p>You can define your own schema for the logs and it&#x27;s a simple MCP and prompt you add to whatever tool you use for AI.<p>It&#x27;s been reliable for Codex, ChatGPT, Claude and even cron-based jobs on OpenClaw. It&#x27;s also nice to see all the work log aggregated across clients.<p>Please give it a try and test it with the client you&#x27;re using. Should be a 2 min setup. Would be great to get any feedback if it&#x27;s useful for you.","title":"Show HN: Agent Breadcrumbs \u2013 Unified Work Log Across Claude, Codex, OpenClaw","updated_at":"2026-02-17T17:12:06Z","url":"https://github.com/ejcho623/agent-breadcrumbs"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"VladCovaci"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["teams","use","claude"],"value":"This is the development process we <em>use</em> to build MVPs and internal tools.<p>To move fast, we combine multiple tools, AI agents, and systems. This lets us compress the product development lifecycle down to 1\u20132 days.<p>Here\u2019s the high-level flow:\nIdea \u2192 Boilerplate \u2192 AI Planning Agents \u2192 Core Features (<em>Claude</em> / Codex / Gemini) \u2192 Deployment<p>Every tool includes repeatable features such as emails, payments, and marketing pages. To avoid rebuilding these each time, we created a modular internal boilerplate which can be seen here. This modular approach allows us to change the design very easily and focus only on the core features of the product. Once the boilerplate is set up, we are ready to go. The documentation can also be found here. The boilerplate features are outlined below:<p>Boilerplate features:\nMarketing pages: Home, About, Pricing, Blog, Contact, Services, Legal Pages\nAuthentification: NextAuth &amp; Google Auth\nPayment\nEmails\nNotifications\nDashboard Structure\nFeature Gating\nSEO &amp; GEO ready\nDatabase Setup<p>AI Planning Agents<p>AI Planning Agents act as our internal agile <em>team</em>.<p>When building with AI, strong planning is essential to ensure the development agent operates within clear guardrails. These agents live directly inside our codebase, making it easy to provide full context for the features we want to build.<p>A simple flow looks like this:<p>Analyst Agent \u2192 creates the Product Brief (http://brief.md) \u2192 PM Agent \u2192 creates the PRD (http://prd.md) \u2192 Architect Agent \u2192 creates the System Architecture (http://architecture.md)\n\u2192 PM Agent \u2192 creates the Epics &amp; Stories (http://epics.md, http://stories.md)<p>Why are these so important? This process gives both us and the development AI agent a clear execution plan with strong guardrails. As a result, the agent does not hallucinate and builds exactly what is required, in the way it is required.<p>Here is an example of one story:<p>## Story 2.9: Send Email Notifications to Submitters on Status Changes\nAs a *feedback submitter*,\nI want *to receive an email when my feedback status changes (e.g., Doing \u2192 Testing \u2192 Finished)*,\nso that *I know the <em>team</em> is working on my suggestion and can see progress*.\n### Acceptance Criteria\n1. When <em>team</em> member changes feedback item status (Story 2.5 drag-and-drop), trigger email notification\n2. Email sent only if submitter provided email address during submission (FR17)\n3. Email subject: &quot;[Project Name] Update: Your feedback is now [Status]&quot;\n4. Email body includes: original feedback title, new status, <em>team</em> comment (if any), link to view on public board\n5. Email sent asynchronously (doesn't block status update)\n6. If email sending fails, log error but allow status update to succeed (NFR12)\n7. No duplicate emails if status changes multiple times quickly (debounce or queue)\n8. Unsubscribe link included (placeholder for now)\n9. Test email delivery in development and production<p>Now that we have everything in place the boilerplate with all repeatable product features (login, dashboard, payments, emails, etc.) and the planning stage completed with clear focus, guardrails, user stories, and architecture we have all the context needed to build with AI (<em>Claude</em>, Codex, or Gemini).<p>In this phase, development happens story by story. With the full planning context in place, the AI agent implements exactly what is required. Depending on the number of features, we can deploy and have a live product ready for real user validation in 1\u20132 days.<p>Here is an example of what we manage to achieve:<p>https://startupkit.today\nhttps://founderspace.work"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["how","code"],"value":"Playbook: <em>How</em> to vibe <em>code</em> a successful app"}},"_tags":["story","author_VladCovaci","story_47048167","ask_hn"],"author":"VladCovaci","children":[47048335],"created_at":"2026-02-17T14:57:29Z","created_at_i":1771340249,"num_comments":1,"objectID":"47048167","points":2,"story_id":47048167,"story_text":"This is the development process we use to build MVPs and internal tools.<p>To move fast, we combine multiple tools, AI agents, and systems. This lets us compress the product development lifecycle down to 1\u20132 days.<p>Here\u2019s the high-level flow:\nIdea \u2192 Boilerplate \u2192 AI Planning Agents \u2192 Core Features (Claude &#x2F; Codex &#x2F; Gemini) \u2192 Deployment<p>Every tool includes repeatable features such as emails, payments, and marketing pages. To avoid rebuilding these each time, we created a modular internal boilerplate which can be seen here. This modular approach allows us to change the design very easily and focus only on the core features of the product. Once the boilerplate is set up, we are ready to go. The documentation can also be found here. The boilerplate features are outlined below:<p>Boilerplate features:\nMarketing pages: Home, About, Pricing, Blog, Contact, Services, Legal Pages\nAuthentification: NextAuth &amp; Google Auth\nPayment\nEmails\nNotifications\nDashboard Structure\nFeature Gating\nSEO &amp; GEO ready\nDatabase Setup<p>AI Planning Agents<p>AI Planning Agents act as our internal agile team.<p>When building with AI, strong planning is essential to ensure the development agent operates within clear guardrails. These agents live directly inside our codebase, making it easy to provide full context for the features we want to build.<p>A simple flow looks like this:<p>Analyst Agent \u2192 creates the Product Brief (http:&#x2F;&#x2F;brief.md) \u2192 PM Agent \u2192 creates the PRD (http:&#x2F;&#x2F;prd.md) \u2192 Architect Agent \u2192 creates the System Architecture (http:&#x2F;&#x2F;architecture.md)\n\u2192 PM Agent \u2192 creates the Epics &amp; Stories (http:&#x2F;&#x2F;epics.md, http:&#x2F;&#x2F;stories.md)<p>Why are these so important? This process gives both us and the development AI agent a clear execution plan with strong guardrails. As a result, the agent does not hallucinate and builds exactly what is required, in the way it is required.<p>Here is an example of one story:<p>## Story 2.9: Send Email Notifications to Submitters on Status Changes\nAs a *feedback submitter*,\nI want *to receive an email when my feedback status changes (e.g., Doing \u2192 Testing \u2192 Finished)*,\nso that *I know the team is working on my suggestion and can see progress*.\n### Acceptance Criteria\n1. When team member changes feedback item status (Story 2.5 drag-and-drop), trigger email notification\n2. Email sent only if submitter provided email address during submission (FR17)\n3. Email subject: &quot;[Project Name] Update: Your feedback is now [Status]&quot;\n4. Email body includes: original feedback title, new status, team comment (if any), link to view on public board\n5. Email sent asynchronously (doesn&#x27;t block status update)\n6. If email sending fails, log error but allow status update to succeed (NFR12)\n7. No duplicate emails if status changes multiple times quickly (debounce or queue)\n8. Unsubscribe link included (placeholder for now)\n9. Test email delivery in development and production<p>Now that we have everything in place the boilerplate with all repeatable product features (login, dashboard, payments, emails, etc.) and the planning stage completed with clear focus, guardrails, user stories, and architecture we have all the context needed to build with AI (Claude, Codex, or Gemini).<p>In this phase, development happens story by story. With the full planning context in place, the AI agent implements exactly what is required. Depending on the number of features, we can deploy and have a live product ready for real user validation in 1\u20132 days.<p>Here is an example of what we manage to achieve:<p>https:&#x2F;&#x2F;startupkit.today\nhttps:&#x2F;&#x2F;founderspace.work","title":"Playbook: How to vibe code a successful app","updated_at":"2026-02-18T10:22:06Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Nlupus"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["how","teams","use","claude","code"],"value":"I've been building lifecycle messaging systems for SaaS companies for 10+ years. Every company needs one (welcome emails, onboarding sequences, upgrade nudges, etc) but building one from scratch is tedious and most <em>teams</em> either copy generic templates or hire a consultant.<p>I took the approach I've battle-tested across dozens of projects and wrapped it into a CLI tool that runs on <em>Claude</em> <em>Code</em>.<p>Mango Lollipop runs entirely through <em>Claude</em> <em>Code</em> locally.<p>You give it your product URL, it analyzes your business, and walks you through building a complete messaging system using the AARRR pirate metrics framework.<p>It will take about 15 minutes from init to a complete v0.1 messaging system ready to implement and test.<p>What it produces:<p>- A structured messaging matrix with triggers, guards, suppressions, and timing for every message\n- Full message copy (email, in-app, SMS, push) written in your brand voice\n- An Excel workbook for your <em>team's</em> source of truth\n- An interactive HTML dashboard with journey maps and message previews\n- Developer hand-off docs with event specs and <em>code</em> examples<p>Demo with sample outputs here <a href=\"https://sr-kai.github.io/mango-lollipop/\" rel=\"nofollow\">https://sr-kai.github.io/mango-lollipop/</a><p>The whole thing is built as markdown skills (slash commands). Each step is independent so you can review, iterate, and course-correct before moving on.<p>I built it on <em>Claude</em> <em>Code</em>, but since the skills are just markdown files, it should work in any AI coding tool that supports them, like Cursor, OpenCode, Windsurf, etc.<p>Different models produce WILDLY different copy, I recommend trying a few to see what works best for your <em>use</em>-case.<p>npm install -g mango-lollipop<p>GitHub: <a href=\"https://github.com/sr-kai/mango-lollipop\" rel=\"nofollow\">https://github.com/sr-kai/mango-lollipop</a><p>I'd love feedback on the approach and <em>how</em> you build similar system.<p>The trigger/wait/guard/suppression model is not the most elegant solution, so if you have a better implementation please share your experience."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Mango Lollipop \u2013 AI-powered lifecycle messaging generator"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/sr-kai/mango-lollipop"}},"_tags":["story","author_Nlupus","story_46997378","show_hn"],"author":"Nlupus","created_at":"2026-02-13T00:38:00Z","created_at_i":1770943080,"num_comments":0,"objectID":"46997378","points":2,"story_id":46997378,"story_text":"I&#x27;ve been building lifecycle messaging systems for SaaS companies for 10+ years. Every company needs one (welcome emails, onboarding sequences, upgrade nudges, etc) but building one from scratch is tedious and most teams either copy generic templates or hire a consultant.<p>I took the approach I&#x27;ve battle-tested across dozens of projects and wrapped it into a CLI tool that runs on Claude Code.<p>Mango Lollipop runs entirely through Claude Code locally.<p>You give it your product URL, it analyzes your business, and walks you through building a complete messaging system using the AARRR pirate metrics framework.<p>It will take about 15 minutes from init to a complete v0.1 messaging system ready to implement and test.<p>What it produces:<p>- A structured messaging matrix with triggers, guards, suppressions, and timing for every message\n- Full message copy (email, in-app, SMS, push) written in your brand voice\n- An Excel workbook for your team&#x27;s source of truth\n- An interactive HTML dashboard with journey maps and message previews\n- Developer hand-off docs with event specs and code examples<p>Demo with sample outputs here <a href=\"https:&#x2F;&#x2F;sr-kai.github.io&#x2F;mango-lollipop&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;sr-kai.github.io&#x2F;mango-lollipop&#x2F;</a><p>The whole thing is built as markdown skills (slash commands). Each step is independent so you can review, iterate, and course-correct before moving on.<p>I built it on Claude Code, but since the skills are just markdown files, it should work in any AI coding tool that supports them, like Cursor, OpenCode, Windsurf, etc.<p>Different models produce WILDLY different copy, I recommend trying a few to see what works best for your use-case.<p>npm install -g mango-lollipop<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;sr-kai&#x2F;mango-lollipop\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;sr-kai&#x2F;mango-lollipop</a><p>I&#x27;d love feedback on the approach and how you build similar system.<p>The trigger&#x2F;wait&#x2F;guard&#x2F;suppression model is not the most elegant solution, so if you have a better implementation please share your experience.","title":"Show HN: Mango Lollipop \u2013 AI-powered lifecycle messaging generator","updated_at":"2026-02-13T08:58:49Z","url":"https://github.com/sr-kai/mango-lollipop"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"boundedreason"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["how","teams","use","claude","code"],"value":"What this is:<p>Copy-paste LLM prompts that turn ChatGPT or <em>Claude</em> into a structured decision analyst for laptops, monitors, tablets, phones, and SaaS subscriptions. You define constraints, weight what matters to your workflow, and get scored recommendations with sensitivity analysis.\nWhy I built this:<p>With the rise of LLMs (AI), I wanted to find a way to harness the computing power and ease of <em>use</em> the chat interface provides.  The major problem: LLMs don\u2019t always provide repeatable, traceable results if you ask the same question twice or even against 2 competing products. That is the dilemma this product aims to solve.  Is this a PDF, yes, but it harnesses my systems analysis experience to help hard-<em>code</em> a framework for a person off the street to turn their AI chat box into an objective decision-helper in just 15 to 20 minutes of <em>use</em>.<p>I spent 10+ years applying decision science in defense and systems analysis\u2014graduate work at Naval Postgraduate School, leading <em>teams</em> through decisions where the cost of choosing wrong wasn't just money; it was mission failure or lives at risk.<p>The method used uses multi-attribute utility theory: define hard constraints (binary gates), eliminate non-viable options, score remaining candidates on mission-critical attributes with explicit weights, then run sensitivity analysis to see what changes the outcome.<p>I <em>use</em> this myself all the time.  The most recent was trying to upgrade my own laptop (Surface Pro stuck at Windows 10).<p>BLUF benefits:<p>\u2022 Helps prevent over-obsessing over specs (32GB RAM! RTX 4080!) while ignoring mission fit (do I really game that often?)<p>\u2022 Fleshes out hard constraints that sometimes come up until after purchase (bought Windows laptop, needs a way to support a MacOS app)<p>\u2022 Future-proofing: ensuring I won\u2019t pay feature I'll statistically never <em>use</em><p>\u2022 Aims to parse through the noise (SEO type posts) and get you a great first-pass research report of what you should value and why.<p>Consumer purchases don't need full enterprise rigor, but they deserve better than &quot;Top 10 Laptops 2026&quot; affiliate listicles or chatbots hallucinating specs.<p><em>How</em> the framework works:\n1. Mission definition: What must work reliably? (Video editing vs office work vs travel)<p>2. Hard constraints: Binary gates (budget ceiling, OS requirements, battery minimums)<p>3. Candidate generation: AI searches current market without SEO or affiliate bias<p>4. Weighted scoring: Performance, battery, reliability, portability\u2014you control the weights<p>5. Efficient frontier: Which options dominate? Which are just expensive?<p>6. Sensitivity analysis: &quot;If battery life matters 25% instead of 15%, MacBook Air wins. If reliability matters more, ThinkPad wins.&quot;<p>The PDFs include example case studies I\u2019ve developed: policy analyst choosing ThinkPad X1 Carbon over MacBook Air (why reliability and docking beat battery life for enterprise work), freelance designer choosing Figma over Affinity Designer (why collaboration features justified 6x higher cost), consultant choosing Obsidian over Notion (why offline-first beat ease-of-<em>use</em>).<p>No barriers: No sign-up. No account. You get a PDF with prompts and case studies. Open ChatGPT or <em>Claude</em> (free version works), paste the prompt, answer questions. That's it.<p>I built this because I was tired of seeing people (and myself) wasting money on impressive-sounding specs that don't match their actual workflow. If you've ever regretted a tech purchase 3 weeks later, this might help.<p>Try it (I'd offer it free but then I loose my IP):\n\u2022 Tech &amp; Electronics: <a href=\"https://decisioncontrolworks.gumroad.com/l/auzhsa\" rel=\"nofollow\">https://decisioncontrolworks.gumroad.com/l/auzhsa</a>\n\u2022 Software &amp; Subscriptions: <a href=\"https://decisioncontrolworks.gumroad.com/l/zaucxt\" rel=\"nofollow\">https://decisioncontrolworks.gumroad.com/l/zaucxt</a><p>Curious what HN thinks\u2014especially if anyone's tried applying formal decision methods to everyday purchases."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Multi-attribute decision frameworks for tech purchases"}},"_tags":["story","author_boundedreason","story_46955606","show_hn"],"author":"boundedreason","children":[47015070,46975995],"created_at":"2026-02-10T05:01:51Z","created_at_i":1770699711,"num_comments":4,"objectID":"46955606","points":1,"story_id":46955606,"story_text":"What this is:<p>Copy-paste LLM prompts that turn ChatGPT or Claude into a structured decision analyst for laptops, monitors, tablets, phones, and SaaS subscriptions. You define constraints, weight what matters to your workflow, and get scored recommendations with sensitivity analysis.\nWhy I built this:<p>With the rise of LLMs (AI), I wanted to find a way to harness the computing power and ease of use the chat interface provides.  The major problem: LLMs don\u2019t always provide repeatable, traceable results if you ask the same question twice or even against 2 competing products. That is the dilemma this product aims to solve.  Is this a PDF, yes, but it harnesses my systems analysis experience to help hard-code a framework for a person off the street to turn their AI chat box into an objective decision-helper in just 15 to 20 minutes of use.<p>I spent 10+ years applying decision science in defense and systems analysis\u2014graduate work at Naval Postgraduate School, leading teams through decisions where the cost of choosing wrong wasn&#x27;t just money; it was mission failure or lives at risk.<p>The method used uses multi-attribute utility theory: define hard constraints (binary gates), eliminate non-viable options, score remaining candidates on mission-critical attributes with explicit weights, then run sensitivity analysis to see what changes the outcome.<p>I use this myself all the time.  The most recent was trying to upgrade my own laptop (Surface Pro stuck at Windows 10).<p>BLUF benefits:<p>\u2022 Helps prevent over-obsessing over specs (32GB RAM! RTX 4080!) while ignoring mission fit (do I really game that often?)<p>\u2022 Fleshes out hard constraints that sometimes come up until after purchase (bought Windows laptop, needs a way to support a MacOS app)<p>\u2022 Future-proofing: ensuring I won\u2019t pay feature I&#x27;ll statistically never use<p>\u2022 Aims to parse through the noise (SEO type posts) and get you a great first-pass research report of what you should value and why.<p>Consumer purchases don&#x27;t need full enterprise rigor, but they deserve better than &quot;Top 10 Laptops 2026&quot; affiliate listicles or chatbots hallucinating specs.<p>How the framework works:\n1. Mission definition: What must work reliably? (Video editing vs office work vs travel)<p>2. Hard constraints: Binary gates (budget ceiling, OS requirements, battery minimums)<p>3. Candidate generation: AI searches current market without SEO or affiliate bias<p>4. Weighted scoring: Performance, battery, reliability, portability\u2014you control the weights<p>5. Efficient frontier: Which options dominate? Which are just expensive?<p>6. Sensitivity analysis: &quot;If battery life matters 25% instead of 15%, MacBook Air wins. If reliability matters more, ThinkPad wins.&quot;<p>The PDFs include example case studies I\u2019ve developed: policy analyst choosing ThinkPad X1 Carbon over MacBook Air (why reliability and docking beat battery life for enterprise work), freelance designer choosing Figma over Affinity Designer (why collaboration features justified 6x higher cost), consultant choosing Obsidian over Notion (why offline-first beat ease-of-use).<p>No barriers: No sign-up. No account. You get a PDF with prompts and case studies. Open ChatGPT or Claude (free version works), paste the prompt, answer questions. That&#x27;s it.<p>I built this because I was tired of seeing people (and myself) wasting money on impressive-sounding specs that don&#x27;t match their actual workflow. If you&#x27;ve ever regretted a tech purchase 3 weeks later, this might help.<p>Try it (I&#x27;d offer it free but then I loose my IP):\n\u2022 Tech &amp; Electronics: <a href=\"https:&#x2F;&#x2F;decisioncontrolworks.gumroad.com&#x2F;l&#x2F;auzhsa\" rel=\"nofollow\">https:&#x2F;&#x2F;decisioncontrolworks.gumroad.com&#x2F;l&#x2F;auzhsa</a>\n\u2022 Software &amp; Subscriptions: <a href=\"https:&#x2F;&#x2F;decisioncontrolworks.gumroad.com&#x2F;l&#x2F;zaucxt\" rel=\"nofollow\">https:&#x2F;&#x2F;decisioncontrolworks.gumroad.com&#x2F;l&#x2F;zaucxt</a><p>Curious what HN thinks\u2014especially if anyone&#x27;s tried applying formal decision methods to everyday purchases.","title":"Show HN: Multi-attribute decision frameworks for tech purchases","updated_at":"2026-02-16T20:35:00Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"bsgeraci"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["how","teams","use","claude","code"],"value":"I'm a software engineer who keeps getting pulled into DevOps no matter <em>how</em> hard I try to escape it. I recently moved into a Lead DevOps Engineer role writing tooling to automate a lot of the pain away. On my own time outside of work, I built Artifact Keeper \u2014 a self-hosted artifact registry that supports 45+ package formats. Security scanning, SSO, replication, WASM plugins \u2014 it's all in the MIT-licensed release. No enterprise tier. No feature gates. No surprise invoices.<p>Your package managers \u2014 pip, npm, docker, cargo, helm, go, all of them \u2014 talk directly to it using their native protocols. Security scanning with Trivy, Grype, and OpenSCAP is built in, with a policy engine that can quarantine bad artifacts before they hit your builds. And if you need a format it doesn't support yet, there's a WASM plugin system so you can add your own without forking the backend.<p>Why I built it:<p>Part of what pulled me into computers in the first place was open source. I grew up poor in New Orleans, and the only hardware I had access to in the early 2000s were some Compaq Pentium IIs my dad brought home after his work was tossing them out. I put Linux on them, and it ran circles around Windows 2000 and Millennium on that low-end hardware. That experience taught me that the best software is software that's open for everyone to see, <em>use</em>, and that actually runs well on whatever you've got.<p>Fast forward to today, and I see the same pattern everywhere: GitLab, JFrog, Harbor, and others ship a limited &quot;community&quot; edition and then hide the features <em>teams</em> actually need behind some paywall. I get it \u2014 paychecks have to come from somewhere. But I wanted to prove that a fully-featured artifact registry could exist as genuinely open-source software. Every feature. No exceptions.<p>The specific features came from real pain points. Artifactory's search is painfully slow \u2014 that's why I integrated Meilisearch. Security scanning that doesn't require a separate enterprise license was another big one. And I wanted replication that didn't need a central coordinator \u2014 so I built a peer mesh where any node can replicate to any other node. I haven't deployed this at work yet \u2014 right now I'm running it at home for my personal projects \u2014 but I'd love to see it tested at scale, and that's a big part of why I'm sharing it here.<p>The AI story (I'm going to be honest about this):<p>I built this in about three weeks using <em>Claude</em> <em>Code</em>. I know a lot of you will say this is probably vibe coding garbage \u2014 but if that's the case, it's an impressive pile of vibe coding garbage. Go look at the codebase. The backend is ~80% Rust with 429 unit tests, 33 PostgreSQL migrations, a layered architecture, and a full CI/CD pipeline with E2E tests, stress testing, and failure injection.<p>AI didn't make the design decisions for me. I still had to design the WASM plugin system, figure out <em>how</em> the scanning engines complement each other, and architect the mesh replication. Years of domain knowledge drove the design \u2014 AI just let me build it way faster. I'm floored at what these tools make possible for a tinkerer and security nerd like me.<p>Tech stack: Rust on Axum, PostgreSQL 16, Meilisearch, Trivy + Grype + OpenSCAP, Wasmtime WASM plugins (hot-reloadable), mesh replication with chunked transfers. Frontend is Next.js 15 plus native Swift (iOS/macOS) and Kotlin (Android) apps. OpenAPI 3.1 spec with auto-generated TypeScript and Rust SDKs.<p>Try it:<p><pre><code>  git clone https://github.com/artifact-keeper/artifact-keeper.git\n  cd artifact-keeper\n  docker compose up -d\n</code></pre>\nThen visit http://localhost:30080<p>Live demo: <a href=\"https://demo.artifactkeeper.com\" rel=\"nofollow\">https://demo.artifactkeeper.com</a>\nDocs: <a href=\"https://artifactkeeper.com/docs/\" rel=\"nofollow\">https://artifactkeeper.com/docs/</a><p>I'd love any feedback \u2014 what you think of the approach, what you'd want to see, what you hate about Artifactory or Nexus that you wish someone would just fix. It doesn't have to be a PR. Open an issue, start a discussion, or just tell me here.<p><a href=\"https://github.com/artifact-keeper\" rel=\"nofollow\">https://github.com/artifact-keeper</a>"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Artifact Keeper \u2013 Open-Source Artifactory/Nexus Alternative in Rust"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/artifact-keeper"}},"_tags":["story","author_bsgeraci","story_46909037","show_hn"],"author":"bsgeraci","children":[46910237,46909898,46910129,46910503,46915730,46912233,46917265,46911929,46910948,46917499,46945846,46920860,46911892,46916733,46911160,46923600,46912699,46912141,46910688,46914046,46913461,46911533,46917136,46909867,46914037,46910433],"created_at":"2026-02-06T04:12:59Z","created_at_i":1770351179,"num_comments":68,"objectID":"46909037","points":166,"story_id":46909037,"story_text":"I&#x27;m a software engineer who keeps getting pulled into DevOps no matter how hard I try to escape it. I recently moved into a Lead DevOps Engineer role writing tooling to automate a lot of the pain away. On my own time outside of work, I built Artifact Keeper \u2014 a self-hosted artifact registry that supports 45+ package formats. Security scanning, SSO, replication, WASM plugins \u2014 it&#x27;s all in the MIT-licensed release. No enterprise tier. No feature gates. No surprise invoices.<p>Your package managers \u2014 pip, npm, docker, cargo, helm, go, all of them \u2014 talk directly to it using their native protocols. Security scanning with Trivy, Grype, and OpenSCAP is built in, with a policy engine that can quarantine bad artifacts before they hit your builds. And if you need a format it doesn&#x27;t support yet, there&#x27;s a WASM plugin system so you can add your own without forking the backend.<p>Why I built it:<p>Part of what pulled me into computers in the first place was open source. I grew up poor in New Orleans, and the only hardware I had access to in the early 2000s were some Compaq Pentium IIs my dad brought home after his work was tossing them out. I put Linux on them, and it ran circles around Windows 2000 and Millennium on that low-end hardware. That experience taught me that the best software is software that&#x27;s open for everyone to see, use, and that actually runs well on whatever you&#x27;ve got.<p>Fast forward to today, and I see the same pattern everywhere: GitLab, JFrog, Harbor, and others ship a limited &quot;community&quot; edition and then hide the features teams actually need behind some paywall. I get it \u2014 paychecks have to come from somewhere. But I wanted to prove that a fully-featured artifact registry could exist as genuinely open-source software. Every feature. No exceptions.<p>The specific features came from real pain points. Artifactory&#x27;s search is painfully slow \u2014 that&#x27;s why I integrated Meilisearch. Security scanning that doesn&#x27;t require a separate enterprise license was another big one. And I wanted replication that didn&#x27;t need a central coordinator \u2014 so I built a peer mesh where any node can replicate to any other node. I haven&#x27;t deployed this at work yet \u2014 right now I&#x27;m running it at home for my personal projects \u2014 but I&#x27;d love to see it tested at scale, and that&#x27;s a big part of why I&#x27;m sharing it here.<p>The AI story (I&#x27;m going to be honest about this):<p>I built this in about three weeks using Claude Code. I know a lot of you will say this is probably vibe coding garbage \u2014 but if that&#x27;s the case, it&#x27;s an impressive pile of vibe coding garbage. Go look at the codebase. The backend is ~80% Rust with 429 unit tests, 33 PostgreSQL migrations, a layered architecture, and a full CI&#x2F;CD pipeline with E2E tests, stress testing, and failure injection.<p>AI didn&#x27;t make the design decisions for me. I still had to design the WASM plugin system, figure out how the scanning engines complement each other, and architect the mesh replication. Years of domain knowledge drove the design \u2014 AI just let me build it way faster. I&#x27;m floored at what these tools make possible for a tinkerer and security nerd like me.<p>Tech stack: Rust on Axum, PostgreSQL 16, Meilisearch, Trivy + Grype + OpenSCAP, Wasmtime WASM plugins (hot-reloadable), mesh replication with chunked transfers. Frontend is Next.js 15 plus native Swift (iOS&#x2F;macOS) and Kotlin (Android) apps. OpenAPI 3.1 spec with auto-generated TypeScript and Rust SDKs.<p>Try it:<p><pre><code>  git clone https:&#x2F;&#x2F;github.com&#x2F;artifact-keeper&#x2F;artifact-keeper.git\n  cd artifact-keeper\n  docker compose up -d\n</code></pre>\nThen visit http:&#x2F;&#x2F;localhost:30080<p>Live demo: <a href=\"https:&#x2F;&#x2F;demo.artifactkeeper.com\" rel=\"nofollow\">https:&#x2F;&#x2F;demo.artifactkeeper.com</a>\nDocs: <a href=\"https:&#x2F;&#x2F;artifactkeeper.com&#x2F;docs&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;artifactkeeper.com&#x2F;docs&#x2F;</a><p>I&#x27;d love any feedback \u2014 what you think of the approach, what you&#x27;d want to see, what you hate about Artifactory or Nexus that you wish someone would just fix. It doesn&#x27;t have to be a PR. Open an issue, start a discussion, or just tell me here.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;artifact-keeper\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;artifact-keeper</a>","title":"Show HN: Artifact Keeper \u2013 Open-Source Artifactory/Nexus Alternative in Rust","updated_at":"2026-02-16T22:52:46Z","url":"https://github.com/artifact-keeper"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"aray07"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["how","teams","use","claude","code"],"value":"Boris Cherny (<em>Claude</em> <em>Code</em> creator) recently dropped a threads on <em>how</em> his <em>team</em> at Anthropic <em>uses</em> <em>Claude</em> <em>Code</em>.<p>The key insight: they don't treat it as a static config. After every correction, they tell <em>Claude</em> &quot;Update your <em>CLAUDE</em>.md so you don't make that mistake again.&quot; <em>Claude</em> writes a rule for itself. They review it, commit it to git. The mistake never happens again.<p>I cross-referenced his tweets with Anthropic's official docs and other best practices for <em>CLAUDE</em>.md and then packaged it into a starter kit:<p><pre><code>  - Fill-in-the-blank templates for Next.js/TypeScript, Python/FastAPI, and a generic\n  catch-all\n  - The workflow patterns his <em>team</em> actually <em>uses</em> (plan mode, verification loops, subagent\n  strategy)\n  - Every claim cited back to the source tweet or doc\n</code></pre>\nRepo: <a href=\"https://github.com/abhishekray07/claude-md-templates\" rel=\"nofollow\">https://github.com/abhishekray07/<em>claude</em>-md-templates</a><p>What's in your <em>CLAUDE</em>.md that's made a measurable difference?"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude"],"value":"Show HN: <em>Claude</em>.md templates based on Boris Cherny's advice"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude"],"value":"https://github.com/abhishekray07/<em>claude</em>-md-templates"}},"_tags":["story","author_aray07","story_46900083","show_hn"],"author":"aray07","created_at":"2026-02-05T14:35:41Z","created_at_i":1770302141,"num_comments":0,"objectID":"46900083","points":10,"story_id":46900083,"story_text":"Boris Cherny (Claude Code creator) recently dropped a threads on how his team at Anthropic uses Claude Code.<p>The key insight: they don&#x27;t treat it as a static config. After every correction, they tell Claude &quot;Update your CLAUDE.md so you don&#x27;t make that mistake again.&quot; Claude writes a rule for itself. They review it, commit it to git. The mistake never happens again.<p>I cross-referenced his tweets with Anthropic&#x27;s official docs and other best practices for CLAUDE.md and then packaged it into a starter kit:<p><pre><code>  - Fill-in-the-blank templates for Next.js&#x2F;TypeScript, Python&#x2F;FastAPI, and a generic\n  catch-all\n  - The workflow patterns his team actually uses (plan mode, verification loops, subagent\n  strategy)\n  - Every claim cited back to the source tweet or doc\n</code></pre>\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;abhishekray07&#x2F;claude-md-templates\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;abhishekray07&#x2F;claude-md-templates</a><p>What&#x27;s in your CLAUDE.md that&#x27;s made a measurable difference?","title":"Show HN: Claude.md templates based on Boris Cherny's advice","updated_at":"2026-02-12T21:30:32Z","url":"https://github.com/abhishekray07/claude-md-templates"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"manumasson"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["how","teams","use","code"],"value":"Hello HN! I have been building this for the past year and I'm excited to share it here because I think it's the sort of thing this community will be very interested in.<p>I see the future of work as orchestrating <em>teams</em> of AI agents. But currently even managing a handful of agents is chaos. The IDE needs to evolve to be agent native, designed from the ground up for agentic engineering and context rather than another VS <em>code</em> fork. I'm building Voicetree towards that vision.<p>A rather insane vision of engineering where you architect projects from hundreds of agent sessions whose outputs are all saved, connected together, and turned into a Markdown mindmap. Then you spatially navigate the graph, hand-holding agents as they recursively fork themselves.<p>This tool is now where the majority of my <em>code</em> gets written, and I'm looking for feedback on <em>how</em> this can become something you would want to <em>use</em> as well"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude","code"],"value":"Show HN: Obsidian meets <em>Claude</em> <em>Code</em>. A Markdown graph for agents and context"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/voicetreelab/voicetree"}},"_tags":["story","author_manumasson","story_46878200","show_hn"],"author":"manumasson","created_at":"2026-02-03T22:23:11Z","created_at_i":1770157391,"num_comments":0,"objectID":"46878200","points":3,"story_id":46878200,"story_text":"Hello HN! I have been building this for the past year and I&#x27;m excited to share it here because I think it&#x27;s the sort of thing this community will be very interested in.<p>I see the future of work as orchestrating teams of AI agents. But currently even managing a handful of agents is chaos. The IDE needs to evolve to be agent native, designed from the ground up for agentic engineering and context rather than another VS code fork. I&#x27;m building Voicetree towards that vision.<p>A rather insane vision of engineering where you architect projects from hundreds of agent sessions whose outputs are all saved, connected together, and turned into a Markdown mindmap. Then you spatially navigate the graph, hand-holding agents as they recursively fork themselves.<p>This tool is now where the majority of my code gets written, and I&#x27;m looking for feedback on how this can become something you would want to use as well","title":"Show HN: Obsidian meets Claude Code. A Markdown graph for agents and context","updated_at":"2026-02-04T15:51:12Z","url":"https://github.com/voicetreelab/voicetree"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"hutchplusplus"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["how","teams","use","claude","code"],"value":"I've been programming since 1997. I've worked in the trenches at international e-commerce companies, lead dev <em>teams</em> at startups, everything in between. I now run a web dev company (not linked cause I'm not tryna drum up new business with &quot;content&quot;). I give webinars. I speak at events. I've been around.<p>So it's from a place of wisdom when I say it's been impossible for me to ignore <em>how</em> absolutely world-destroying AI/LLM technology is on so very many levels. Not just for <em>code</em>rs. The whole of it. The environmental impact, the global job market, education; it's truly catastrophic, our industry is embracing it, and don't understand why. We of all people should know better.<p>That's not even taking into account their efficacy, or lack thereof. I do my best to keep up. I've listened with an open mind to what advocates have to say. I've held my nose and experimented with them in a variety of ways; vibe coding, small fns, proposals, research, etc. I've tried to engineer better prompts with rubrics and xml-based formatting. I remain unconvinced any of this is worth the global cost.<p>As a coding tool, it's often flat out wrong and produces unusable or destructive results. Search &quot;cursor delete files&quot; and read about vibe <em>code</em>rs who have had swaths of <em>code</em> deleted because <em>claude</em> got silly and these dummies don't know about version control. Look up the AI 12x problem; <em>how</em> coding time has dropped significantly, but <em>code</em> review and maintenance time has skyrocketed: https://webmatrices.com/post/vibe-coding-has-a-12x-cost-problem-maintainers-are-done<p>It gets worse the further you zoom out.<p><em>How</em> many hallucinations and &quot;<em>use</em> krazy glue to stick cheese to your pizza&quot;s until people realize LLMs _suck at knowing things_ and they're not improving. We've reached the theoretical limits of the tech. Beyond small tasks like grammar and natural-language search, they get brittle af.<p>So why is everyone hyping AI? Why is everyone gung-ho to put half of the world's knowledge-workers out of a job and push our power grid to its absolute limits for something that has the success rate of a freshman intern?<p>--<p>Economically it's no better.<p>The ghouls who run these AI companies and their infestors are betting on AGI. But as the engineers in the field know, AGI is not possible with the LLM modality. They hope building massive data centers to hold historic amounts of processing power and training data will somehow crack it. But &quot;morer and betterer&quot; is a hail mary \u2014 even without new limitations re: copyrighted material.<p>Instead, they'll continue accruing massive circular debts. Nvidia investing in (loaning money to) openAI so openAI can buy more nvidia chips so nvidia and invest more until it spins out of control. AI companies are losing _billions of dollars_ every year with no path to solvency.<p>It's no wonder Apple peaced the fuck out of their LLM efforts and just offloaded to Google. There's no win scenario in dumping billions into a plagiarism-and-lying machine.<p>--<p>In the end, I just don't understand <em>how</em> people can continue to advocate for these things. With all we know about <em>how</em> these work and what it all costs and will cost society on a global scale, you'd have to be deluded, ignorant, or callous to advocate or even casually <em>use</em> this shit. I understand some folks feel like they _have_ to learn these things to ensure they still have a role in our industry. But none of this is sustainable.<p>I don't get it."},"title":{"matchLevel":"none","matchedWords":[],"value":"Thoughts on AI/LLM usage from a 25 year industry vet"}},"_tags":["story","author_hutchplusplus","story_46830235","ask_hn"],"author":"hutchplusplus","created_at":"2026-01-30T21:34:45Z","created_at_i":1769808885,"num_comments":0,"objectID":"46830235","points":5,"story_id":46830235,"story_text":"I&#x27;ve been programming since 1997. I&#x27;ve worked in the trenches at international e-commerce companies, lead dev teams at startups, everything in between. I now run a web dev company (not linked cause I&#x27;m not tryna drum up new business with &quot;content&quot;). I give webinars. I speak at events. I&#x27;ve been around.<p>So it&#x27;s from a place of wisdom when I say it&#x27;s been impossible for me to ignore how absolutely world-destroying AI&#x2F;LLM technology is on so very many levels. Not just for coders. The whole of it. The environmental impact, the global job market, education; it&#x27;s truly catastrophic, our industry is embracing it, and don&#x27;t understand why. We of all people should know better.<p>That&#x27;s not even taking into account their efficacy, or lack thereof. I do my best to keep up. I&#x27;ve listened with an open mind to what advocates have to say. I&#x27;ve held my nose and experimented with them in a variety of ways; vibe coding, small fns, proposals, research, etc. I&#x27;ve tried to engineer better prompts with rubrics and xml-based formatting. I remain unconvinced any of this is worth the global cost.<p>As a coding tool, it&#x27;s often flat out wrong and produces unusable or destructive results. Search &quot;cursor delete files&quot; and read about vibe coders who have had swaths of code deleted because claude got silly and these dummies don&#x27;t know about version control. Look up the AI 12x problem; how coding time has dropped significantly, but code review and maintenance time has skyrocketed: https:&#x2F;&#x2F;webmatrices.com&#x2F;post&#x2F;vibe-coding-has-a-12x-cost-problem-maintainers-are-done<p>It gets worse the further you zoom out.<p>How many hallucinations and &quot;use krazy glue to stick cheese to your pizza&quot;s until people realize LLMs _suck at knowing things_ and they&#x27;re not improving. We&#x27;ve reached the theoretical limits of the tech. Beyond small tasks like grammar and natural-language search, they get brittle af.<p>So why is everyone hyping AI? Why is everyone gung-ho to put half of the world&#x27;s knowledge-workers out of a job and push our power grid to its absolute limits for something that has the success rate of a freshman intern?<p>--<p>Economically it&#x27;s no better.<p>The ghouls who run these AI companies and their infestors are betting on AGI. But as the engineers in the field know, AGI is not possible with the LLM modality. They hope building massive data centers to hold historic amounts of processing power and training data will somehow crack it. But &quot;morer and betterer&quot; is a hail mary \u2014 even without new limitations re: copyrighted material.<p>Instead, they&#x27;ll continue accruing massive circular debts. Nvidia investing in (loaning money to) openAI so openAI can buy more nvidia chips so nvidia and invest more until it spins out of control. AI companies are losing _billions of dollars_ every year with no path to solvency.<p>It&#x27;s no wonder Apple peaced the fuck out of their LLM efforts and just offloaded to Google. There&#x27;s no win scenario in dumping billions into a plagiarism-and-lying machine.<p>--<p>In the end, I just don&#x27;t understand how people can continue to advocate for these things. With all we know about how these work and what it all costs and will cost society on a global scale, you&#x27;d have to be deluded, ignorant, or callous to advocate or even casually use this shit. I understand some folks feel like they _have_ to learn these things to ensure they still have a role in our industry. But none of this is sustainable.<p>I don&#x27;t get it.","title":"Thoughts on AI/LLM usage from a 25 year industry vet","updated_at":"2026-01-31T19:27:28Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Evanson"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["how","teams","use","claude","code"],"value":"Hey HN! I built Lumina \u2013 an open-source observability platform for AI/LLM applications. Self-host it in 5 minutes with Docker Compose, all features included.<p>The Problem:<p>I've been building LLM apps for the past year, and I kept running into the same issues:\n- LLM responses would randomly change after prompt tweaks, breaking things.\n- Costs would spike unexpectedly (turns out a bug was hitting GPT-4 instead of 3.5).\n- No easy way to compare &quot;before vs after&quot; when testing prompt changes.\n- Existing tools were either too expensive or missing features in free tiers.<p>What I Built:<p>Lumina is OpenTelemetry-native, meaning:\n- Works with your existing OTEL stack (Datadog, Grafana, etc.).\n- No vendor lock-in, standard trace format.\n- Integrates in 3 lines of <em>code</em>.<p>Key features:\n - Cost &amp; quality monitoring \u2013 Automatic alerts when costs spike, or responses degrade.\n - Replay testing \u2013 Capture production traces, replay them after changes, see diffs.\n - Semantic comparison \u2013 Not just string matching \u2013 <em>uses</em> <em>Claude</em> to judge if responses are &quot;better&quot; or &quot;worse.&quot;\n - Self-hosted tier \u2013 50k traces/day, 7-day retention, ALL features included (alerts, replay, semantic scoring)<p><em>How</em> it works:<p>```bash\n# Start Lumina\ngit clone <a href=\"https://github.com/use-lumina/Lumina\" rel=\"nofollow\">https://github.com/<em>use</em>-lumina/Lumina</a>\ncd Lumina/infra/docker\ndocker-compose up -d\n```<p>```typescript\n// Add to your app (no API key needed for self-hosted!)\nimport { Lumina } from '@uselumina/sdk';<p>const lumina = new Lumina({\n  endpoint: 'http://localhost:8080/v1/traces',\n});<p>// Wrap your LLM call\nconst response = await lumina.traceLLM(\n  async () =&gt; await openai.chat.completions.create({...}),\n  { provider: 'openai', model: 'gpt-4', prompt: '...' }\n);\n```<p>That's it. Every LLM call is now tracked with cost, latency, tokens, and quality scores.<p>What makes it different:<p>1. Free self-hosted with limits that work \u2013 50k traces/day and 7-day retention (resets daily at midnight UTC). All features included: alerts, replay testing, and semantic scoring. Perfect for most development and small production workloads. Need more? Upgrade to managed cloud.<p>2. OpenTelemetry-native \u2013 Not another proprietary format. <em>Use</em> standard OTEL exporters, works with existing infra. Can send traces to both Lumina AND Datadog simultaneously.<p>3. Replay testing \u2013 The killer feature. Capture 100 production traces, change your prompt, replay them all, and get a semantic diff report. Like snapshot testing for LLMs.<p>4. Fast \u2013 Built with Bun, Postgres, Redis, NATS. Sub-500ms from trace to alert. Handles 10k+ traces/min on a single machine.<p>What I'm looking for:<p>- Feedback on the approach (is OTEL the right foundation?)\n- Bug reports (tested on Mac/Linux/WSL2, but I'm sure there are issues)\n- Ideas for what features matter most (alerts? replay? cost tracking?)\n- Help with the semantic scorer (currently <em>uses</em> <em>Claude</em>, want to make it pluggable)<p>Why open source:<p>I want this to be the standard for LLM observability. That only works if it's:\n- Free to <em>use</em> and modify (Apache 2.0)\n- Easy to self-host (Docker Compose, no cloud dependencies)\n- Open to contributions (good first issues tagged)<p>The business model is managed hosting for <em>teams</em> that don't want to run infrastructure. But the core product is and always will be free.<p>Try it:\n- GitHub: <a href=\"https://github.com/use-lumina/Lumina\" rel=\"nofollow\">https://github.com/<em>use</em>-lumina/Lumina</a>\n- Docs: <a href=\"https://docs.uselumina.io\" rel=\"nofollow\">https://docs.uselumina.io</a>\n- Quick start: 5 minutes from `git clone` to dashboard<p>I'd love to hear what you think! Especially interested in:\n- What observability problems are you hitting with LLMs\n- Missing features that would make this useful for you\n- Any similar tools you're using (and what they do better)<p>Thanks for reading!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Lumina \u2013 Open-source observability for AI systems(OpenTelemetry-native)"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["use"],"value":"https://github.com/<em>use</em>-lumina/Lumina"}},"_tags":["story","author_Evanson","story_46781731","show_hn"],"author":"Evanson","created_at":"2026-01-27T16:00:06Z","created_at_i":1769529606,"num_comments":0,"objectID":"46781731","points":1,"story_id":46781731,"story_text":"Hey HN! I built Lumina \u2013 an open-source observability platform for AI&#x2F;LLM applications. Self-host it in 5 minutes with Docker Compose, all features included.<p>The Problem:<p>I&#x27;ve been building LLM apps for the past year, and I kept running into the same issues:\n- LLM responses would randomly change after prompt tweaks, breaking things.\n- Costs would spike unexpectedly (turns out a bug was hitting GPT-4 instead of 3.5).\n- No easy way to compare &quot;before vs after&quot; when testing prompt changes.\n- Existing tools were either too expensive or missing features in free tiers.<p>What I Built:<p>Lumina is OpenTelemetry-native, meaning:\n- Works with your existing OTEL stack (Datadog, Grafana, etc.).\n- No vendor lock-in, standard trace format.\n- Integrates in 3 lines of code.<p>Key features:\n - Cost &amp; quality monitoring \u2013 Automatic alerts when costs spike, or responses degrade.\n - Replay testing \u2013 Capture production traces, replay them after changes, see diffs.\n - Semantic comparison \u2013 Not just string matching \u2013 uses Claude to judge if responses are &quot;better&quot; or &quot;worse.&quot;\n - Self-hosted tier \u2013 50k traces&#x2F;day, 7-day retention, ALL features included (alerts, replay, semantic scoring)<p>How it works:<p>```bash\n# Start Lumina\ngit clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\ncd Lumina&#x2F;infra&#x2F;docker\ndocker-compose up -d\n```<p>```typescript\n&#x2F;&#x2F; Add to your app (no API key needed for self-hosted!)\nimport { Lumina } from &#x27;@uselumina&#x2F;sdk&#x27;;<p>const lumina = new Lumina({\n  endpoint: &#x27;http:&#x2F;&#x2F;localhost:8080&#x2F;v1&#x2F;traces&#x27;,\n});<p>&#x2F;&#x2F; Wrap your LLM call\nconst response = await lumina.traceLLM(\n  async () =&gt; await openai.chat.completions.create({...}),\n  { provider: &#x27;openai&#x27;, model: &#x27;gpt-4&#x27;, prompt: &#x27;...&#x27; }\n);\n```<p>That&#x27;s it. Every LLM call is now tracked with cost, latency, tokens, and quality scores.<p>What makes it different:<p>1. Free self-hosted with limits that work \u2013 50k traces&#x2F;day and 7-day retention (resets daily at midnight UTC). All features included: alerts, replay testing, and semantic scoring. Perfect for most development and small production workloads. Need more? Upgrade to managed cloud.<p>2. OpenTelemetry-native \u2013 Not another proprietary format. Use standard OTEL exporters, works with existing infra. Can send traces to both Lumina AND Datadog simultaneously.<p>3. Replay testing \u2013 The killer feature. Capture 100 production traces, change your prompt, replay them all, and get a semantic diff report. Like snapshot testing for LLMs.<p>4. Fast \u2013 Built with Bun, Postgres, Redis, NATS. Sub-500ms from trace to alert. Handles 10k+ traces&#x2F;min on a single machine.<p>What I&#x27;m looking for:<p>- Feedback on the approach (is OTEL the right foundation?)\n- Bug reports (tested on Mac&#x2F;Linux&#x2F;WSL2, but I&#x27;m sure there are issues)\n- Ideas for what features matter most (alerts? replay? cost tracking?)\n- Help with the semantic scorer (currently uses Claude, want to make it pluggable)<p>Why open source:<p>I want this to be the standard for LLM observability. That only works if it&#x27;s:\n- Free to use and modify (Apache 2.0)\n- Easy to self-host (Docker Compose, no cloud dependencies)\n- Open to contributions (good first issues tagged)<p>The business model is managed hosting for teams that don&#x27;t want to run infrastructure. But the core product is and always will be free.<p>Try it:\n- GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\n- Docs: <a href=\"https:&#x2F;&#x2F;docs.uselumina.io\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.uselumina.io</a>\n- Quick start: 5 minutes from `git clone` to dashboard<p>I&#x27;d love to hear what you think! Especially interested in:\n- What observability problems are you hitting with LLMs\n- Missing features that would make this useful for you\n- Any similar tools you&#x27;re using (and what they do better)<p>Thanks for reading!","title":"Show HN: Lumina \u2013 Open-source observability for AI systems(OpenTelemetry-native)","updated_at":"2026-01-27T16:02:58Z","url":"https://github.com/use-lumina/Lumina"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"iggycodexs"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["how","teams","use","claude","code"],"value":"Hey HN! I built Lumina \u2013 an open-source observability platform for AI/LLM applications. Self-host it in 5 minutes with Docker Compose, all features included.<p>The Problem:<p>I've been building LLM apps for the past year, and I kept running into the same issues:\n- LLM responses would randomly change after prompt tweaks, breaking things\n- Costs would spike unexpectedly (turns out a bug was hitting GPT-4 instead of 3.5)\n- No easy way to compare &quot;before vs after&quot; when testing prompt changes\n- Existing tools were either too expensive or missing features in free tiers<p>What I Built:<p>Lumina is OpenTelemetry-native, meaning:\n- Works with your existing OTEL stack (Datadog, Grafana, etc.)\n- No vendor lock-in \u2013 standard trace format\n- Integrates in 3 lines of <em>code</em><p>Key features:\n- Cost &amp; quality monitoring\n\u2013 Automatic alerts when costs spike or responses degrade\n- Replay testing\n\u2013 Capture production traces, replay them after changes, see diffs\n- Semantic comparison\n\u2013 Not just string matching \n\u2013 <em>uses</em> <em>Claude</em> to judge if responses are &quot;better&quot; or &quot;worse&quot;\n- Self-hosted tier \n\u2013 50k traces/day, 7-day retention, ALL features included (alerts, replay, semantic scoring)<p><em>How</em> it works:<p>Start Lumina<p>git clone <a href=\"https://github.com/use-lumina/Lumina\" rel=\"nofollow\">https://github.com/<em>use</em>-lumina/Lumina</a>\ncd Lumina/infra/docker\ndocker-compose up -d<p>// Add to your app (no API key needed for self-hosted!)<p>import { Lumina } from '@uselumina/sdk';<p>const lumina = new Lumina({\n  endpoint: 'http://localhost:8080/v1/traces',\n});<p>// Wrap your LLM call\nconst response = await lumina.traceLLM(\n  async () =&gt; await openai.chat.completions.create({...}),\n  { provider: 'openai', model: 'gpt-4', prompt: '...' }\n);<p>That's it. Every LLM call is now tracked with cost, latency, tokens, and quality scores.<p>What makes it different:<p>1. Free self-hosted with limits that work\n\u2013 50k traces/day and 7-day retention (resets daily at midnight UTC). All features included: alerts, replay testing, semantic scoring. Perfect for most development and small production workloads. Need more? Upgrade to managed cloud.<p>2. OpenTelemetry-native \u2013 Not another proprietary format. <em>Use</em> standard OTEL exporters, works with existing infra. Can send traces to both Lumina AND Datadog simultaneously.<p>3. Replay testing \u2013 The killer feature. Capture 100 production traces, change your prompt, replay them all, get a semantic diff report. Like snapshot testing for LLMs.<p>4. Fast\n\u2013 Built with Bun, Postgres, Redis, NATS. Sub-500ms from trace to alert. Handles 10k+ traces/min on a single machine.<p>What I'm looking for:<p>- Feedback on the approach (is OTEL the right foundation?)\n- Bug reports (tested on Mac/Linux/WSL2, but I'm sure there are issues)\n- Ideas for what features matter most (alerts? replay? cost tracking?)\n- Help with the semantic scorer (currently <em>uses</em> <em>Claude</em>, want to make it pluggable)<p>Why open source:<p>I want this to be the standard for LLM observability. That only works if it's:\n- Free to <em>use</em> and modify (Apache 2.0)\n- Easy to self-host (Docker Compose, no cloud dependencies)\n- Open to contributions (good first issues tagged)<p>The business model is managed hosting for <em>teams</em> who don't want to run infrastructure. But the core product is and always will be free.<p>Try it:\n- GitHub: <a href=\"https://github.com/use-lumina/Lumina\" rel=\"nofollow\">https://github.com/<em>use</em>-lumina/Lumina</a>\n- Demo video: [YouTube link]\n- Docs: <a href=\"https://docs.uselumina.io\" rel=\"nofollow\">https://docs.uselumina.io</a>\n- Quick start: 5 minutes from `git clone` to dashboard<p>I'd love to hear what you think! Especially interested in:\n- What observability problems you're hitting with LLMs\n- Missing features that would make this useful for you\n- Any similar tools you're using (and what they do better)<p>Thanks for reading!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Lumina \u2013 Open-source observability for LLM applications"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["use"],"value":"https://github.com/<em>use</em>-lumina/Lumina"}},"_tags":["story","author_iggycodexs","story_46751546","show_hn"],"author":"iggycodexs","children":[46754171],"created_at":"2026-01-25T07:08:52Z","created_at_i":1769324932,"num_comments":2,"objectID":"46751546","points":6,"story_id":46751546,"story_text":"Hey HN! I built Lumina \u2013 an open-source observability platform for AI&#x2F;LLM applications. Self-host it in 5 minutes with Docker Compose, all features included.<p>The Problem:<p>I&#x27;ve been building LLM apps for the past year, and I kept running into the same issues:\n- LLM responses would randomly change after prompt tweaks, breaking things\n- Costs would spike unexpectedly (turns out a bug was hitting GPT-4 instead of 3.5)\n- No easy way to compare &quot;before vs after&quot; when testing prompt changes\n- Existing tools were either too expensive or missing features in free tiers<p>What I Built:<p>Lumina is OpenTelemetry-native, meaning:\n- Works with your existing OTEL stack (Datadog, Grafana, etc.)\n- No vendor lock-in \u2013 standard trace format\n- Integrates in 3 lines of code<p>Key features:\n- Cost &amp; quality monitoring\n\u2013 Automatic alerts when costs spike or responses degrade\n- Replay testing\n\u2013 Capture production traces, replay them after changes, see diffs\n- Semantic comparison\n\u2013 Not just string matching \n\u2013 uses Claude to judge if responses are &quot;better&quot; or &quot;worse&quot;\n- Self-hosted tier \n\u2013 50k traces&#x2F;day, 7-day retention, ALL features included (alerts, replay, semantic scoring)<p>How it works:<p>Start Lumina<p>git clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\ncd Lumina&#x2F;infra&#x2F;docker\ndocker-compose up -d<p>&#x2F;&#x2F; Add to your app (no API key needed for self-hosted!)<p>import { Lumina } from &#x27;@uselumina&#x2F;sdk&#x27;;<p>const lumina = new Lumina({\n  endpoint: &#x27;http:&#x2F;&#x2F;localhost:8080&#x2F;v1&#x2F;traces&#x27;,\n});<p>&#x2F;&#x2F; Wrap your LLM call\nconst response = await lumina.traceLLM(\n  async () =&gt; await openai.chat.completions.create({...}),\n  { provider: &#x27;openai&#x27;, model: &#x27;gpt-4&#x27;, prompt: &#x27;...&#x27; }\n);<p>That&#x27;s it. Every LLM call is now tracked with cost, latency, tokens, and quality scores.<p>What makes it different:<p>1. Free self-hosted with limits that work\n\u2013 50k traces&#x2F;day and 7-day retention (resets daily at midnight UTC). All features included: alerts, replay testing, semantic scoring. Perfect for most development and small production workloads. Need more? Upgrade to managed cloud.<p>2. OpenTelemetry-native \u2013 Not another proprietary format. Use standard OTEL exporters, works with existing infra. Can send traces to both Lumina AND Datadog simultaneously.<p>3. Replay testing \u2013 The killer feature. Capture 100 production traces, change your prompt, replay them all, get a semantic diff report. Like snapshot testing for LLMs.<p>4. Fast\n\u2013 Built with Bun, Postgres, Redis, NATS. Sub-500ms from trace to alert. Handles 10k+ traces&#x2F;min on a single machine.<p>What I&#x27;m looking for:<p>- Feedback on the approach (is OTEL the right foundation?)\n- Bug reports (tested on Mac&#x2F;Linux&#x2F;WSL2, but I&#x27;m sure there are issues)\n- Ideas for what features matter most (alerts? replay? cost tracking?)\n- Help with the semantic scorer (currently uses Claude, want to make it pluggable)<p>Why open source:<p>I want this to be the standard for LLM observability. That only works if it&#x27;s:\n- Free to use and modify (Apache 2.0)\n- Easy to self-host (Docker Compose, no cloud dependencies)\n- Open to contributions (good first issues tagged)<p>The business model is managed hosting for teams who don&#x27;t want to run infrastructure. But the core product is and always will be free.<p>Try it:\n- GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\n- Demo video: [YouTube link]\n- Docs: <a href=\"https:&#x2F;&#x2F;docs.uselumina.io\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.uselumina.io</a>\n- Quick start: 5 minutes from `git clone` to dashboard<p>I&#x27;d love to hear what you think! Especially interested in:\n- What observability problems you&#x27;re hitting with LLMs\n- Missing features that would make this useful for you\n- Any similar tools you&#x27;re using (and what they do better)<p>Thanks for reading!","title":"Show HN: Lumina \u2013 Open-source observability for LLM applications","updated_at":"2026-01-26T20:07:55Z","url":"https://github.com/use-lumina/Lumina"}],"hitsPerPage":10,"nbHits":85,"nbPages":9,"page":0,"params":"query=how+teams+use+Claude+Code&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":22,"processingTimingsMS":{"_request":{"queue":10,"roundTrip":22},"afterFetch":{"format":{"highlighting":2,"total":2}},"fetch":{"query":6,"scanning":14,"total":21},"total":22},"query":"how teams use Claude Code","serverTimeMS":35}
