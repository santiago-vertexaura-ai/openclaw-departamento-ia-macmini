{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"uejfiweun"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"At my large tech company, we're all being pushed to use AI. I, and most people I work with, have had success using the chatbots and Cursor-style tools and more recently <em>Claude</em> <em>Code</em> to accelerate the process of writing <em>code</em>.<p>Yet, with a few people in my network, it's like they're living 10 years ahead. Guys are automating everything in their jobs, spinning up 10 specialized agents at a time and running multi-agent pipelines, just doing all sorts of crazy things with this tech that I just can't even fathom. It seems like it's making them way more productive.<p>I have found a way to fit <em>code</em>-writing and question-answering chatbots into my <em>workflow</em>. I have NOT done the same in terms of these crazy Agent setups. There's clearly a way to leverage these tools to turbocharge your productivity, like at least 2x or maybe even 10x. But what is it?<p>Are there any Agentic power users out there who can enlighten me? What are the best ways to take advantage of these new tools?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: Any AI / Agent power users out there? Do you have any tips?"}},"_tags":["story","author_uejfiweun","story_47058199","ask_hn"],"author":"uejfiweun","children":[47059255,47060319,47059094],"created_at":"2026-02-18T07:15:50Z","created_at_i":1771398950,"num_comments":3,"objectID":"47058199","points":3,"story_id":47058199,"story_text":"At my large tech company, we&#x27;re all being pushed to use AI. I, and most people I work with, have had success using the chatbots and Cursor-style tools and more recently Claude Code to accelerate the process of writing code.<p>Yet, with a few people in my network, it&#x27;s like they&#x27;re living 10 years ahead. Guys are automating everything in their jobs, spinning up 10 specialized agents at a time and running multi-agent pipelines, just doing all sorts of crazy things with this tech that I just can&#x27;t even fathom. It seems like it&#x27;s making them way more productive.<p>I have found a way to fit code-writing and question-answering chatbots into my workflow. I have NOT done the same in terms of these crazy Agent setups. There&#x27;s clearly a way to leverage these tools to turbocharge your productivity, like at least 2x or maybe even 10x. But what is it?<p>Are there any Agentic power users out there who can enlighten me? What are the best ways to take advantage of these new tools?","title":"Ask HN: Any AI / Agent power users out there? Do you have any tips?","updated_at":"2026-02-18T12:39:06Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"luckygreen"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"I've spent the last week trying to use <em>Claude</em> Opus in an IDE via a localhost proxy that wraps the <em>Claude</em> <em>Code</em> CLI.<p>The proxy exists because there's no way to get programmatic API access to <em>Claude</em> on a Pro ($20/mo) or Max ($200/mo) subscription.<p>This isn't an oversight. Anthropic has actively taken steps to prevent subscription-based API access. They've shut down third-party bridges, tightened terms of service, and made it clear this is corporate policy, not neglect, not a missing feature, but a deliberate business decision.<p>Meanwhile, OpenAI opened API access to ChatGPT Pro/Plus subscribers. The result is predictable.<p>A CTO friend is prototyping an electronic warfare detection system. His existing customers include a major European MNO that operates across dozens of countries. He's currently splitting ~$80/month across four AI subscriptions, constantly rationing tokens, juggling providers. He'd switch to <em>Claude</em> Max tomorrow if it included IDE API access. Instead, he's moving his entire prototype to Codex/OpenAI, because they made it possible.<p>Here's why this matters beyond one developer: The prototype becomes the demo. The demo becomes the procurement spec. The spec becomes a multi-year enterprise contract. This particular MNO signs seven-figure deals per country rollout. Once the prototype is built on OpenAI, <em>Claude</em> never enters the conversation. Anthropic doesn't just lose one deal, they lose an entire multi-country enterprise pipeline they'll never even know existed.<p>This isn't an isolated case. In my circle, senior engineers, CTOs, infrastructure people, nearly everyone would switch from Pro to Max if it meant using <em>Claude</em> in their IDE the way they can now use OpenAI. That's going from $20/month to $200/month per user, zero acquisition cost. Even if only 10-20% of the broader Pro base converts, the revenue math is overwhelming.<p>The proxy ecosystem (<em>claude</em>-max-api-proxy, Antigravity, and others) is direct proof of unmet demand. Developers are writing <em>code</em> to work around a billing boundary. That's not abuse, that's a market signal Anthropic is choosing to ignore.<p><em>Claude</em> Opus is technically superior to Codex for most development tasks. That means nothing when developers can't access it in their <em>workflow</em>.<p>Anthropic built the best model and then made a policy decision to keep it out of the environment where long-term platform adoption is decided."},"title":{"matchLevel":"none","matchedWords":[],"value":"Anthropic's pricing wall is routing enterprise revenue to OpenAI"}},"_tags":["story","author_luckygreen","story_47057752","ask_hn"],"author":"luckygreen","children":[47058509],"created_at":"2026-02-18T06:08:49Z","created_at_i":1771394929,"num_comments":1,"objectID":"47057752","points":4,"story_id":47057752,"story_text":"I&#x27;ve spent the last week trying to use Claude Opus in an IDE via a localhost proxy that wraps the Claude Code CLI.<p>The proxy exists because there&#x27;s no way to get programmatic API access to Claude on a Pro ($20&#x2F;mo) or Max ($200&#x2F;mo) subscription.<p>This isn&#x27;t an oversight. Anthropic has actively taken steps to prevent subscription-based API access. They&#x27;ve shut down third-party bridges, tightened terms of service, and made it clear this is corporate policy, not neglect, not a missing feature, but a deliberate business decision.<p>Meanwhile, OpenAI opened API access to ChatGPT Pro&#x2F;Plus subscribers. The result is predictable.<p>A CTO friend is prototyping an electronic warfare detection system. His existing customers include a major European MNO that operates across dozens of countries. He&#x27;s currently splitting ~$80&#x2F;month across four AI subscriptions, constantly rationing tokens, juggling providers. He&#x27;d switch to Claude Max tomorrow if it included IDE API access. Instead, he&#x27;s moving his entire prototype to Codex&#x2F;OpenAI, because they made it possible.<p>Here&#x27;s why this matters beyond one developer: The prototype becomes the demo. The demo becomes the procurement spec. The spec becomes a multi-year enterprise contract. This particular MNO signs seven-figure deals per country rollout. Once the prototype is built on OpenAI, Claude never enters the conversation. Anthropic doesn&#x27;t just lose one deal, they lose an entire multi-country enterprise pipeline they&#x27;ll never even know existed.<p>This isn&#x27;t an isolated case. In my circle, senior engineers, CTOs, infrastructure people, nearly everyone would switch from Pro to Max if it meant using Claude in their IDE the way they can now use OpenAI. That&#x27;s going from $20&#x2F;month to $200&#x2F;month per user, zero acquisition cost. Even if only 10-20% of the broader Pro base converts, the revenue math is overwhelming.<p>The proxy ecosystem (claude-max-api-proxy, Antigravity, and others) is direct proof of unmet demand. Developers are writing code to work around a billing boundary. That&#x27;s not abuse, that&#x27;s a market signal Anthropic is choosing to ignore.<p>Claude Opus is technically superior to Codex for most development tasks. That means nothing when developers can&#x27;t access it in their workflow.<p>Anthropic built the best model and then made a policy decision to keep it out of the environment where long-term platform adoption is decided.","title":"Anthropic's pricing wall is routing enterprise revenue to OpenAI","updated_at":"2026-02-18T08:08:50Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"maccraft"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"Think about ordering pizza. Instead of opening DoorDash, browsing restaurants, customizing your order, and checking out, you could just say &quot;Hey AI, order me a pepperoni pizza.&quot; Done. When you think about it that way, why would anyone need DoorDash anymore?<p>For simple stuff like this, yeah, AI replacing apps feels inevitable. But I've been thinking about the harder cases, and I'm not so sure.<p>## Most real-world apps solve really messy problems<p>Take something like Jira. It's not just a pretty interface sitting on top of simple logic. Jira is enforcing <em>workflows</em> across teams, managing who can see what, tracking state changes, hooking into CI/CD, keeping audit trails, all while dozens of people with different roles are working in it at the same time. Can you honestly replace all of that with &quot;Hey AI, manage my project&quot;?<p>## And then there's the cost<p>Here's the part that I think people aren't talking about enough.<p>Let's say you need to build a complex financial model in a spreadsheet. Hundreds of rows, nested formulas, pivot tables, cross-sheet references, the whole thing. Now imagine trying to do that entirely by talking to AI.<p>&quot;Move that column.&quot; &quot;No, the other one.&quot; &quot;Now add a VLOOKUP that pulls from the other sheet.&quot; &quot;Actually, the range is wrong.&quot; Every single back-and-forth is burning tokens. If the model is complex enough, you could easily spend more on AI inference in one sitting than you'd pay for Excel for an entire year. And the spreadsheet app just... does it. Deterministic logic, minimal compute, instant feedback.<p>I think this pattern holds more broadly than people realize: the more complex and repetitive the task, the more tokens you burn, and the harder it is for &quot;just ask AI&quot; to compete with a $10/month app on cost alone.<p>Here's where it gets ironic, though. AI coding agents like Cursor and <em>Claude</em> <em>Code</em> are making it way cheaper to <i>build</i> apps. So AI might not shrink the app market at all. It could actually grow it by making apps cheaper to create, while simultaneously making &quot;just use AI directly&quot; expensive for anything non-trivial.<p>*Curious what HN thinks:*<p>- Are we overestimating AI's ability to replace purpose-built software?<p>- Will inference costs drop and AI capabilities advance enough to make this argument irrelevant?<p>- Or will everyone just become a developer, building their own apps for their own needs?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: Can AI replace apps, or will economics keep the app market alive?"}},"_tags":["story","author_maccraft","story_47054299","ask_hn"],"author":"maccraft","children":[47054540],"created_at":"2026-02-17T22:24:33Z","created_at_i":1771367073,"num_comments":1,"objectID":"47054299","points":1,"story_id":47054299,"story_text":"Think about ordering pizza. Instead of opening DoorDash, browsing restaurants, customizing your order, and checking out, you could just say &quot;Hey AI, order me a pepperoni pizza.&quot; Done. When you think about it that way, why would anyone need DoorDash anymore?<p>For simple stuff like this, yeah, AI replacing apps feels inevitable. But I&#x27;ve been thinking about the harder cases, and I&#x27;m not so sure.<p>## Most real-world apps solve really messy problems<p>Take something like Jira. It&#x27;s not just a pretty interface sitting on top of simple logic. Jira is enforcing workflows across teams, managing who can see what, tracking state changes, hooking into CI&#x2F;CD, keeping audit trails, all while dozens of people with different roles are working in it at the same time. Can you honestly replace all of that with &quot;Hey AI, manage my project&quot;?<p>## And then there&#x27;s the cost<p>Here&#x27;s the part that I think people aren&#x27;t talking about enough.<p>Let&#x27;s say you need to build a complex financial model in a spreadsheet. Hundreds of rows, nested formulas, pivot tables, cross-sheet references, the whole thing. Now imagine trying to do that entirely by talking to AI.<p>&quot;Move that column.&quot; &quot;No, the other one.&quot; &quot;Now add a VLOOKUP that pulls from the other sheet.&quot; &quot;Actually, the range is wrong.&quot; Every single back-and-forth is burning tokens. If the model is complex enough, you could easily spend more on AI inference in one sitting than you&#x27;d pay for Excel for an entire year. And the spreadsheet app just... does it. Deterministic logic, minimal compute, instant feedback.<p>I think this pattern holds more broadly than people realize: the more complex and repetitive the task, the more tokens you burn, and the harder it is for &quot;just ask AI&quot; to compete with a $10&#x2F;month app on cost alone.<p>Here&#x27;s where it gets ironic, though. AI coding agents like Cursor and Claude Code are making it way cheaper to <i>build</i> apps. So AI might not shrink the app market at all. It could actually grow it by making apps cheaper to create, while simultaneously making &quot;just use AI directly&quot; expensive for anything non-trivial.<p>*Curious what HN thinks:*<p>- Are we overestimating AI&#x27;s ability to replace purpose-built software?<p>- Will inference costs drop and AI capabilities advance enough to make this argument irrelevant?<p>- Or will everyone just become a developer, building their own apps for their own needs?","title":"Ask HN: Can AI replace apps, or will economics keep the app market alive?","updated_at":"2026-02-17T23:56:49Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"rhl"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"I read a lot of research papers for work. My <em>workflow</em> evolved around an ever-growing inbox of bookmarked papers from arXiv et al. Great for exploration, but hard to keep track of what I read.<p>Distillate bridges the tools I already use: Zotero (literature management), reMarkable (reader + highlighter), and Obsidian (notes). It automates the whole pipeline:<p>$ distillate<p>save to Zotero \u2500\u2500&gt; auto-syncs to reMarkable<p><pre><code>                        \u2502\n\n         read &amp; highlight on tablet\n         just move to Read/ when done\n\n                        \u2502\n\n                        V\n\n         auto-saves notes + highlights\n</code></pre>\nIt polls Zotero for new papers, uploads PDFs to the reMarkable via rmapi, then watches for papers you've finished reading in your Read folder. When it finds one, it:<p>- Parses .rm files using rmscene to extract highlighted text (GlyphRange items)<p>- Searches for that text in the original PDF using PyMuPDF and adds highlight annotations<p>- Enriches metadata from Semantic Scholar (publication date, venue, citations)<p>- Creates a structured markdown note with metadata, highlights grouped by page, and the annotated PDF (I keep mine in an Obsidian vault)<p>The core <em>workflow</em> just needs Zotero and a reMarkable \u2014 no paid APIs, no cloud backend, your notes stay on your machine. Optional extras if you plug them in:<p>- AI summaries via <em>Claude</em> (one-liner + key learnings from your highlights)<p>- Daily reading suggestions from your queue<p>- Weekly email digest via Resend<p>- Obsidian Bases database for tracking your reading<p>Stack: rmapi for reMarkable Cloud, rmscene for .rm parsing, PyMuPDF for PDF annotation. Python 3.10+, pip installable.<p>The trickiest part was highlight extraction: reMarkable stores highlighted text as GlyphRange items in a scene tree, and matching that text back to positions in the original PDF required fuzzy search with OCR cleanup, plus special merging logic for e.g. cross-page highlights. Happy to say it works well ~99% of the time now.<p>Install: pip install distillate &amp;&amp; distillate --init<p><em>Code</em>: <a href=\"https://github.com/rlacombe/distillate\" rel=\"nofollow\">https://github.com/rlacombe/distillate</a><p>Site: <a href=\"https://distillate.dev\" rel=\"nofollow\">https://distillate.dev</a><p>I built this for myself but would love feedback, especially from other reMarkable + Zotero users. What's missing from your <em>workflow</em>? What else should I add?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Distillate \u2013 Zotero papers \u2192 reMarkable highlights \u2192 Obsidian notes"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://distillate.dev"}},"_tags":["story","author_rhl","story_47053179","show_hn"],"author":"rhl","children":[47054713,47053352],"created_at":"2026-02-17T20:54:12Z","created_at_i":1771361652,"num_comments":3,"objectID":"47053179","points":3,"story_id":47053179,"story_text":"I read a lot of research papers for work. My workflow evolved around an ever-growing inbox of bookmarked papers from arXiv et al. Great for exploration, but hard to keep track of what I read.<p>Distillate bridges the tools I already use: Zotero (literature management), reMarkable (reader + highlighter), and Obsidian (notes). It automates the whole pipeline:<p>$ distillate<p>save to Zotero \u2500\u2500&gt; auto-syncs to reMarkable<p><pre><code>                        \u2502\n\n         read &amp; highlight on tablet\n         just move to Read&#x2F; when done\n\n                        \u2502\n\n                        V\n\n         auto-saves notes + highlights\n</code></pre>\nIt polls Zotero for new papers, uploads PDFs to the reMarkable via rmapi, then watches for papers you&#x27;ve finished reading in your Read folder. When it finds one, it:<p>- Parses .rm files using rmscene to extract highlighted text (GlyphRange items)<p>- Searches for that text in the original PDF using PyMuPDF and adds highlight annotations<p>- Enriches metadata from Semantic Scholar (publication date, venue, citations)<p>- Creates a structured markdown note with metadata, highlights grouped by page, and the annotated PDF (I keep mine in an Obsidian vault)<p>The core workflow just needs Zotero and a reMarkable \u2014 no paid APIs, no cloud backend, your notes stay on your machine. Optional extras if you plug them in:<p>- AI summaries via Claude (one-liner + key learnings from your highlights)<p>- Daily reading suggestions from your queue<p>- Weekly email digest via Resend<p>- Obsidian Bases database for tracking your reading<p>Stack: rmapi for reMarkable Cloud, rmscene for .rm parsing, PyMuPDF for PDF annotation. Python 3.10+, pip installable.<p>The trickiest part was highlight extraction: reMarkable stores highlighted text as GlyphRange items in a scene tree, and matching that text back to positions in the original PDF required fuzzy search with OCR cleanup, plus special merging logic for e.g. cross-page highlights. Happy to say it works well ~99% of the time now.<p>Install: pip install distillate &amp;&amp; distillate --init<p>Code: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;rlacombe&#x2F;distillate\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;rlacombe&#x2F;distillate</a><p>Site: <a href=\"https:&#x2F;&#x2F;distillate.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;distillate.dev</a><p>I built this for myself but would love feedback, especially from other reMarkable + Zotero users. What&#x27;s missing from your workflow? What else should I add?","title":"Show HN: Distillate \u2013 Zotero papers \u2192 reMarkable highlights \u2192 Obsidian notes","updated_at":"2026-02-17T23:29:34Z","url":"https://distillate.dev"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Dimittri"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https://sonarly.com\">https://sonarly.com</a>), an AI engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here's a demo: <a href=\"https://www.youtube.com/watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https://www.youtube.com/watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it's a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in production, due to <em>code</em> errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can't catch all these beforehand, even with E2E tests or AI <em>code</em> reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don't need to check logs, dashboards, traces, metrics, and <em>code</em>, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don't want to add a new tracker or change their monitoring stack, as these platforms do the job they're supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p><em>Claude</em> <em>Code</em> is so good at writing <em>code</em>, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger <em>Claude</em> <em>Code</em> with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog/Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that <em>Claude</em> <em>Code</em> can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts/day. Here is what their <em>workflow</em> looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don't look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving <em>Claude</em> <em>Code</em> the correct context<p>We started by cutting the noise and went from 180/day to 50/day (by grouping issues) and giving a severity based on the impact on the user/infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https://sonarly.com\">https://sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current <em>workflows</em> when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated <em>workflow</em> to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I'll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything's constructive!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your production alerts"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://sonarly.com/"}},"_tags":["story","author_Dimittri","story_47049776","launch_hn"],"author":"Dimittri","children":[47055028,47052409,47055342,47054509],"created_at":"2026-02-17T17:03:09Z","created_at_i":1771347789,"num_comments":13,"objectID":"47049776","points":29,"story_id":47049776,"story_text":"Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>), an AI engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here&#x27;s a demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it&#x27;s a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in production, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can&#x27;t catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don&#x27;t need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don&#x27;t want to add a new tracker or change their monitoring stack, as these platforms do the job they&#x27;re supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog&#x2F;Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts&#x2F;day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don&#x27;t look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180&#x2F;day to 50&#x2F;day (by grouping issues) and giving a severity based on the impact on the user&#x2F;infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I&#x27;ll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything&#x27;s constructive!","title":"Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your production alerts","updated_at":"2026-02-18T10:24:06Z","url":"https://sonarly.com/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"robotelvis"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"Hi HN, I'm Rob. I built Broomy because I got frustrated with the one-thing-at-a-time <em>workflow</em> of existing coding tools.<p>When I work with AI coding agents, I typically have 5-10 tasks going at once across different branches. The agent works on one thing while I review another, merge a third, and kick off a fourth. Existing IDEs aren't built for this \u2014 they assume you're doing one thing at a time.<p>Broomy is a desktop app (Electron + React) that lets you:<p>- Run lots of agent sessions simultaneously and see at a glance which are working, idle, or need your attention\n- Work with any terminal-based agent (<em>Claude</em> <em>Code</em>, Aider, Codex, etc.)\n- Review <em>code</em>, manage branches, and handle merges with AI assistance\n- Use built-in IDE features (Monaco editor, file explorer, git integration, inline terminals) \u2014 all designed around multi-agent <em>workflows</em><p>I've been using it daily for a few weeks and my productivity has dramatically improved compared to working in Cursor. The key insight is that most of the time you spend &quot;coding with AI&quot; is actually waiting \u2014 and Broomy lets you fill that wait time with other tasks.<p>This is a first public release (v0.6.0). Pre-built binaries are available for macOS. It should work on Linux and Windows too \u2014 build from source is straightforward (clone, pnpm install, pnpm start:dist).<p>MIT licensed. Built as a personal project, not affiliated with my employer.<p>Repo: <a href=\"https://github.com/Broomy-AI/broomy\" rel=\"nofollow\">https://github.com/Broomy-AI/broomy</a>\nWebsite: <a href=\"https://broomy.org\" rel=\"nofollow\">https://broomy.org</a><p>Happy to answer questions."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Broomy \u2013 Open-source app for working with many AI agents at once"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://broomy.org/"}},"_tags":["story","author_robotelvis","story_47048886","show_hn"],"author":"robotelvis","created_at":"2026-02-17T15:55:38Z","created_at_i":1771343738,"num_comments":0,"objectID":"47048886","points":1,"story_id":47048886,"story_text":"Hi HN, I&#x27;m Rob. I built Broomy because I got frustrated with the one-thing-at-a-time workflow of existing coding tools.<p>When I work with AI coding agents, I typically have 5-10 tasks going at once across different branches. The agent works on one thing while I review another, merge a third, and kick off a fourth. Existing IDEs aren&#x27;t built for this \u2014 they assume you&#x27;re doing one thing at a time.<p>Broomy is a desktop app (Electron + React) that lets you:<p>- Run lots of agent sessions simultaneously and see at a glance which are working, idle, or need your attention\n- Work with any terminal-based agent (Claude Code, Aider, Codex, etc.)\n- Review code, manage branches, and handle merges with AI assistance\n- Use built-in IDE features (Monaco editor, file explorer, git integration, inline terminals) \u2014 all designed around multi-agent workflows<p>I&#x27;ve been using it daily for a few weeks and my productivity has dramatically improved compared to working in Cursor. The key insight is that most of the time you spend &quot;coding with AI&quot; is actually waiting \u2014 and Broomy lets you fill that wait time with other tasks.<p>This is a first public release (v0.6.0). Pre-built binaries are available for macOS. It should work on Linux and Windows too \u2014 build from source is straightforward (clone, pnpm install, pnpm start:dist).<p>MIT licensed. Built as a personal project, not affiliated with my employer.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Broomy-AI&#x2F;broomy\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Broomy-AI&#x2F;broomy</a>\nWebsite: <a href=\"https:&#x2F;&#x2F;broomy.org\" rel=\"nofollow\">https:&#x2F;&#x2F;broomy.org</a><p>Happy to answer questions.","title":"Show HN: Broomy \u2013 Open-source app for working with many AI agents at once","updated_at":"2026-02-17T15:58:34Z","url":"https://broomy.org/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"thijsverreck"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"I take notes, and draft designs on a reMarkable tablet and wanted <em>Claude</em> to be able to reference them while I <em>code</em>.<p>So I built an Open Source MCP server that connects to the reMarkable Cloud API and gives AI assistants (<em>Claude</em> <em>Code</em>, OpenClaw, etc) read-only access to your entire library.<p>What it does:<p>- Read notebooks, PDFs, and ebooks with full text extraction\n- Full-text search across your library (SQLite FTS5 index)\n- Render pages as PNG/SVG \u2014 useful for hand-drawn diagrams and wireframes\n- Handwriting OCR using the client's own LLM via MCP sampling (no external API keys needed)<p>Setup is super easy using the following command:<p>curl -fsSL <a href=\"https://thijsverreck.com/setup.sh\" rel=\"nofollow\">https://thijsverreck.com/setup.sh</a> | sh<p>It installs dependencies, registers your tablet, and configures both <em>Claude</em> <em>Code</em> andClaude Desktop. The server runs via uvx and auto-updates on each launch.<p>Built with Python, runs on the MCP protocol so it works with any compatible client. Everything is read-only \u2014 it never writes to your tablet.<p>Would love feedback, especially from other reMarkable users who've been wanting better integration with their dev <em>workflows</em>."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude"],"value":"Show HN: Rm-MCP \u2013 Give <em>Claude</em>/OpenClaw access to your reMarkable tablet"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/wavyrai/rm-mcp"}},"_tags":["story","author_thijsverreck","story_47047484","show_hn"],"author":"thijsverreck","created_at":"2026-02-17T13:52:53Z","created_at_i":1771336373,"num_comments":0,"objectID":"47047484","points":3,"story_id":47047484,"story_text":"I take notes, and draft designs on a reMarkable tablet and wanted Claude to be able to reference them while I code.<p>So I built an Open Source MCP server that connects to the reMarkable Cloud API and gives AI assistants (Claude Code, OpenClaw, etc) read-only access to your entire library.<p>What it does:<p>- Read notebooks, PDFs, and ebooks with full text extraction\n- Full-text search across your library (SQLite FTS5 index)\n- Render pages as PNG&#x2F;SVG \u2014 useful for hand-drawn diagrams and wireframes\n- Handwriting OCR using the client&#x27;s own LLM via MCP sampling (no external API keys needed)<p>Setup is super easy using the following command:<p>curl -fsSL <a href=\"https:&#x2F;&#x2F;thijsverreck.com&#x2F;setup.sh\" rel=\"nofollow\">https:&#x2F;&#x2F;thijsverreck.com&#x2F;setup.sh</a> | sh<p>It installs dependencies, registers your tablet, and configures both Claude Code andClaude Desktop. The server runs via uvx and auto-updates on each launch.<p>Built with Python, runs on the MCP protocol so it works with any compatible client. Everything is read-only \u2014 it never writes to your tablet.<p>Would love feedback, especially from other reMarkable users who&#x27;ve been wanting better integration with their dev workflows.","title":"Show HN: Rm-MCP \u2013 Give Claude/OpenClaw access to your reMarkable tablet","updated_at":"2026-02-17T14:24:33Z","url":"https://github.com/wavyrai/rm-mcp"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"madcash"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"Hey HN \u2013 I built VoteShip, a feature request and voting platform where AI agents are first-class users.<p>The idea: Your coding agent (<em>Claude</em> <em>Code</em>, Cursor, OpenClaw, Codex, etc.) should be able to check what users are asking for, find duplicates, plan what to build next, and write the changelog \u2014 without you ever opening a dashboard. VoteShip makes that possible through MCP and a full REST API.<p>How it works with agents: Install @voteship/mcp-server (22 tools, 5 resources, 4 <em>workflow</em> prompts) and your agent can:\n  - Pull your unreviewed feedback inbox and triage it\n  - Detect duplicate requests via semantic search (pgvector + Voyage embeddings)\n  - Check vote counts and suggest what to build this sprint\n  - Draft a changelog entry after shipping a feature\n  - Create, tag, and update posts programmatically<p>An agent like OpenClaw can autonomously monitor your board, identify trending requests, and open PRs to address them \u2014 closing the loop from &quot;user asks for feature&quot; to &quot;agent ships it&quot; without human intervention. The REST API and webhooks work just as well for custom agent pipelines.<p>Why I built it: I was using Canny and kept hitting walls \u2014 no anonymous voting (their most-requested feature, ironically), AI features paywalled, and per-&quot;tracked-user&quot; pricing that punishes you for having an active community. And none of the incumbents have any agent integration at all.<p>What else it does:\n  - Public voting boards with anonymous voting (no signup required)\n  - Public roadmaps and changelogs\n  - 7 built-in AI features \u2014 duplicate detection, auto-categorization, sentiment analysis, changelog generation\n  - Embeddable widget (Preact + Shadow DOM)\n  - Import from Canny/Nolt/UserVoice in ~2 minutes<p>Pricing: Free / $5 / $15 / $25 per month. Flat pricing, no per-user fees. The $15 plan includes everything Canny charges $359/mo for.<p>Try it: <a href=\"https://voteship.app\" rel=\"nofollow\">https://voteship.app</a><p>MCP server: npx @voteship/mcp-server (npm)<p>Happy to answer questions about the MCP implementation, agent <em>workflows</em>, or anything else."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: VoteShip \u2013 Feature request platform built for AI agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://voteship.app/"}},"_tags":["story","author_madcash","story_47040022","show_hn"],"author":"madcash","created_at":"2026-02-16T20:38:12Z","created_at_i":1771274292,"num_comments":0,"objectID":"47040022","points":1,"story_id":47040022,"story_text":"Hey HN \u2013 I built VoteShip, a feature request and voting platform where AI agents are first-class users.<p>The idea: Your coding agent (Claude Code, Cursor, OpenClaw, Codex, etc.) should be able to check what users are asking for, find duplicates, plan what to build next, and write the changelog \u2014 without you ever opening a dashboard. VoteShip makes that possible through MCP and a full REST API.<p>How it works with agents: Install @voteship&#x2F;mcp-server (22 tools, 5 resources, 4 workflow prompts) and your agent can:\n  - Pull your unreviewed feedback inbox and triage it\n  - Detect duplicate requests via semantic search (pgvector + Voyage embeddings)\n  - Check vote counts and suggest what to build this sprint\n  - Draft a changelog entry after shipping a feature\n  - Create, tag, and update posts programmatically<p>An agent like OpenClaw can autonomously monitor your board, identify trending requests, and open PRs to address them \u2014 closing the loop from &quot;user asks for feature&quot; to &quot;agent ships it&quot; without human intervention. The REST API and webhooks work just as well for custom agent pipelines.<p>Why I built it: I was using Canny and kept hitting walls \u2014 no anonymous voting (their most-requested feature, ironically), AI features paywalled, and per-&quot;tracked-user&quot; pricing that punishes you for having an active community. And none of the incumbents have any agent integration at all.<p>What else it does:\n  - Public voting boards with anonymous voting (no signup required)\n  - Public roadmaps and changelogs\n  - 7 built-in AI features \u2014 duplicate detection, auto-categorization, sentiment analysis, changelog generation\n  - Embeddable widget (Preact + Shadow DOM)\n  - Import from Canny&#x2F;Nolt&#x2F;UserVoice in ~2 minutes<p>Pricing: Free &#x2F; $5 &#x2F; $15 &#x2F; $25 per month. Flat pricing, no per-user fees. The $15 plan includes everything Canny charges $359&#x2F;mo for.<p>Try it: <a href=\"https:&#x2F;&#x2F;voteship.app\" rel=\"nofollow\">https:&#x2F;&#x2F;voteship.app</a><p>MCP server: npx @voteship&#x2F;mcp-server (npm)<p>Happy to answer questions about the MCP implementation, agent workflows, or anything else.","title":"Show HN: VoteShip \u2013 Feature request platform built for AI agents","updated_at":"2026-02-16T20:43:31Z","url":"https://voteship.app/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"hcwilk"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"I've been using <em>Claude</em> <em>Code</em> ever since it came out, and I've seen good success with it. However, Matt Shumer's recent article [1] included a piece I'm curious about:<p>When describing his <em>workflow</em> now, he says &quot;Then, and this is the part that would have been unthinkable a year ago, it opens the app itself. It clicks through the buttons. It tests the features. It uses the app the way a person would.&quot;<p>Unless I missed a major development, do these coding agents have the ability to use feedback loops like this? If so, how have you all built this functionality into your <em>workflow</em>?<p>Any guidance / advice / documentation I can read would be super helpful!<p>[1]: https://x.com/mattshumer_/status/2021256989876109403"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: Feedback Loop with Coding Agents?"}},"_tags":["story","author_hcwilk","story_47038200","ask_hn"],"author":"hcwilk","created_at":"2026-02-16T18:11:54Z","created_at_i":1771265514,"num_comments":0,"objectID":"47038200","points":1,"story_id":47038200,"story_text":"I&#x27;ve been using Claude Code ever since it came out, and I&#x27;ve seen good success with it. However, Matt Shumer&#x27;s recent article [1] included a piece I&#x27;m curious about:<p>When describing his workflow now, he says &quot;Then, and this is the part that would have been unthinkable a year ago, it opens the app itself. It clicks through the buttons. It tests the features. It uses the app the way a person would.&quot;<p>Unless I missed a major development, do these coding agents have the ability to use feedback loops like this? If so, how have you all built this functionality into your workflow?<p>Any guidance &#x2F; advice &#x2F; documentation I can read would be super helpful!<p>[1]: https:&#x2F;&#x2F;x.com&#x2F;mattshumer_&#x2F;status&#x2F;2021256989876109403","title":"Ask HN: Feedback Loop with Coding Agents?","updated_at":"2026-02-16T18:17:00Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"laurex"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"I Tried New <em>Claude</em> <em>Code</em> Ollama <em>Workflow</em> (It's Wild and Free)"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"https://medium.com/@joe.njenga/i-tried-new-<em>claude</em>-<em>code</em>-ollama-<em>workflow</em>-its-wild-free-cb7a12b733b5"}},"_tags":["story","author_laurex","story_47037956"],"author":"laurex","created_at":"2026-02-16T17:50:41Z","created_at_i":1771264241,"num_comments":0,"objectID":"47037956","points":1,"story_id":47037956,"title":"I Tried New Claude Code Ollama Workflow (It's Wild and Free)","updated_at":"2026-02-16T17:56:16Z","url":"https://medium.com/@joe.njenga/i-tried-new-claude-code-ollama-workflow-its-wild-free-cb7a12b733b5"}],"hitsPerPage":10,"nbHits":454,"nbPages":46,"page":0,"params":"query=Claude+Code+workflows&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":17,"processingTimingsMS":{"_request":{"roundTrip":18},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":7,"scanning":8,"total":16},"total":17},"query":"Claude Code workflows","serverTimeMS":19}
