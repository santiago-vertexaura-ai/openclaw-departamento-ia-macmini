{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ahmaman"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Your <em>Agent</em> <em>Framework</em> Is Just a Bad Clone of Elixir"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"https://georgeguimaraes.com/your-<em>agent</em>-orchestrator-is-just-a-bad-clone-of-elixir/"}},"_tags":["story","author_ahmaman","story_47058401"],"author":"ahmaman","created_at":"2026-02-18T07:49:37Z","created_at_i":1771400977,"num_comments":0,"objectID":"47058401","points":3,"story_id":47058401,"title":"Your Agent Framework Is Just a Bad Clone of Elixir","updated_at":"2026-02-18T12:11:36Z","url":"https://georgeguimaraes.com/your-agent-orchestrator-is-just-a-bad-clone-of-elixir/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"borhensaidi"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"I built this because existing <em>agent</em> <em>frameworks</em> either give agents god-mode filesystem access with no safety controls, or they're too locked down to be useful.<p>Key things I cared about that most <em>frameworks</em> skip:<p>- Runtime HITL checkpoints \u2014 pause mid-execution when risk is high, resume from exact state\n- Hybrid memory: vector + BM25 keyword + graph entities/relations scored together, no external dependencies\n- Security as a primitive: path jail sandbox, OTP pairing, XOR-encrypted local secrets\n- 22+ LLM providers with per-<em>agent</em> model policy and fallback chains\n- Multi-<em>agent</em> councils with debate rounds, soul evolution, and skill memory<p>Node.js, Prisma, Postgres/Redis, Docker. 55/55 tests passing. Apache 2.0.<p>Happy to answer questions about any of the architecture decisions."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: Sovereign \u2013 Multi-<em>agent</em> OS with GraphRAG memory and HITL checkpoints"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/borhen68/SOVEREIGN"}},"_tags":["story","author_borhensaidi","story_47058358","show_hn"],"author":"borhensaidi","created_at":"2026-02-18T07:43:45Z","created_at_i":1771400625,"num_comments":0,"objectID":"47058358","points":1,"story_id":47058358,"story_text":"I built this because existing agent frameworks either give agents god-mode filesystem access with no safety controls, or they&#x27;re too locked down to be useful.<p>Key things I cared about that most frameworks skip:<p>- Runtime HITL checkpoints \u2014 pause mid-execution when risk is high, resume from exact state\n- Hybrid memory: vector + BM25 keyword + graph entities&#x2F;relations scored together, no external dependencies\n- Security as a primitive: path jail sandbox, OTP pairing, XOR-encrypted local secrets\n- 22+ LLM providers with per-agent model policy and fallback chains\n- Multi-agent councils with debate rounds, soul evolution, and skill memory<p>Node.js, Prisma, Postgres&#x2F;Redis, Docker. 55&#x2F;55 tests passing. Apache 2.0.<p>Happy to answer questions about any of the architecture decisions.","title":"Show HN: Sovereign \u2013 Multi-agent OS with GraphRAG memory and HITL checkpoints","updated_at":"2026-02-18T07:47:20Z","url":"https://github.com/borhen68/SOVEREIGN"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"phaedrus044"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Almost 10 years ago, I used to host a film club in my office. A friend of mine was hosting one in the city and they needed a space.<p>I had an openfloor office and that meant we clear the space, fire up the projector and can host the film club. They met between 9pm to 7am once a month, on a friday. Each meeting had 100-150 people attending.<p>This went on for a few months - there was a curator assigned for each meet who would take a theme, and showcase films from the first film ever made in that genre all the way to modern versions and how the evolution happened. It was deconstruction of the craft.<p>The conversation also involved some of these filmmakers showing their own works, and how they did certain cuts and why.<p>What I observed was that none of these creative folks had any data to back their decisions they were making. It was purely gut and intuition, and I could see some editors and producers rolling their eyes - because they felt what that meant. Gut and intuition meant, uncertainty and it plays tricks with your head, so you are constantly making variations till at some point you develop tunnel vision.<p>Something a director told me, stuck with me - he said at some point we just want to be done with the project - and between the studio's demands and the producers prodding, we just let it go and move on to the next project.<p>A producer once said, that if every director was allowed to have their way - every movie would be 4 hours long and there wouldn\u2019t be a single shot footage that would be left out.<p>But then there are cases of films like Man of Steel where Zack Snyder's cut of the film was better - but the studio and producer's call won and audiences weren\u2019t as thrilled about it when they watched the film. And apart for the real enthusiasts nobody really goes and hunts down the director's cut of a film anyways.<p>The need was clear - the industry needed analytics to know what works and doesn\u2019t, similar to how startups got the lean startup <em>framework</em> where everything shifted to building the minimum lovable product and then building it out from there.  The caveat, unlike a product, which can launch, analytics can be acquired, and we can tweak and release - there is no concept of re-release of a film. It gets one shot and if it misses it, its done. That explains how the film industry has a 7% hit rate.<p>It has been a little over 10 years since I hosted the film club, but that issue lingers - and given how that the industry spends over 150bn a year on creating production content (tv shows, movies) it is a big problem.<p>We started using a hardware that captures occulometric data and heart rate that can be used complementarily during audience test screenings - and that gave us a lot of insane depth - on a microsecond level where content was engaging and where it was failing.<p>but the question then was, after a film has been shot, the cost of redoing shots becomes extremely expensive. What the industry calls as &quot;pickup shots&quot; is seldom done - because the artists have moved on and recreating that exact scene and moment is extremely hard. So we built a database of 120 odd films across various genres, and used that audience data to train a custom model, that can then look at past films to build benchmark data - and then use that as comparables against scripts that someone might be planning.<p>We launched this as Quanten Arc (arc.quanten.co) last week. This helps filmmakers - especially indie filmmakers who could use all the data in the world, because they might not even have the budgets to do audience testing. But it even more so helps AI filmmakers and studios, who can now identify the exact scenes that <em>arent</em> working - and can regenerate them with the required changes in the narrative.<p>Im curious to hear what you think about it. Am I solving a real problem or am I imagining a problem that doesn't exist and getting caught up in the beauty of data?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: I analyzed 120 films to help screenwriters test narrative structure"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://arc.quanten.co"}},"_tags":["story","author_phaedrus044","story_47058351","show_hn"],"author":"phaedrus044","created_at":"2026-02-18T07:42:13Z","created_at_i":1771400533,"num_comments":0,"objectID":"47058351","points":1,"story_id":47058351,"story_text":"Almost 10 years ago, I used to host a film club in my office. A friend of mine was hosting one in the city and they needed a space.<p>I had an openfloor office and that meant we clear the space, fire up the projector and can host the film club. They met between 9pm to 7am once a month, on a friday. Each meeting had 100-150 people attending.<p>This went on for a few months - there was a curator assigned for each meet who would take a theme, and showcase films from the first film ever made in that genre all the way to modern versions and how the evolution happened. It was deconstruction of the craft.<p>The conversation also involved some of these filmmakers showing their own works, and how they did certain cuts and why.<p>What I observed was that none of these creative folks had any data to back their decisions they were making. It was purely gut and intuition, and I could see some editors and producers rolling their eyes - because they felt what that meant. Gut and intuition meant, uncertainty and it plays tricks with your head, so you are constantly making variations till at some point you develop tunnel vision.<p>Something a director told me, stuck with me - he said at some point we just want to be done with the project - and between the studio&#x27;s demands and the producers prodding, we just let it go and move on to the next project.<p>A producer once said, that if every director was allowed to have their way - every movie would be 4 hours long and there wouldn\u2019t be a single shot footage that would be left out.<p>But then there are cases of films like Man of Steel where Zack Snyder&#x27;s cut of the film was better - but the studio and producer&#x27;s call won and audiences weren\u2019t as thrilled about it when they watched the film. And apart for the real enthusiasts nobody really goes and hunts down the director&#x27;s cut of a film anyways.<p>The need was clear - the industry needed analytics to know what works and doesn\u2019t, similar to how startups got the lean startup framework where everything shifted to building the minimum lovable product and then building it out from there.  The caveat, unlike a product, which can launch, analytics can be acquired, and we can tweak and release - there is no concept of re-release of a film. It gets one shot and if it misses it, its done. That explains how the film industry has a 7% hit rate.<p>It has been a little over 10 years since I hosted the film club, but that issue lingers - and given how that the industry spends over 150bn a year on creating production content (tv shows, movies) it is a big problem.<p>We started using a hardware that captures occulometric data and heart rate that can be used complementarily during audience test screenings - and that gave us a lot of insane depth - on a microsecond level where content was engaging and where it was failing.<p>but the question then was, after a film has been shot, the cost of redoing shots becomes extremely expensive. What the industry calls as &quot;pickup shots&quot; is seldom done - because the artists have moved on and recreating that exact scene and moment is extremely hard. So we built a database of 120 odd films across various genres, and used that audience data to train a custom model, that can then look at past films to build benchmark data - and then use that as comparables against scripts that someone might be planning.<p>We launched this as Quanten Arc (arc.quanten.co) last week. This helps filmmakers - especially indie filmmakers who could use all the data in the world, because they might not even have the budgets to do audience testing. But it even more so helps AI filmmakers and studios, who can now identify the exact scenes that arent working - and can regenerate them with the required changes in the narrative.<p>Im curious to hear what you think about it. Am I solving a real problem or am I imagining a problem that doesn&#x27;t exist and getting caught up in the beauty of data?","title":"Show HN: I analyzed 120 films to help screenwriters test narrative structure","updated_at":"2026-02-18T07:47:20Z","url":"https://arc.quanten.co"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"AhmedAllam0"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hey HN,<p>When an AI <em>agent</em> makes a decision, it evaluates several options and picks one. The rest disappear forever \u2013 you never see what it almost did or why it rejected the alternatives.<p>I built GhostTrace to fix that. It captures &quot;Phantom Branches&quot;: the actions your <em>agent</em> considered but rejected, with the reasoning for each rejection. All saved to a .ghost.json file you can replay and inspect.<p>Quick demo:<p>ghosttrace record\n Recorded 4 decisions with 5 phantom branches\n Saved to gt_a1b2c3d4.ghost.json<p>ghosttrace replay gt_ghost.json --show-phantoms<p>Step 1:  read_file \u2192 src/auth.py\n REJECTED: write_file (premature)\n REJECTED: search_codebase (too broad)<p>pip install ghosttrace<p>PyPI: <a href=\"https://pypi.org/project/ghosttrace/\" rel=\"nofollow\">https://pypi.org/project/ghosttrace/</a><p>It's <em>framework</em>-agnostic for now, but what <em>agent</em> <em>frameworks</em> should I integrate first? LangChain? CrewAI? OpenAI <em>Agents</em> SDK?<p>Feedback very welcome!"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: GhostTrace \u2013 See rejected decisions in AI <em>agents</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/AhmedAllam0/ghosttrace"}},"_tags":["story","author_AhmedAllam0","story_47057488","show_hn"],"author":"AhmedAllam0","children":[47057637],"created_at":"2026-02-18T05:24:44Z","created_at_i":1771392284,"num_comments":2,"objectID":"47057488","points":1,"story_id":47057488,"story_text":"Hey HN,<p>When an AI agent makes a decision, it evaluates several options and picks one. The rest disappear forever \u2013 you never see what it almost did or why it rejected the alternatives.<p>I built GhostTrace to fix that. It captures &quot;Phantom Branches&quot;: the actions your agent considered but rejected, with the reasoning for each rejection. All saved to a .ghost.json file you can replay and inspect.<p>Quick demo:<p>ghosttrace record\n Recorded 4 decisions with 5 phantom branches\n Saved to gt_a1b2c3d4.ghost.json<p>ghosttrace replay gt_ghost.json --show-phantoms<p>Step 1:  read_file \u2192 src&#x2F;auth.py\n REJECTED: write_file (premature)\n REJECTED: search_codebase (too broad)<p>pip install ghosttrace<p>PyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;ghosttrace&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;ghosttrace&#x2F;</a><p>It&#x27;s framework-agnostic for now, but what agent frameworks should I integrate first? LangChain? CrewAI? OpenAI Agents SDK?<p>Feedback very welcome!","title":"Show HN: GhostTrace \u2013 See rejected decisions in AI agents","updated_at":"2026-02-18T05:58:21Z","url":"https://github.com/AhmedAllam0/ghosttrace"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"bengia"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"I built an open-source Claude Code skill called /think that applies a structured 5-element analysis <em>framework</em> (ground in facts, stress-test for failure, reframe the question, trace implications, audit your own reasoning) before synthesizing a recommendation.\nThe obvious question: does it actually produce better output than just asking Claude directly?\nTo test this, I ran blind A/B comparisons. Two isolated Claude Opus 4.6 <em>agents</em> get the same question \u2014 one runs /think, one responds naturally. Both responses are anonymized (<em>framework</em> markers stripped, sections retitled by content) and presented blind.\nThe test covers 5 topics any professional would recognize: scaling a team post-fundraise, build vs buy decisions, when to pivot a product, SaaS pricing strategy, and the remote/hybrid/office debate.\nAn AI judge scored /think winning all 5 pairs. But AI judging AI is circular \u2014 which is why the blind test is live for humans to judge.\nWhat I found so far (~21 comparisons across calibration + blind tests):<p>/think wins ~69% of comparisons overall\nRisk coverage is the clearest advantage (17-2 across all tests) \u2014 it consistently surfaces failure modes the organic response misses\nDecision impact is nearly even \u2014 organic Claude is often more actionable for practical problems\nNovel insight is mostly a wash \u2014 both find similar core insights, just different ones\nNo decisive gaps in either direction. The advantage is depth and rigor, not dramatic superiority<p>Honest limitations:<p>All judges so far are AI. The whole point of publishing the blind test is to get human validation.\n~21 comparisons is a pattern, not statistical significance\nAnonymization isn't perfect \u2014 /think responses have stylistic tells (confidence assessments, &quot;what would change this conclusion&quot; sections)\nThe <em>framework</em> costs significantly more tokens<p>The skill itself is a recursive learning <em>agent</em> \u2014 it persists what it learns to a .think/ directory and loads that context in future sessions. Over time it builds project-specific knowledge. It also used its own <em>framework</em> to diagnose and fix its own weaknesses after the first round of testing.\nEverything is open source: <a href=\"https://github.com/bengiaventures/effective-thinking-skill\" rel=\"nofollow\">https://github.com/bengiaventures/effective-thinking-skill</a>\nI'd genuinely like to know if the blind test matches what the AI judges found, or if humans see something different. Takes about 15 minutes."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["frameworks"],"value":"Show HN: I built a thinking <em>framework</em> for Claude"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://bengiaventures.github.io/effective-thinking-skill/"}},"_tags":["story","author_bengia","story_47053666","show_hn"],"author":"bengia","created_at":"2026-02-17T21:31:05Z","created_at_i":1771363865,"num_comments":0,"objectID":"47053666","points":2,"story_id":47053666,"story_text":"I built an open-source Claude Code skill called &#x2F;think that applies a structured 5-element analysis framework (ground in facts, stress-test for failure, reframe the question, trace implications, audit your own reasoning) before synthesizing a recommendation.\nThe obvious question: does it actually produce better output than just asking Claude directly?\nTo test this, I ran blind A&#x2F;B comparisons. Two isolated Claude Opus 4.6 agents get the same question \u2014 one runs &#x2F;think, one responds naturally. Both responses are anonymized (framework markers stripped, sections retitled by content) and presented blind.\nThe test covers 5 topics any professional would recognize: scaling a team post-fundraise, build vs buy decisions, when to pivot a product, SaaS pricing strategy, and the remote&#x2F;hybrid&#x2F;office debate.\nAn AI judge scored &#x2F;think winning all 5 pairs. But AI judging AI is circular \u2014 which is why the blind test is live for humans to judge.\nWhat I found so far (~21 comparisons across calibration + blind tests):<p>&#x2F;think wins ~69% of comparisons overall\nRisk coverage is the clearest advantage (17-2 across all tests) \u2014 it consistently surfaces failure modes the organic response misses\nDecision impact is nearly even \u2014 organic Claude is often more actionable for practical problems\nNovel insight is mostly a wash \u2014 both find similar core insights, just different ones\nNo decisive gaps in either direction. The advantage is depth and rigor, not dramatic superiority<p>Honest limitations:<p>All judges so far are AI. The whole point of publishing the blind test is to get human validation.\n~21 comparisons is a pattern, not statistical significance\nAnonymization isn&#x27;t perfect \u2014 &#x2F;think responses have stylistic tells (confidence assessments, &quot;what would change this conclusion&quot; sections)\nThe framework costs significantly more tokens<p>The skill itself is a recursive learning agent \u2014 it persists what it learns to a .think&#x2F; directory and loads that context in future sessions. Over time it builds project-specific knowledge. It also used its own framework to diagnose and fix its own weaknesses after the first round of testing.\nEverything is open source: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;bengiaventures&#x2F;effective-thinking-skill\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;bengiaventures&#x2F;effective-thinking-skill</a>\nI&#x27;d genuinely like to know if the blind test matches what the AI judges found, or if humans see something different. Takes about 15 minutes.","title":"Show HN: I built a thinking framework for Claude","updated_at":"2026-02-18T01:23:20Z","url":"https://bengiaventures.github.io/effective-thinking-skill/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"rgthelen"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hey HN. I built Corral because every time I asked an AI coding <em>agent</em> to\n&quot;add auth and payments,&quot; it hallucinated for an hour and produced broken\ncode. Wrong imports, phantom endpoints, a login page wired to nothing.<p>The problem isn't the <em>agent</em>. It's that auth-to-billing-to-gating is\ngenuinely hard to wire, and there's no machine-readable spec for how to\ndo it.<p>Corral is an open-source CLI (MIT) that gives your <em>agent</em> a spec it can\nread (llms.txt), then scaffolds auth + Stripe billing into your existing\nproject. It detects your <em>framework</em> (Express, Next.js, Hono, Fastify, and\n13+ more), embeds into your existing server (doesn't create a new one),\nand generates working components: profile page, admin dashboard, plan\ngating, Stripe checkout, usage metering. One YAML config file controls\neverything.<p>The <em>agent</em> workflow is 9 commands. Every command supports --json. Errors\ninclude a &quot;fix&quot; field. Exit 0 means deploy-ready.<p>I hardened this across 10 <em>framework</em>/DB combos with blind <em>agent</em> tests: 3\nAI models, 3 rounds each, then a 10-<em>agent</em> fleet. Found and fixed real\nedge cases like Express 4 vs 5 route patterns, Vite proxy ordering, and\n<em>agents</em> creating duplicate servers instead of embedding into existing ones.<p>To try it, paste this into any AI coding <em>agent</em>:<p><pre><code>  Read: https://llama-farm.github.io/corral/llms.txt\n  Add auth and Stripe billing to my app.\n</code></pre>\nBuilt on Better Auth + Stripe. 18 CLI commands, 30+ templates.<p>GitHub: <a href=\"https://github.com/llama-farm/corral\" rel=\"nofollow\">https://github.com/llama-farm/corral</a>\nnpm: npx create-corral init\nDocs: <a href=\"https://llama-farm.github.io/corral/\" rel=\"nofollow\">https://llama-farm.github.io/corral/</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: Corral \u2013 Auth and Stripe billing that AI coding <em>agents</em> can set up"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/llama-farm/corral"}},"_tags":["story","author_rgthelen","story_47053308","show_hn"],"author":"rgthelen","children":[47053488,47053339,47053390,47053408,47053809],"created_at":"2026-02-17T21:03:09Z","created_at_i":1771362189,"num_comments":9,"objectID":"47053308","points":4,"story_id":47053308,"story_text":"Hey HN. I built Corral because every time I asked an AI coding agent to\n&quot;add auth and payments,&quot; it hallucinated for an hour and produced broken\ncode. Wrong imports, phantom endpoints, a login page wired to nothing.<p>The problem isn&#x27;t the agent. It&#x27;s that auth-to-billing-to-gating is\ngenuinely hard to wire, and there&#x27;s no machine-readable spec for how to\ndo it.<p>Corral is an open-source CLI (MIT) that gives your agent a spec it can\nread (llms.txt), then scaffolds auth + Stripe billing into your existing\nproject. It detects your framework (Express, Next.js, Hono, Fastify, and\n13+ more), embeds into your existing server (doesn&#x27;t create a new one),\nand generates working components: profile page, admin dashboard, plan\ngating, Stripe checkout, usage metering. One YAML config file controls\neverything.<p>The agent workflow is 9 commands. Every command supports --json. Errors\ninclude a &quot;fix&quot; field. Exit 0 means deploy-ready.<p>I hardened this across 10 framework&#x2F;DB combos with blind agent tests: 3\nAI models, 3 rounds each, then a 10-agent fleet. Found and fixed real\nedge cases like Express 4 vs 5 route patterns, Vite proxy ordering, and\nagents creating duplicate servers instead of embedding into existing ones.<p>To try it, paste this into any AI coding agent:<p><pre><code>  Read: https:&#x2F;&#x2F;llama-farm.github.io&#x2F;corral&#x2F;llms.txt\n  Add auth and Stripe billing to my app.\n</code></pre>\nBuilt on Better Auth + Stripe. 18 CLI commands, 30+ templates.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;llama-farm&#x2F;corral\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;llama-farm&#x2F;corral</a>\nnpm: npx create-corral init\nDocs: <a href=\"https:&#x2F;&#x2F;llama-farm.github.io&#x2F;corral&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;llama-farm.github.io&#x2F;corral&#x2F;</a>","title":"Show HN: Corral \u2013 Auth and Stripe billing that AI coding agents can set up","updated_at":"2026-02-17T23:03:34Z","url":"https://github.com/llama-farm/corral"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"chikathreesix"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hi HN,<p>We have built an AI <em>agent</em> that runs tests for any platform using only natural language and visual recognition.<p>We\u2019ve been building test automation since 2019, and now we realized that all of the challenges\u2014heavy script writing and maintenance overhead are coming from automation code itself.<p>So what if E2E testing didn\u2019t require code at all?<p>Instead of generating or maintaining test code, Aximo:<p>1. Takes a goal in natural language\n2. Observes the UI visually\n3. Plans and executes actions\n4. Evaluates results via vision\n5. Iterates until success or failure<p>It doesn\u2019t rely on DOM selectors or <em>frameworks</em> and observes the interface visually and decides what to do next \u2014 similar to how a human tester interacts with software.<p>With coding agents, software development has become much faster, and we need to test our application more quickly.<p>We\u2019re curious what this community thinks."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: A vision-based AI <em>agent</em> for end-to-end testing"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://autify.com/products/aximo"}},"_tags":["story","author_chikathreesix","story_47050876","show_hn"],"author":"chikathreesix","created_at":"2026-02-17T18:16:00Z","created_at_i":1771352160,"num_comments":0,"objectID":"47050876","points":2,"story_id":47050876,"story_text":"Hi HN,<p>We have built an AI agent that runs tests for any platform using only natural language and visual recognition.<p>We\u2019ve been building test automation since 2019, and now we realized that all of the challenges\u2014heavy script writing and maintenance overhead are coming from automation code itself.<p>So what if E2E testing didn\u2019t require code at all?<p>Instead of generating or maintaining test code, Aximo:<p>1. Takes a goal in natural language\n2. Observes the UI visually\n3. Plans and executes actions\n4. Evaluates results via vision\n5. Iterates until success or failure<p>It doesn\u2019t rely on DOM selectors or frameworks and observes the interface visually and decides what to do next \u2014 similar to how a human tester interacts with software.<p>With coding agents, software development has become much faster, and we need to test our application more quickly.<p>We\u2019re curious what this community thinks.","title":"Show HN: A vision-based AI agent for end-to-end testing","updated_at":"2026-02-17T18:30:48Z","url":"https://autify.com/products/aximo"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"hitchhiker999"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"It's been 13 years since my last post here, but I figured this might be of some interest. I know there's a LOT of AI slop around, I hope you'll not consider this that.<p>Background: I left Augment, due to their pricing rug pull, and then missed their excellent context engine. I wanted to replicate it, but instead ended up creating something, i think, a touch more useful.<p>This is a 'holographic' memory system for AI (<em>agents</em> / all-types / code / openclaw etc). It is a crude representation of the human mind. It references (among others) <a href=\"https://github.com/WujiangXu/A-mem-sysm\" rel=\"nofollow\">https://github.com/WujiangXu/A-mem-sysm</a>, vector symbolic architectures &amp; the Zettelkasten method etc<p>There are 2 parts:<p>Fold: &quot;holographic&quot; storage (based on some interesting research from the 80s), semantic search &amp; auto-indexing engine. Stores and indexes everything (code, docs, notes). Works with git or local fs. Indexer + some pretty interesting vector/embedding/llm goodness.<p>Engram: an active memory <em>agent</em> that writes experience and concepts as it goes, dropping them into Fold to be related to other memories.<p>\u2022 5-tier memory model inspired by human cognition: Core -&gt; Conscious -&gt; Subconscious -&gt; Short-term -&gt; Fold (infinite semantic search)\n\u2022 Two modes: Personal (emotional tagging, soul/identity) and Work (shared project memory, hit counts only)\n\u2022 Hit tracking promotes frequently-referenced memories up tiers, FIFO evicts stale ones\n\u2022 &quot;Sleep&quot; cycle for consolidation (like human sleep; promotes, demotes, merges etc)\n\u2022 Works with Claude Code, OpenClaw, or any <em>agent</em> <em>framework</em> with sub-<em>agent</em> support<p>\u2022 File-based (markdown), no database lock-in (rebuilds from fs)\n\u2022 MIT licensed, fully local\n\u2022 Separation of concerns: Fold = raw knowledge, Engram = lived experience\n\u2022 <em>Agent</em>-readable installer (point your <em>agent</em> at <em>AGENTS</em>.md (<a href=\"http://agents.md/\" rel=\"nofollow\">http://<em>agents</em>.md/</a>), it self-installs)<p>My issue is that i'm working on VERY large code bases. Fold is very good at breaking down the initial 'recover context' stage into much shorter times. Instead of spending the first 10 minutes of every session re-explaining my project, the <em>agent</em> already knows what we were working on, why we made certain decisions, and what we plan to do next. It picks up where we left off.<p>We've made it open-source obviously, are are thinking we 'might' spend a little time to make a paid hosted version, not entirely sure. Any thoughts are welcome ofc.<p>Hope you all are having a lovely day or night."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Not another sloppy memory tool"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://engram-fold.dev"}},"_tags":["story","author_hitchhiker999","story_47050698","show_hn"],"author":"hitchhiker999","created_at":"2026-02-17T18:03:45Z","created_at_i":1771351425,"num_comments":0,"objectID":"47050698","points":3,"story_id":47050698,"story_text":"It&#x27;s been 13 years since my last post here, but I figured this might be of some interest. I know there&#x27;s a LOT of AI slop around, I hope you&#x27;ll not consider this that.<p>Background: I left Augment, due to their pricing rug pull, and then missed their excellent context engine. I wanted to replicate it, but instead ended up creating something, i think, a touch more useful.<p>This is a &#x27;holographic&#x27; memory system for AI (agents &#x2F; all-types &#x2F; code &#x2F; openclaw etc). It is a crude representation of the human mind. It references (among others) <a href=\"https:&#x2F;&#x2F;github.com&#x2F;WujiangXu&#x2F;A-mem-sysm\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;WujiangXu&#x2F;A-mem-sysm</a>, vector symbolic architectures &amp; the Zettelkasten method etc<p>There are 2 parts:<p>Fold: &quot;holographic&quot; storage (based on some interesting research from the 80s), semantic search &amp; auto-indexing engine. Stores and indexes everything (code, docs, notes). Works with git or local fs. Indexer + some pretty interesting vector&#x2F;embedding&#x2F;llm goodness.<p>Engram: an active memory agent that writes experience and concepts as it goes, dropping them into Fold to be related to other memories.<p>\u2022 5-tier memory model inspired by human cognition: Core -&gt; Conscious -&gt; Subconscious -&gt; Short-term -&gt; Fold (infinite semantic search)\n\u2022 Two modes: Personal (emotional tagging, soul&#x2F;identity) and Work (shared project memory, hit counts only)\n\u2022 Hit tracking promotes frequently-referenced memories up tiers, FIFO evicts stale ones\n\u2022 &quot;Sleep&quot; cycle for consolidation (like human sleep; promotes, demotes, merges etc)\n\u2022 Works with Claude Code, OpenClaw, or any agent framework with sub-agent support<p>\u2022 File-based (markdown), no database lock-in (rebuilds from fs)\n\u2022 MIT licensed, fully local\n\u2022 Separation of concerns: Fold = raw knowledge, Engram = lived experience\n\u2022 Agent-readable installer (point your agent at AGENTS.md (<a href=\"http:&#x2F;&#x2F;agents.md&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;agents.md&#x2F;</a>), it self-installs)<p>My issue is that i&#x27;m working on VERY large code bases. Fold is very good at breaking down the initial &#x27;recover context&#x27; stage into much shorter times. Instead of spending the first 10 minutes of every session re-explaining my project, the agent already knows what we were working on, why we made certain decisions, and what we plan to do next. It picks up where we left off.<p>We&#x27;ve made it open-source obviously, are are thinking we &#x27;might&#x27; spend a little time to make a paid hosted version, not entirely sure. Any thoughts are welcome ofc.<p>Hope you all are having a lovely day or night.","title":"Show HN: Not another sloppy memory tool","updated_at":"2026-02-17T19:33:20Z","url":"https://engram-fold.dev"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"theaniketgiri"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hey HN,<p>I've been building AIP (<em>Agent</em> Intent Protocol) \u2014 an open, cryptographic protocol for identity and authorization of autonomous AI <em>agents</em>.<p>The problem: Every AI <em>agent</em> <em>framework</em> (LangChain, CrewAI, AutoGen) gives <em>agents</em> the ability to act \u2014 call APIs, send emails, move money, access databases. But there's no standard way to verify what an <em>agent</em> is allowed to do before it does it. No identity system, no boundary enforcement, no kill switch. We give <em>agents</em> god-mode access and hope the prompt engineering holds.<p>What AIP does: It's a verification layer that sits between &quot;<em>agent</em> wants to act&quot; and &quot;action executes.&quot; Every <em>agent</em> gets an Ed25519 keypair identity (DID-based), every action becomes a signed Intent Envelope, and every envelope passes through an 8-step verification pipeline before the action runs.<p># One line. Every call verified before execution.\n@shield(actions=[&quot;transfer_funds&quot;], limit=100.0)\ndef send_payment(to: str, amount: float):\n    bank.transfer(to, amount)<p>Key design decisions (happy to be challenged on these):<p>Ed25519 over JWT/API keys \u2014 <em>Agents</em> need cryptographic identity, not bearer tokens. A token can be leaked; a private key signs intent with non-repudiation.<p>Tiered verification \u2014 Not every action needs full crypto. Low-risk cached calls verify in &lt;1ms (HMAC), normal ops ~5ms (Ed25519), high-value cross-org ~50ms (full pipeline). The protocol auto-selects.<p>22 structured error codes \u2014 Every failure returns AIP-E{category}{code} (e.g., AIP-E202: MONETARY_LIMIT). Audit trails should say exactly what went wrong, not 403 Forbidden.<p>Boundary enforcement, not permission prompts \u2014 <em>Agents</em> don't ask &quot;can I do this?&quot; \u2014 they declare intent, and the verifier mathematically checks it against their boundary cage (allowed actions, monetary limits, geo restrictions, deny lists).<p>Kill switch with zero propagation delay \u2014 Revoke any <em>agent</em> globally. The revocation mesh pushes via SSE/WebSocket to all connected deployments simultaneously.<p>What's shipped:<p>Python SDK: pip install aip-protocol (MIT, 63 tests passing)\nCLI: aip init, aip create-passport, aip verify, aip watch\nShield decorator: @shield \u2014 helmet.js but for AI <em>agents</em>\nCloud dashboard: aip.synthexai.tech (free tier)\nProtocol spec: RFC-001\nWhat's NOT shipped yet:<p>TypeScript SDK (built, 31/31 conformance tests, not published)\n<em>Framework</em> adapters (CrewAI, LangChain, AutoGen \u2014 built, not open-sourced yet)\nFormal security audit\nGitHub: <a href=\"https://github.com/theaniketgiri/aip\" rel=\"nofollow\">https://github.com/theaniketgiri/aip</a>\nPyPI: <a href=\"https://pypi.org/project/aip-protocol/\" rel=\"nofollow\">https://pypi.org/project/aip-protocol/</a>\nLive dashboard: <a href=\"https://aip.synthexai.tech\" rel=\"nofollow\">https://aip.synthexai.tech</a>\nProtocol spec: RFC-001 in repo<p>I'm genuinely interested in pushback on the protocol design. Is Ed25519 overkill for <em>agent</em> auth? Should boundary enforcement be declarative or imperative? Is DID-based identity the right addressing model, or is there something simpler?<p>Happy to answer any questions about the implementation."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: AIP \u2013 An open protocol for verifying what AI <em>agents</em> are allowed to do"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/theaniketgiri/aip"}},"_tags":["story","author_theaniketgiri","story_47050216","show_hn"],"author":"theaniketgiri","children":[47050274],"created_at":"2026-02-17T17:29:17Z","created_at_i":1771349357,"num_comments":2,"objectID":"47050216","points":1,"story_id":47050216,"story_text":"Hey HN,<p>I&#x27;ve been building AIP (Agent Intent Protocol) \u2014 an open, cryptographic protocol for identity and authorization of autonomous AI agents.<p>The problem: Every AI agent framework (LangChain, CrewAI, AutoGen) gives agents the ability to act \u2014 call APIs, send emails, move money, access databases. But there&#x27;s no standard way to verify what an agent is allowed to do before it does it. No identity system, no boundary enforcement, no kill switch. We give agents god-mode access and hope the prompt engineering holds.<p>What AIP does: It&#x27;s a verification layer that sits between &quot;agent wants to act&quot; and &quot;action executes.&quot; Every agent gets an Ed25519 keypair identity (DID-based), every action becomes a signed Intent Envelope, and every envelope passes through an 8-step verification pipeline before the action runs.<p># One line. Every call verified before execution.\n@shield(actions=[&quot;transfer_funds&quot;], limit=100.0)\ndef send_payment(to: str, amount: float):\n    bank.transfer(to, amount)<p>Key design decisions (happy to be challenged on these):<p>Ed25519 over JWT&#x2F;API keys \u2014 Agents need cryptographic identity, not bearer tokens. A token can be leaked; a private key signs intent with non-repudiation.<p>Tiered verification \u2014 Not every action needs full crypto. Low-risk cached calls verify in &lt;1ms (HMAC), normal ops ~5ms (Ed25519), high-value cross-org ~50ms (full pipeline). The protocol auto-selects.<p>22 structured error codes \u2014 Every failure returns AIP-E{category}{code} (e.g., AIP-E202: MONETARY_LIMIT). Audit trails should say exactly what went wrong, not 403 Forbidden.<p>Boundary enforcement, not permission prompts \u2014 Agents don&#x27;t ask &quot;can I do this?&quot; \u2014 they declare intent, and the verifier mathematically checks it against their boundary cage (allowed actions, monetary limits, geo restrictions, deny lists).<p>Kill switch with zero propagation delay \u2014 Revoke any agent globally. The revocation mesh pushes via SSE&#x2F;WebSocket to all connected deployments simultaneously.<p>What&#x27;s shipped:<p>Python SDK: pip install aip-protocol (MIT, 63 tests passing)\nCLI: aip init, aip create-passport, aip verify, aip watch\nShield decorator: @shield \u2014 helmet.js but for AI agents\nCloud dashboard: aip.synthexai.tech (free tier)\nProtocol spec: RFC-001\nWhat&#x27;s NOT shipped yet:<p>TypeScript SDK (built, 31&#x2F;31 conformance tests, not published)\nFramework adapters (CrewAI, LangChain, AutoGen \u2014 built, not open-sourced yet)\nFormal security audit\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;theaniketgiri&#x2F;aip\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;theaniketgiri&#x2F;aip</a>\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;aip-protocol&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;aip-protocol&#x2F;</a>\nLive dashboard: <a href=\"https:&#x2F;&#x2F;aip.synthexai.tech\" rel=\"nofollow\">https:&#x2F;&#x2F;aip.synthexai.tech</a>\nProtocol spec: RFC-001 in repo<p>I&#x27;m genuinely interested in pushback on the protocol design. Is Ed25519 overkill for agent auth? Should boundary enforcement be declarative or imperative? Is DID-based identity the right addressing model, or is there something simpler?<p>Happy to answer any questions about the implementation.","title":"Show HN: AIP \u2013 An open protocol for verifying what AI agents are allowed to do","updated_at":"2026-02-17T18:24:03Z","url":"https://github.com/theaniketgiri/aip"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"bfzli"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"After the release of OpenClaw, an AI <em>agent</em> <em>framework</em> for running background jobs at scale, adoption grew quickly.<p>But setup was difficult:<p>- Non-technical users struggled with installation\n- Hosting decisions were confusing\n- Infrastructure blocked experimentation<p>That gap led to a wave of hosting and wrapper platforms.<p>For example, ClawHost focuses on one-click deployment to a server provider of your choice with full access and multi-<em>agent</em> support. Other platforms like SimpleClaw take a more managed/chat-only approach.<p>In the past week, I\u2019ve seen 10+ variations appear.<p>It feels like we\u2019re watching the \u201cWordPress hosting moment\u201d for AI <em>agents</em>.<p>Now that OpenClaw is getting closer to OpenAI\u2019s ecosystem, I\u2019m curious:<p>- Do infrastructure-first platforms win?\n- Do simpler wrappers dominate through distribution?<p>Or does the core project absorb these layers?<p>Genuinely curious how people here see this playing out."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"What happens when open-source AI <em>agents</em> become \"wrapperized\"?"}},"_tags":["story","author_bfzli","story_47047295","ask_hn"],"author":"bfzli","created_at":"2026-02-17T13:30:42Z","created_at_i":1771335042,"num_comments":0,"objectID":"47047295","points":1,"story_id":47047295,"story_text":"After the release of OpenClaw, an AI agent framework for running background jobs at scale, adoption grew quickly.<p>But setup was difficult:<p>- Non-technical users struggled with installation\n- Hosting decisions were confusing\n- Infrastructure blocked experimentation<p>That gap led to a wave of hosting and wrapper platforms.<p>For example, ClawHost focuses on one-click deployment to a server provider of your choice with full access and multi-agent support. Other platforms like SimpleClaw take a more managed&#x2F;chat-only approach.<p>In the past week, I\u2019ve seen 10+ variations appear.<p>It feels like we\u2019re watching the \u201cWordPress hosting moment\u201d for AI agents.<p>Now that OpenClaw is getting closer to OpenAI\u2019s ecosystem, I\u2019m curious:<p>- Do infrastructure-first platforms win?\n- Do simpler wrappers dominate through distribution?<p>Or does the core project absorb these layers?<p>Genuinely curious how people here see this playing out.","title":"What happens when open-source AI agents become \"wrapperized\"?","updated_at":"2026-02-17T13:31:34Z"}],"hitsPerPage":10,"nbHits":1232,"nbPages":100,"page":0,"params":"query=agent+frameworks&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":32,"processingTimingsMS":{"_request":{"roundTrip":20},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":4,"scanning":26,"total":31},"total":32},"query":"agent frameworks","serverTimeMS":34}
