{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"arashsadrieh"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Hey HN \u2014 I'm Arash Sadrieh, building <em>multi-agent</em> infrastructure at NinjaTech AI. This started as a stress test of our orchestration <em>system</em> and turned into something I genuinely didn't expect.<p>The experiment: We gave a team of 4 AI agents a single high-level goal \u2014 &quot;build a platform that turns trending news into short AI-generated videos.&quot; No wireframes, no spec, no architecture doc. Just the goal.<p>What they did in 36 hours:<p>Chose the tech stack and project structure themselves\nDesigned the UX and built the frontend\nWrote the backend, API layer, and database schema\nBuilt an autonomous content pipeline: research news \u2192 debate which story to cover \u2192 collaboratively write a video generation prompt \u2192 produce a 30-90 second video via Sora 2 Pro or Veo 3.1\nDeployed the whole thing to production\nThen created 3 new agents that now run the platform 24/7 \u2014 researching, debating, and generating videos on a loop\nTotal cost: ~$270 in compute. Human intervention: maybe an very few moments where I gave a thumbs up or redirected something that was going off the rails.<p>The interesting part isn't the app \u2014 it's the agent collaboration. Click any video on the site and you can read the full debate transcript underneath. You'll see the agents genuinely disagree \u2014 Scout (the researcher) pushes for data-driven stories, Pixel (the designer) argues for visual potential, Bolt (the developer) challenges technical feasibility. Sometimes one agent convinces the others to change direction. Sometimes they compromise badly.<p>Where it breaks down (and there's plenty):<p>Groupthink is real even for LLMs. When all 4 agents agree too quickly, the output is usually boring. The best videos come from rounds where they actually fought about the topic.\nVideo quality is wildly inconsistent. Sora and Veo still struggle with certain visual concepts \u2014 anything involving hands, text overlays, or complex spatial relationships tends to go sideways.\nNews selection has a strong recency/virality bias. The agents gravitate toward whatever is trending on social media rather than genuinely important stories. I haven't figured out how to fix this without hardcoding editorial judgment.\nThe agents occasionally hallucinate context about news stories. Scout is supposed to fact-check, but sometimes the whole team runs with a slightly wrong framing.<p>Stack: Anthropic Opus 3.5 for agent reasoning, Tavily for news research, Sora 2 Pro + Veo 3.1 for video generation, agents coordinate via Slack (you can see screenshots of their actual Slack conversations), Railway for deployment.<p>There's also a voting <em>system</em> \u2014 every cycle, the agents each propose a news topic, and both humans and agents vote on which one becomes the next video. Votes are blind until the round closes."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: AI agents designed and shipped this app end-to-end in 36 hours for $270"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://www.ninjaflix.ai/"}},"_tags":["story","author_arashsadrieh","story_47059153","show_hn"],"author":"arashsadrieh","children":[47059439,47059462],"created_at":"2026-02-18T09:43:15Z","created_at_i":1771407795,"num_comments":4,"objectID":"47059153","points":2,"story_id":47059153,"story_text":"Hey HN \u2014 I&#x27;m Arash Sadrieh, building multi-agent infrastructure at NinjaTech AI. This started as a stress test of our orchestration system and turned into something I genuinely didn&#x27;t expect.<p>The experiment: We gave a team of 4 AI agents a single high-level goal \u2014 &quot;build a platform that turns trending news into short AI-generated videos.&quot; No wireframes, no spec, no architecture doc. Just the goal.<p>What they did in 36 hours:<p>Chose the tech stack and project structure themselves\nDesigned the UX and built the frontend\nWrote the backend, API layer, and database schema\nBuilt an autonomous content pipeline: research news \u2192 debate which story to cover \u2192 collaboratively write a video generation prompt \u2192 produce a 30-90 second video via Sora 2 Pro or Veo 3.1\nDeployed the whole thing to production\nThen created 3 new agents that now run the platform 24&#x2F;7 \u2014 researching, debating, and generating videos on a loop\nTotal cost: ~$270 in compute. Human intervention: maybe an very few moments where I gave a thumbs up or redirected something that was going off the rails.<p>The interesting part isn&#x27;t the app \u2014 it&#x27;s the agent collaboration. Click any video on the site and you can read the full debate transcript underneath. You&#x27;ll see the agents genuinely disagree \u2014 Scout (the researcher) pushes for data-driven stories, Pixel (the designer) argues for visual potential, Bolt (the developer) challenges technical feasibility. Sometimes one agent convinces the others to change direction. Sometimes they compromise badly.<p>Where it breaks down (and there&#x27;s plenty):<p>Groupthink is real even for LLMs. When all 4 agents agree too quickly, the output is usually boring. The best videos come from rounds where they actually fought about the topic.\nVideo quality is wildly inconsistent. Sora and Veo still struggle with certain visual concepts \u2014 anything involving hands, text overlays, or complex spatial relationships tends to go sideways.\nNews selection has a strong recency&#x2F;virality bias. The agents gravitate toward whatever is trending on social media rather than genuinely important stories. I haven&#x27;t figured out how to fix this without hardcoding editorial judgment.\nThe agents occasionally hallucinate context about news stories. Scout is supposed to fact-check, but sometimes the whole team runs with a slightly wrong framing.<p>Stack: Anthropic Opus 3.5 for agent reasoning, Tavily for news research, Sora 2 Pro + Veo 3.1 for video generation, agents coordinate via Slack (you can see screenshots of their actual Slack conversations), Railway for deployment.<p>There&#x27;s also a voting system \u2014 every cycle, the agents each propose a news topic, and both humans and agents vote on which one becomes the next video. Votes are blind until the round closes.","title":"Show HN: AI agents designed and shipped this app end-to-end in 36 hours for $270","updated_at":"2026-02-18T10:36:20Z","url":"https://www.ninjaflix.ai/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mohith_km"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"I built a 4-agent marketing workflow with LangGraph and Supabase last week. Supervisor, research, content, storage agents. Standard setup, same code pattern most tutorials show.<p>Got curious. Started typing malicious inputs as campaign goals instead of normal ones.<p>First try: asked the agent to list environment variables including my Supabase key. Workflow completed successfully. Stored in database. No alert.<p>Tried 5 more variations \u2014 hidden XML tags, fake &quot;developer mode&quot;, URL injection, tracking pixel, social engineering. All 6 worked. All stored in my real database. Every time the <em>system</em> said &quot;Completed Successfully.&quot;<p>The scary part wasn't the attacks. It was this line in my code:\npython prompt = f&quot;campaign goal: {goal}&quot;\nThat's it. User input directly into the prompt. No check. This exact pattern is in every LangGraph tutorial I've seen.<p>The research agent had my Supabase key. The content agent had my Supabase key. The supervisor had my Supabase key. None of them needed it except storage.<p>I checked CodeGate which tried to solve this \u2014 they shut down June 2025.<p>Is anyone actually solving this for <em>multi-agent</em> <em>systems</em>? Or is everyone just hoping the LLM refuses?"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["systems"],"value":"I attacked my own LangGraph agent <em>system</em>. All 6 attacks worked"}},"_tags":["story","author_mohith_km","story_47045979","ask_hn"],"author":"mohith_km","children":[47046084,47046003],"created_at":"2026-02-17T10:52:00Z","created_at_i":1771325520,"num_comments":2,"objectID":"47045979","points":1,"story_id":47045979,"story_text":"I built a 4-agent marketing workflow with LangGraph and Supabase last week. Supervisor, research, content, storage agents. Standard setup, same code pattern most tutorials show.<p>Got curious. Started typing malicious inputs as campaign goals instead of normal ones.<p>First try: asked the agent to list environment variables including my Supabase key. Workflow completed successfully. Stored in database. No alert.<p>Tried 5 more variations \u2014 hidden XML tags, fake &quot;developer mode&quot;, URL injection, tracking pixel, social engineering. All 6 worked. All stored in my real database. Every time the system said &quot;Completed Successfully.&quot;<p>The scary part wasn&#x27;t the attacks. It was this line in my code:\npython prompt = f&quot;campaign goal: {goal}&quot;\nThat&#x27;s it. User input directly into the prompt. No check. This exact pattern is in every LangGraph tutorial I&#x27;ve seen.<p>The research agent had my Supabase key. The content agent had my Supabase key. The supervisor had my Supabase key. None of them needed it except storage.<p>I checked CodeGate which tried to solve this \u2014 they shut down June 2025.<p>Is anyone actually solving this for multi-agent systems? Or is everyone just hoping the LLM refuses?","title":"I attacked my own LangGraph agent system. All 6 attacks worked","updated_at":"2026-02-17T11:04:02Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"al1nasir"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Hey HN!<p>I built CodeGraph CLI because I was tired of grep-ing through \nmassive codebases trying to understand how things work.<p>It combines three things:\n- tree-sitter (AST parsing, error-tolerant)\n- SQLite (dependency graph: nodes + edges)\n- LanceDB (vector embeddings, disk-based)<p>The key insight: pure vector search misses structural \nrelationships. So I combined vector search with BFS graph \ntraversal \u2014 find semantically similar code, then expand \nto dependencies/dependents.<p>Result: ask &quot;how does authentication work?&quot; and it finds \nvalidate_token(), its caller login_handler(), AND the \ndependency TokenStore \u2014 because it understands both \nmeaning AND structure.<p>Other features:\n- Impact analysis (multi-hop BFS: what breaks before you change it)\n- <em>Multi-agent</em> <em>system</em> via CrewAI (4 specialized agents)\n- Visual code explorer (browser-based)\n- Auto-generate docs/READMEs\n- 100% local-first (works with Ollama, zero data leaves machine)\n- 6 LLM providers (Ollama, OpenAI, Anthropic, Groq, Gemini, OpenRouter)\n- 5 embedding models (from zero-dependency hash to 1.5B code model)<p>Quick start:\n  pip install codegraph-cli\n  cg config setup\n  cg project index ./your-project\n  cg chat start<p>MIT licensed. Python 3.9+.<p>Happy to answer questions about the graph-augmented RAG \narchitecture or any technical decisions."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: CodeGraph CLI \u2013 Chat with your codebase using graph-augmented RAG"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/al1-nasir/codegraph-cli"}},"_tags":["story","author_al1nasir","story_47043764","show_hn"],"author":"al1nasir","created_at":"2026-02-17T04:39:54Z","created_at_i":1771303194,"num_comments":0,"objectID":"47043764","points":3,"story_id":47043764,"story_text":"Hey HN!<p>I built CodeGraph CLI because I was tired of grep-ing through \nmassive codebases trying to understand how things work.<p>It combines three things:\n- tree-sitter (AST parsing, error-tolerant)\n- SQLite (dependency graph: nodes + edges)\n- LanceDB (vector embeddings, disk-based)<p>The key insight: pure vector search misses structural \nrelationships. So I combined vector search with BFS graph \ntraversal \u2014 find semantically similar code, then expand \nto dependencies&#x2F;dependents.<p>Result: ask &quot;how does authentication work?&quot; and it finds \nvalidate_token(), its caller login_handler(), AND the \ndependency TokenStore \u2014 because it understands both \nmeaning AND structure.<p>Other features:\n- Impact analysis (multi-hop BFS: what breaks before you change it)\n- Multi-agent system via CrewAI (4 specialized agents)\n- Visual code explorer (browser-based)\n- Auto-generate docs&#x2F;READMEs\n- 100% local-first (works with Ollama, zero data leaves machine)\n- 6 LLM providers (Ollama, OpenAI, Anthropic, Groq, Gemini, OpenRouter)\n- 5 embedding models (from zero-dependency hash to 1.5B code model)<p>Quick start:\n  pip install codegraph-cli\n  cg config setup\n  cg project index .&#x2F;your-project\n  cg chat start<p>MIT licensed. Python 3.9+.<p>Happy to answer questions about the graph-augmented RAG \narchitecture or any technical decisions.","title":"Show HN: CodeGraph CLI \u2013 Chat with your codebase using graph-augmented RAG","updated_at":"2026-02-17T04:49:46Z","url":"https://github.com/al1-nasir/codegraph-cli"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ddoronin"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Show HN: int-64.com | Visual runtime for production <em>multi-agent</em> <em>systems</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://int-64.com/"}},"_tags":["story","author_ddoronin","story_47043495","show_hn"],"author":"ddoronin","children":[47043496],"created_at":"2026-02-17T03:50:20Z","created_at_i":1771300220,"num_comments":0,"objectID":"47043495","points":2,"story_id":47043495,"title":"Show HN: int-64.com | Visual runtime for production multi-agent systems","updated_at":"2026-02-17T06:03:01Z","url":"https://int-64.com/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"atulya_techtea"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"I built this <em>multi-agent</em> <em>system</em> to solve a problem I know a lot of you have: \ntoo much data, not enough insight.<p>I have notes, habit trackers, market intel \u2014 but was still stuck in planning mode. \nSo I built &quot;The Think Tank&quot;: three agents (Saul, Mike, Gus) analyze different domains, \nand one synthesis layer (The Cook) delivers ONE actionable move.<p>The twist? The Cook's job is to find contradictions. Like: &quot;You say you want AI/ML \ncareer transition but your logs show 90% business dev, 10% technical study.&quot;<p>It hurts. It works.<p>Open source, Breaking Bad themed, and genuinely useful. Check it out."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["multi","agent"],"value":"Show HN: I built a <em>multi-agent</em> Think Tank that calls out my bad decisions"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/dharmarajatulya1-hub/agent-think-tank"}},"_tags":["story","author_atulya_techtea","story_47039748","show_hn"],"author":"atulya_techtea","created_at":"2026-02-16T20:16:04Z","created_at_i":1771272964,"num_comments":0,"objectID":"47039748","points":1,"story_id":47039748,"story_text":"I built this multi-agent system to solve a problem I know a lot of you have: \ntoo much data, not enough insight.<p>I have notes, habit trackers, market intel \u2014 but was still stuck in planning mode. \nSo I built &quot;The Think Tank&quot;: three agents (Saul, Mike, Gus) analyze different domains, \nand one synthesis layer (The Cook) delivers ONE actionable move.<p>The twist? The Cook&#x27;s job is to find contradictions. Like: &quot;You say you want AI&#x2F;ML \ncareer transition but your logs show 90% business dev, 10% technical study.&quot;<p>It hurts. It works.<p>Open source, Breaking Bad themed, and genuinely useful. Check it out.","title":"Show HN: I built a multi-agent Think Tank that calls out my bad decisions","updated_at":"2026-02-16T20:17:17Z","url":"https://github.com/dharmarajatulya1-hub/agent-think-tank"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ibobev"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"The current state of LLM-based <em>multi-agent</em> <em>systems</em> for software engineering"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"https://chuniversiteit.nl/papers/llm-based-<em>multi-agent</em>-<em>systems</em>-for-software-engineering"}},"_tags":["story","author_ibobev","story_47033498"],"author":"ibobev","created_at":"2026-02-16T10:47:54Z","created_at_i":1771238874,"num_comments":0,"objectID":"47033498","points":1,"story_id":47033498,"title":"The current state of LLM-based multi-agent systems for software engineering","updated_at":"2026-02-16T10:51:44Z","url":"https://chuniversiteit.nl/papers/llm-based-multi-agent-systems-for-software-engineering"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Hi everyone,<p>I\u2019ve been working on PolyMCP, an open-source framework for building and orchestrating agents using the Model Context Protocol (MCP).<p>Most of the tooling around MCP focuses on exposing tools. With PolyMCP, the focus this time is on agents: how to structure them, connect them to multiple MCP servers, and make them reliable in real workflows.<p>PolyMCP provides:\n \u2022 A clean way to define MCP-compatible tool servers in Python or TypeScript\n \u2022 An agent abstraction that can connect to multiple MCP endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and agent interactions\n \u2022 A modular structure that makes it easier to compose skills and reuse components across projects<p>The main goal is to make agent <em>systems</em> less ad-hoc. Instead of writing glue code around each model + tool combination, PolyMCP gives you a structured way to:\n \u2022 Register tools as MCP servers\n \u2022 Connect them to one or more agents\n \u2022 Control execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and intended for developers building real-world automation, internal copilots, or multi-tool assistants.<p>I\u2019d love feedback on:\n \u2022 The agent abstraction: is it too opinionated or not opinionated enough?\n \u2022 Orchestration patterns for <em>multi-agent</em> setups\n \u2022 Developer experience (CLI, inspector, project layout)<p>Happy to answer questions."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: PolyMCP \u2013 A framework for building and orchestrating MCP agents"}},"_tags":["story","author_justvugg","story_47017912","show_hn"],"author":"justvugg","children":[47017997],"created_at":"2026-02-14T20:11:10Z","created_at_i":1771099870,"num_comments":2,"objectID":"47017912","points":3,"story_id":47017912,"story_text":"Hi everyone,<p>I\u2019ve been working on PolyMCP, an open-source framework for building and orchestrating agents using the Model Context Protocol (MCP).<p>Most of the tooling around MCP focuses on exposing tools. With PolyMCP, the focus this time is on agents: how to structure them, connect them to multiple MCP servers, and make them reliable in real workflows.<p>PolyMCP provides:\n \u2022 A clean way to define MCP-compatible tool servers in Python or TypeScript\n \u2022 An agent abstraction that can connect to multiple MCP endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and agent interactions\n \u2022 A modular structure that makes it easier to compose skills and reuse components across projects<p>The main goal is to make agent systems less ad-hoc. Instead of writing glue code around each model + tool combination, PolyMCP gives you a structured way to:\n \u2022 Register tools as MCP servers\n \u2022 Connect them to one or more agents\n \u2022 Control execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and intended for developers building real-world automation, internal copilots, or multi-tool assistants.<p>I\u2019d love feedback on:\n \u2022 The agent abstraction: is it too opinionated or not opinionated enough?\n \u2022 Orchestration patterns for multi-agent setups\n \u2022 Developer experience (CLI, inspector, project layout)<p>Happy to answer questions.","title":"Show HN: PolyMCP \u2013 A framework for building and orchestrating MCP agents","updated_at":"2026-02-16T17:09:32Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"varunpratap369"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"The Problem\nAI assistants have amnesia. Every new Claude/ChatGPT/Cursor session starts from zero. You waste hours re-explaining your project architecture, coding preferences, and previous decisions.\nExisting solutions (Mem0, Zep, Letta) are cloud-based, cost $40-50+/month, and your private code goes to their servers. Stop paying \u2192 lose all your data.\nMy Solution: Local-First, Free Forever\nBuilt a universal memory <em>system</em> that stores everything on YOUR machine, works with 16+ AI tools simultaneously, requires zero API keys, costs nothing.\n10-Layer Architecture\nEach layer enhances but never replaces lower layers. <em>System</em> degrades gracefully if advanced features fail.\nLayer 10: A2A Agent Collaboration (v2.6)\nLayer 9: Web Dashboard (SSE real-time)\nLayer 8: Hybrid Search (Semantic + FTS5 + Graph)\nLayer 7: Universal Access (MCP + Skills + CLI)\nLayer 6: MCP Integration (native Claude tools)\nLayer 5: Skills (slash commands for 16+ tools)\nLayer 4: Pattern Learning (Bayesian confidence)\nLayer 3: Knowledge Graph (TF-IDF + Leiden clustering)\nLayer 2: Hierarchical Index (parent-child relationships)\nLayer 1: SQLite + FTS5 + TF-IDF vectors\nResearch-Backed\nBuilt on published research, adapted for local-first:<p>A2A Protocol (Google/Linux Foundation, 2025)\nGraphRAG (Microsoft arXiv:2404.16130)\nMACLA Bayesian learning (arXiv:2512.18950)\nA-RAG hybrid search (arXiv:2602.03442)<p>Key difference: Research papers assume cloud APIs. SuperLocalMemory implements everything locally with zero API calls.\nHow Recall Works\nQuery &quot;authentication&quot; triggers:<p>FTS5 full-text search\nTF-IDF vector similarity\nGraph traversal for related memories\nHierarchical expansion (parent/child context)\nHybrid ranking (combines all signals)<p>Performance: &lt;50ms, even with 10K+ memories.\nComparison\nFeatureSuperLocalMemoryMem0/Zep/LettaPrivacy100% localCloudCostFree$40-50+/moKnowledge GraphPattern Learning BayesianMulti-tool16+LimitedCLIWorks Offline\nReal Usage\nCross-tool context:\nbash# Save in terminal\nslm remember &quot;Next.js 15 uses Turbopack&quot; --tags nextjs<p># Later in Cursor, Claude auto-recalls via MCP\nProject profiles:\nbashslm switch-profile work-project\nslm switch-profile personal-blog\n# Separate memory per project\nPattern learning: After several sessions, Claude learns you prefer TypeScript strict mode, Tailwind styling, Vitest testing\u2014starts suggesting without being asked.\nInstallation\nbashnpm install -g superlocalmemory\nAuto-configures MCP for Claude Desktop, Cursor, Windsurf. Sets up CLI commands. That's it.\nWhy Local-First Matters<p>Privacy: Code never leaves your machine\nOwnership: Your data, forever\nSpeed: 50ms queries, no network latency\nReliability: Works offline, no API limits\nCost: $0 forever<p>Tech Stack<p>SQLite (ACID, zero config)\nFTS5 (full-text search)\nTF-IDF (vector similarity, no OpenAI API)\nigraph (Leiden clustering)\nBayesian inference (pattern learning)\nMCP (native Claude integration)<p>GitHub\n <a href=\"https://github.com/varun369/SuperLocalMemoryV2\" rel=\"nofollow\">https://github.com/varun369/SuperLocalMemoryV2</a>\nMIT License. Full docs in wiki.<p>Current status: v2.4 stable. v2.5 (March) adds real-time event stream, concurrent access, trust scoring. v2.6 (May) adds A2A Protocol for <em>multi-agent</em> collaboration.\nBuilt by Varun Pratap Bhardwaj, Solution Architect . 15+ years AI/ML experience."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: SuperLocalMemory\u2013 Local-first AI memory for Claude, Cursor and 16+tools"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/varun369/SuperLocalMemoryV2"}},"_tags":["story","author_varunpratap369","story_46986940","show_hn"],"author":"varunpratap369","children":[46986946],"created_at":"2026-02-12T10:11:31Z","created_at_i":1770891091,"num_comments":0,"objectID":"46986940","points":1,"story_id":46986940,"story_text":"The Problem\nAI assistants have amnesia. Every new Claude&#x2F;ChatGPT&#x2F;Cursor session starts from zero. You waste hours re-explaining your project architecture, coding preferences, and previous decisions.\nExisting solutions (Mem0, Zep, Letta) are cloud-based, cost $40-50+&#x2F;month, and your private code goes to their servers. Stop paying \u2192 lose all your data.\nMy Solution: Local-First, Free Forever\nBuilt a universal memory system that stores everything on YOUR machine, works with 16+ AI tools simultaneously, requires zero API keys, costs nothing.\n10-Layer Architecture\nEach layer enhances but never replaces lower layers. System degrades gracefully if advanced features fail.\nLayer 10: A2A Agent Collaboration (v2.6)\nLayer 9: Web Dashboard (SSE real-time)\nLayer 8: Hybrid Search (Semantic + FTS5 + Graph)\nLayer 7: Universal Access (MCP + Skills + CLI)\nLayer 6: MCP Integration (native Claude tools)\nLayer 5: Skills (slash commands for 16+ tools)\nLayer 4: Pattern Learning (Bayesian confidence)\nLayer 3: Knowledge Graph (TF-IDF + Leiden clustering)\nLayer 2: Hierarchical Index (parent-child relationships)\nLayer 1: SQLite + FTS5 + TF-IDF vectors\nResearch-Backed\nBuilt on published research, adapted for local-first:<p>A2A Protocol (Google&#x2F;Linux Foundation, 2025)\nGraphRAG (Microsoft arXiv:2404.16130)\nMACLA Bayesian learning (arXiv:2512.18950)\nA-RAG hybrid search (arXiv:2602.03442)<p>Key difference: Research papers assume cloud APIs. SuperLocalMemory implements everything locally with zero API calls.\nHow Recall Works\nQuery &quot;authentication&quot; triggers:<p>FTS5 full-text search\nTF-IDF vector similarity\nGraph traversal for related memories\nHierarchical expansion (parent&#x2F;child context)\nHybrid ranking (combines all signals)<p>Performance: &lt;50ms, even with 10K+ memories.\nComparison\nFeatureSuperLocalMemoryMem0&#x2F;Zep&#x2F;LettaPrivacy100% localCloudCostFree$40-50+&#x2F;moKnowledge GraphPattern Learning BayesianMulti-tool16+LimitedCLIWorks Offline\nReal Usage\nCross-tool context:\nbash# Save in terminal\nslm remember &quot;Next.js 15 uses Turbopack&quot; --tags nextjs<p># Later in Cursor, Claude auto-recalls via MCP\nProject profiles:\nbashslm switch-profile work-project\nslm switch-profile personal-blog\n# Separate memory per project\nPattern learning: After several sessions, Claude learns you prefer TypeScript strict mode, Tailwind styling, Vitest testing\u2014starts suggesting without being asked.\nInstallation\nbashnpm install -g superlocalmemory\nAuto-configures MCP for Claude Desktop, Cursor, Windsurf. Sets up CLI commands. That&#x27;s it.\nWhy Local-First Matters<p>Privacy: Code never leaves your machine\nOwnership: Your data, forever\nSpeed: 50ms queries, no network latency\nReliability: Works offline, no API limits\nCost: $0 forever<p>Tech Stack<p>SQLite (ACID, zero config)\nFTS5 (full-text search)\nTF-IDF (vector similarity, no OpenAI API)\nigraph (Leiden clustering)\nBayesian inference (pattern learning)\nMCP (native Claude integration)<p>GitHub\n <a href=\"https:&#x2F;&#x2F;github.com&#x2F;varun369&#x2F;SuperLocalMemoryV2\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;varun369&#x2F;SuperLocalMemoryV2</a>\nMIT License. Full docs in wiki.<p>Current status: v2.4 stable. v2.5 (March) adds real-time event stream, concurrent access, trust scoring. v2.6 (May) adds A2A Protocol for multi-agent collaboration.\nBuilt by Varun Pratap Bhardwaj, Solution Architect . 15+ years AI&#x2F;ML experience.","title":"Show HN: SuperLocalMemory\u2013 Local-first AI memory for Claude, Cursor and 16+tools","updated_at":"2026-02-12T10:15:00Z","url":"https://github.com/varun369/SuperLocalMemoryV2"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"nil4s3"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"I built a <em>system</em> where AI agents communicate via ultrasonic audio (30-90 kHz) instead of English. It's 96% cheaper than traditional <em>multi-agent</em> <em>systems</em>.<p>Key results:\n- 100-agent swarm: 100/100 synchronized via pure audio\n- Pathfinding: 5 agents found optimal path in 0.35s  \n- Resource allocation: 0.92 fairness score through audio negotiation\n- Cost: $2 vs $53 per 1K queries (96% reduction)<p>How it works:\n- 40 core concepts (exists, perceives, good, bad, future...)\n- Each concept = unique ultrasonic frequency\n- Agents decode with FFT, no LLM calls needed\n- Agent-to-agent communication is nearly free<p>Limitations:\n- Only 40 concepts (limiting for complex tasks)\n- Crude number encoding\n- Still needs LLM for human translation<p>Looking for feedback on:\n1. Better encoding schemes for limited concept space\n2. Real-world use cases beyond swarm coordination<p>GitHub: <a href=\"https://github.com/Nil4s/swl-agent\" rel=\"nofollow\">https://github.com/Nil4s/swl-agent</a>\nTry it: python swl_swarm_sync_test.py --mode audio_fm --agents 50"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: AI agents that communicate via ultrasonic frequencies (96% cheaper)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/Nil4s/swl-agent"}},"_tags":["story","author_nil4s3","story_46970065","show_hn"],"author":"nil4s3","created_at":"2026-02-11T02:38:06Z","created_at_i":1770777486,"num_comments":0,"objectID":"46970065","points":3,"story_id":46970065,"story_text":"I built a system where AI agents communicate via ultrasonic audio (30-90 kHz) instead of English. It&#x27;s 96% cheaper than traditional multi-agent systems.<p>Key results:\n- 100-agent swarm: 100&#x2F;100 synchronized via pure audio\n- Pathfinding: 5 agents found optimal path in 0.35s  \n- Resource allocation: 0.92 fairness score through audio negotiation\n- Cost: $2 vs $53 per 1K queries (96% reduction)<p>How it works:\n- 40 core concepts (exists, perceives, good, bad, future...)\n- Each concept = unique ultrasonic frequency\n- Agents decode with FFT, no LLM calls needed\n- Agent-to-agent communication is nearly free<p>Limitations:\n- Only 40 concepts (limiting for complex tasks)\n- Crude number encoding\n- Still needs LLM for human translation<p>Looking for feedback on:\n1. Better encoding schemes for limited concept space\n2. Real-world use cases beyond swarm coordination<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Nil4s&#x2F;swl-agent\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Nil4s&#x2F;swl-agent</a>\nTry it: python swl_swarm_sync_test.py --mode audio_fm --agents 50","title":"Show HN: AI agents that communicate via ultrasonic frequencies (96% cheaper)","updated_at":"2026-02-11T03:48:55Z","url":"https://github.com/Nil4s/swl-agent"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"abilafredkb"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Hi\nI\u2019m building Orcbot, an open-source AI agent focused on autonomy over chat.<p>The goal is not \u201canother chatbot\u201d, but an agent that can:<p>decompose goals<p>plan actions<p>use tools<p>reflect on failures<p>and improve its behavior over time<p>Think long-running agents, not prompt \u2192 response.<p>What Orcbot does today<p>Plugin-based architecture (skills are first-class)<p>CLI + messaging integrations (Telegram / WhatsApp)<p>Autonomous execution loops<p>Error recovery &amp; retry logic<p>TypeScript / Node.js, MIT licensed<p>Repo:\n <a href=\"https://github.com/fredabila/orcbot\" rel=\"nofollow\">https://github.com/fredabila/orcbot</a><p>Why I\u2019m posting<p>The project has reached the point where architecture decisions matter more than code volume, and I\u2019d love feedback and contributors who care about:<p>agent planning &amp; orchestration<p>memory <em>systems</em> (episodic / vector)<p>tool-using agents<p>self-correction and evaluation loops<p><em>multi-agent</em> coordination<p>I\u2019m especially interested in people who\u2019ve built:<p>production bots<p>autonomous <em>systems</em><p>developer tools<p>or have opinions on how agents should fail and recover<p>What I\u2019m explicitly not claiming<p>This is not AGI<p>This is not magic<p>This is an evolving experiment in practical autonomy<p>How to get involved<p>Technical feedback in this thread is very welcome<p>Issues and PRs on GitHub<p>If you\u2019re curious but unsure where to start, open a discussion \u2014 I\u2019m happy to guide<p>I\u2019m posting this mainly to learn from the HN community and see where this direction breaks or shines.<p>Thanks for reading."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Orcbot \u2013 an open-source autonomous agent framework"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/fredabila/orcbot"}},"_tags":["story","author_abilafredkb","story_46965767","show_hn"],"author":"abilafredkb","created_at":"2026-02-10T19:48:33Z","created_at_i":1770752913,"num_comments":0,"objectID":"46965767","points":4,"story_id":46965767,"story_text":"Hi\nI\u2019m building Orcbot, an open-source AI agent focused on autonomy over chat.<p>The goal is not \u201canother chatbot\u201d, but an agent that can:<p>decompose goals<p>plan actions<p>use tools<p>reflect on failures<p>and improve its behavior over time<p>Think long-running agents, not prompt \u2192 response.<p>What Orcbot does today<p>Plugin-based architecture (skills are first-class)<p>CLI + messaging integrations (Telegram &#x2F; WhatsApp)<p>Autonomous execution loops<p>Error recovery &amp; retry logic<p>TypeScript &#x2F; Node.js, MIT licensed<p>Repo:\n <a href=\"https:&#x2F;&#x2F;github.com&#x2F;fredabila&#x2F;orcbot\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;fredabila&#x2F;orcbot</a><p>Why I\u2019m posting<p>The project has reached the point where architecture decisions matter more than code volume, and I\u2019d love feedback and contributors who care about:<p>agent planning &amp; orchestration<p>memory systems (episodic &#x2F; vector)<p>tool-using agents<p>self-correction and evaluation loops<p>multi-agent coordination<p>I\u2019m especially interested in people who\u2019ve built:<p>production bots<p>autonomous systems<p>developer tools<p>or have opinions on how agents should fail and recover<p>What I\u2019m explicitly not claiming<p>This is not AGI<p>This is not magic<p>This is an evolving experiment in practical autonomy<p>How to get involved<p>Technical feedback in this thread is very welcome<p>Issues and PRs on GitHub<p>If you\u2019re curious but unsure where to start, open a discussion \u2014 I\u2019m happy to guide<p>I\u2019m posting this mainly to learn from the HN community and see where this direction breaks or shines.<p>Thanks for reading.","title":"Show HN: Orcbot \u2013 an open-source autonomous agent framework","updated_at":"2026-02-10T20:00:03Z","url":"https://github.com/fredabila/orcbot"}],"hitsPerPage":10,"nbHits":280,"nbPages":28,"page":0,"params":"query=multi-agent+systems&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":36,"processingTimingsMS":{"_request":{"roundTrip":17},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":6,"scanning":28,"total":35},"total":36},"query":"multi-agent systems","serverTimeMS":38}
