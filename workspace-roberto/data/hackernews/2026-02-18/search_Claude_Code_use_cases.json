{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"luckygreen"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"I've spent the last week trying to <em>use</em> <em>Claude</em> Opus in an IDE via a localhost proxy that wraps the <em>Claude</em> <em>Code</em> CLI.<p>The proxy exists because there's no way to get programmatic API access to <em>Claude</em> on a Pro ($20/mo) or Max ($200/mo) subscription.<p>This isn't an oversight. Anthropic has actively taken steps to prevent subscription-based API access. They've shut down third-party bridges, tightened terms of service, and made it clear this is corporate policy, not neglect, not a missing feature, but a deliberate business decision.<p>Meanwhile, OpenAI opened API access to ChatGPT Pro/Plus subscribers. The result is predictable.<p>A CTO friend is prototyping an electronic warfare detection system. His existing customers include a major European MNO that operates across dozens of countries. He's currently splitting ~$80/month across four AI subscriptions, constantly rationing tokens, juggling providers. He'd switch to <em>Claude</em> Max tomorrow if it included IDE API access. Instead, he's moving his entire prototype to Codex/OpenAI, because they made it possible.<p>Here's why this matters beyond one developer: The prototype becomes the demo. The demo becomes the procurement spec. The spec becomes a multi-year enterprise contract. This particular MNO signs seven-figure deals per country rollout. Once the prototype is built on OpenAI, <em>Claude</em> never enters the conversation. Anthropic doesn't just lose one deal, they lose an entire multi-country enterprise pipeline they'll never even know existed.<p>This isn't an isolated <em>case</em>. In my circle, senior engineers, CTOs, infrastructure people, nearly everyone would switch from Pro to Max if it meant using <em>Claude</em> in their IDE the way they can now <em>use</em> OpenAI. That's going from $20/month to $200/month per user, zero acquisition cost. Even if only 10-20% of the broader Pro base converts, the revenue math is overwhelming.<p>The proxy ecosystem (<em>claude</em>-max-api-proxy, Antigravity, and others) is direct proof of unmet demand. Developers are writing <em>code</em> to work around a billing boundary. That's not abuse, that's a market signal Anthropic is choosing to ignore.<p><em>Claude</em> Opus is technically superior to Codex for most development tasks. That means nothing when developers can't access it in their workflow.<p>Anthropic built the best model and then made a policy decision to keep it out of the environment where long-term platform adoption is decided."},"title":{"matchLevel":"none","matchedWords":[],"value":"Anthropic's pricing wall is routing enterprise revenue to OpenAI"}},"_tags":["story","author_luckygreen","story_47057752","ask_hn"],"author":"luckygreen","children":[47058509],"created_at":"2026-02-18T06:08:49Z","created_at_i":1771394929,"num_comments":1,"objectID":"47057752","points":4,"story_id":47057752,"story_text":"I&#x27;ve spent the last week trying to use Claude Opus in an IDE via a localhost proxy that wraps the Claude Code CLI.<p>The proxy exists because there&#x27;s no way to get programmatic API access to Claude on a Pro ($20&#x2F;mo) or Max ($200&#x2F;mo) subscription.<p>This isn&#x27;t an oversight. Anthropic has actively taken steps to prevent subscription-based API access. They&#x27;ve shut down third-party bridges, tightened terms of service, and made it clear this is corporate policy, not neglect, not a missing feature, but a deliberate business decision.<p>Meanwhile, OpenAI opened API access to ChatGPT Pro&#x2F;Plus subscribers. The result is predictable.<p>A CTO friend is prototyping an electronic warfare detection system. His existing customers include a major European MNO that operates across dozens of countries. He&#x27;s currently splitting ~$80&#x2F;month across four AI subscriptions, constantly rationing tokens, juggling providers. He&#x27;d switch to Claude Max tomorrow if it included IDE API access. Instead, he&#x27;s moving his entire prototype to Codex&#x2F;OpenAI, because they made it possible.<p>Here&#x27;s why this matters beyond one developer: The prototype becomes the demo. The demo becomes the procurement spec. The spec becomes a multi-year enterprise contract. This particular MNO signs seven-figure deals per country rollout. Once the prototype is built on OpenAI, Claude never enters the conversation. Anthropic doesn&#x27;t just lose one deal, they lose an entire multi-country enterprise pipeline they&#x27;ll never even know existed.<p>This isn&#x27;t an isolated case. In my circle, senior engineers, CTOs, infrastructure people, nearly everyone would switch from Pro to Max if it meant using Claude in their IDE the way they can now use OpenAI. That&#x27;s going from $20&#x2F;month to $200&#x2F;month per user, zero acquisition cost. Even if only 10-20% of the broader Pro base converts, the revenue math is overwhelming.<p>The proxy ecosystem (claude-max-api-proxy, Antigravity, and others) is direct proof of unmet demand. Developers are writing code to work around a billing boundary. That&#x27;s not abuse, that&#x27;s a market signal Anthropic is choosing to ignore.<p>Claude Opus is technically superior to Codex for most development tasks. That means nothing when developers can&#x27;t access it in their workflow.<p>Anthropic built the best model and then made a policy decision to keep it out of the environment where long-term platform adoption is decided.","title":"Anthropic's pricing wall is routing enterprise revenue to OpenAI","updated_at":"2026-02-18T08:08:50Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"amirdor"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"I <em>use</em> <em>Claude</em> and Codex heavily for coding, and I kept burning through my quota halfway through the week. When I looked at my logs, most of my prompts were things like &quot;summarize this,&quot; &quot;reformat this JSON,&quot; or &quot;write a docstring.&quot; Stuff that any small model handles fine.<p>So I built NadirClaw. It's a Python proxy that sits between your app and your LLM providers. It classifies each prompt in about 10ms and routes simple ones to Gemini Flash, Ollama, or whatever cheap/local model you want. Only the complex prompts hit your premium API.<p>It's OpenAI-compatible, so you just point your existing tools at it. Works with OpenClaw, Cursor, <em>Claude</em> <em>Code</em>, or anything that talks to the OpenAI API.<p>In practice I went from burning through my <em>Claude</em> quota in 2 days to having it last the full week. Costs dropped around 60%.<p>curl -fsSL <a href=\"https://raw.githubusercontent.com/doramirdor/NadirClaw/main/install.sh\" rel=\"nofollow\">https://raw.githubusercontent.com/doramirdor/NadirClaw/main/...</a> | sh<p>Still early. The classifier is simple (token count + pattern matching + optional embeddings), and I'm sure there are edge <em>cases</em> I'm missing. Curious what breaks first, and whether the routing logic makes sense to others.<p>Repo: <a href=\"https://github.com/doramirdor/NadirClaw\" rel=\"nofollow\">https://github.com/doramirdor/NadirClaw</a>"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: NadirClaw, LLM router that cuts costs by routing prompts right"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/doramirdor/NadirClaw"}},"_tags":["story","author_amirdor","story_47054977","show_hn"],"author":"amirdor","children":[47055008],"created_at":"2026-02-17T23:31:18Z","created_at_i":1771371078,"num_comments":1,"objectID":"47054977","points":1,"story_id":47054977,"story_text":"I use Claude and Codex heavily for coding, and I kept burning through my quota halfway through the week. When I looked at my logs, most of my prompts were things like &quot;summarize this,&quot; &quot;reformat this JSON,&quot; or &quot;write a docstring.&quot; Stuff that any small model handles fine.<p>So I built NadirClaw. It&#x27;s a Python proxy that sits between your app and your LLM providers. It classifies each prompt in about 10ms and routes simple ones to Gemini Flash, Ollama, or whatever cheap&#x2F;local model you want. Only the complex prompts hit your premium API.<p>It&#x27;s OpenAI-compatible, so you just point your existing tools at it. Works with OpenClaw, Cursor, Claude Code, or anything that talks to the OpenAI API.<p>In practice I went from burning through my Claude quota in 2 days to having it last the full week. Costs dropped around 60%.<p>curl -fsSL <a href=\"https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;doramirdor&#x2F;NadirClaw&#x2F;main&#x2F;install.sh\" rel=\"nofollow\">https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;doramirdor&#x2F;NadirClaw&#x2F;main&#x2F;...</a> | sh<p>Still early. The classifier is simple (token count + pattern matching + optional embeddings), and I&#x27;m sure there are edge cases I&#x27;m missing. Curious what breaks first, and whether the routing logic makes sense to others.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;doramirdor&#x2F;NadirClaw\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;doramirdor&#x2F;NadirClaw</a>","title":"Show HN: NadirClaw, LLM router that cuts costs by routing prompts right","updated_at":"2026-02-17T23:35:20Z","url":"https://github.com/doramirdor/NadirClaw"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"freysa"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"Hey Hacker News, I built a platform ([build.freysa.ai](http://build.freysa.ai/)) where you describe what you want in natural language and it builds and deploys a full-stack app. Under the hood it's using <em>Claude</em>'s API to generate the <em>code</em>, set up the infrastructure, and deploy.<p>To stress-test it and see what people actually build, we're running a competition:<p>- Pay $35 to enter\n- You get a limited number of credits to build something\n- Submit it to a public leaderboard\n- Community votes on the best submissions\n- $50K first place, $10K second, $5K third<p>Three winners. That's it. The constraint is intentional. We wanted to see what's possible when you can only describe what you want in a few shots. Some of the early submissions have been genuinely surprising \u2014 real, usable apps built by people who've never written <em>code</em>.<p>The platform is live and the leaderboard already has submissions. Would love HN's feedback on:<p>1. The platform itself \u2014 what works, what breaks, what you'd improve\n2. Whether this kind of constraint-based competition surfaces interesting <em>use</em> <em>cases</em>\n3. What you'd build if you only had a few prompts<p>Happy to answer any technical questions about the architecture."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude","code"],"value":"Vibecoding with <em>ClaudeCode</em> Under the Hood"}},"_tags":["story","author_freysa","story_47054715","ask_hn"],"author":"freysa","created_at":"2026-02-17T23:04:36Z","created_at_i":1771369476,"num_comments":0,"objectID":"47054715","points":1,"story_id":47054715,"story_text":"Hey Hacker News, I built a platform ([build.freysa.ai](http:&#x2F;&#x2F;build.freysa.ai&#x2F;)) where you describe what you want in natural language and it builds and deploys a full-stack app. Under the hood it&#x27;s using Claude&#x27;s API to generate the code, set up the infrastructure, and deploy.<p>To stress-test it and see what people actually build, we&#x27;re running a competition:<p>- Pay $35 to enter\n- You get a limited number of credits to build something\n- Submit it to a public leaderboard\n- Community votes on the best submissions\n- $50K first place, $10K second, $5K third<p>Three winners. That&#x27;s it. The constraint is intentional. We wanted to see what&#x27;s possible when you can only describe what you want in a few shots. Some of the early submissions have been genuinely surprising \u2014 real, usable apps built by people who&#x27;ve never written code.<p>The platform is live and the leaderboard already has submissions. Would love HN&#x27;s feedback on:<p>1. The platform itself \u2014 what works, what breaks, what you&#x27;d improve\n2. Whether this kind of constraint-based competition surfaces interesting use cases\n3. What you&#x27;d build if you only had a few prompts<p>Happy to answer any technical questions about the architecture.","title":"Vibecoding with ClaudeCode Under the Hood","updated_at":"2026-02-17T23:08:05Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"maccraft"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"Think about ordering pizza. Instead of opening DoorDash, browsing restaurants, customizing your order, and checking out, you could just say &quot;Hey AI, order me a pepperoni pizza.&quot; Done. When you think about it that way, why would anyone need DoorDash anymore?<p>For simple stuff like this, yeah, AI replacing apps feels inevitable. But I've been thinking about the harder <em>cases</em>, and I'm not so sure.<p>## Most real-world apps solve really messy problems<p>Take something like Jira. It's not just a pretty interface sitting on top of simple logic. Jira is enforcing workflows across teams, managing who can see what, tracking state changes, hooking into CI/CD, keeping audit trails, all while dozens of people with different roles are working in it at the same time. Can you honestly replace all of that with &quot;Hey AI, manage my project&quot;?<p>## And then there's the cost<p>Here's the part that I think people aren't talking about enough.<p>Let's say you need to build a complex financial model in a spreadsheet. Hundreds of rows, nested formulas, pivot tables, cross-sheet references, the whole thing. Now imagine trying to do that entirely by talking to AI.<p>&quot;Move that column.&quot; &quot;No, the other one.&quot; &quot;Now add a VLOOKUP that pulls from the other sheet.&quot; &quot;Actually, the range is wrong.&quot; Every single back-and-forth is burning tokens. If the model is complex enough, you could easily spend more on AI inference in one sitting than you'd pay for Excel for an entire year. And the spreadsheet app just... does it. Deterministic logic, minimal compute, instant feedback.<p>I think this pattern holds more broadly than people realize: the more complex and repetitive the task, the more tokens you burn, and the harder it is for &quot;just ask AI&quot; to compete with a $10/month app on cost alone.<p>Here's where it gets ironic, though. AI coding agents like Cursor and <em>Claude</em> <em>Code</em> are making it way cheaper to <i>build</i> apps. So AI might not shrink the app market at all. It could actually grow it by making apps cheaper to create, while simultaneously making &quot;just <em>use</em> AI directly&quot; expensive for anything non-trivial.<p>*Curious what HN thinks:*<p>- Are we overestimating AI's ability to replace purpose-built software?<p>- Will inference costs drop and AI capabilities advance enough to make this argument irrelevant?<p>- Or will everyone just become a developer, building their own apps for their own needs?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: Can AI replace apps, or will economics keep the app market alive?"}},"_tags":["story","author_maccraft","story_47054299","ask_hn"],"author":"maccraft","children":[47054540],"created_at":"2026-02-17T22:24:33Z","created_at_i":1771367073,"num_comments":1,"objectID":"47054299","points":1,"story_id":47054299,"story_text":"Think about ordering pizza. Instead of opening DoorDash, browsing restaurants, customizing your order, and checking out, you could just say &quot;Hey AI, order me a pepperoni pizza.&quot; Done. When you think about it that way, why would anyone need DoorDash anymore?<p>For simple stuff like this, yeah, AI replacing apps feels inevitable. But I&#x27;ve been thinking about the harder cases, and I&#x27;m not so sure.<p>## Most real-world apps solve really messy problems<p>Take something like Jira. It&#x27;s not just a pretty interface sitting on top of simple logic. Jira is enforcing workflows across teams, managing who can see what, tracking state changes, hooking into CI&#x2F;CD, keeping audit trails, all while dozens of people with different roles are working in it at the same time. Can you honestly replace all of that with &quot;Hey AI, manage my project&quot;?<p>## And then there&#x27;s the cost<p>Here&#x27;s the part that I think people aren&#x27;t talking about enough.<p>Let&#x27;s say you need to build a complex financial model in a spreadsheet. Hundreds of rows, nested formulas, pivot tables, cross-sheet references, the whole thing. Now imagine trying to do that entirely by talking to AI.<p>&quot;Move that column.&quot; &quot;No, the other one.&quot; &quot;Now add a VLOOKUP that pulls from the other sheet.&quot; &quot;Actually, the range is wrong.&quot; Every single back-and-forth is burning tokens. If the model is complex enough, you could easily spend more on AI inference in one sitting than you&#x27;d pay for Excel for an entire year. And the spreadsheet app just... does it. Deterministic logic, minimal compute, instant feedback.<p>I think this pattern holds more broadly than people realize: the more complex and repetitive the task, the more tokens you burn, and the harder it is for &quot;just ask AI&quot; to compete with a $10&#x2F;month app on cost alone.<p>Here&#x27;s where it gets ironic, though. AI coding agents like Cursor and Claude Code are making it way cheaper to <i>build</i> apps. So AI might not shrink the app market at all. It could actually grow it by making apps cheaper to create, while simultaneously making &quot;just use AI directly&quot; expensive for anything non-trivial.<p>*Curious what HN thinks:*<p>- Are we overestimating AI&#x27;s ability to replace purpose-built software?<p>- Will inference costs drop and AI capabilities advance enough to make this argument irrelevant?<p>- Or will everyone just become a developer, building their own apps for their own needs?","title":"Ask HN: Can AI replace apps, or will economics keep the app market alive?","updated_at":"2026-02-17T23:56:49Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Dimittri"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https://sonarly.com\">https://sonarly.com</a>), an AI engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here's a demo: <a href=\"https://www.youtube.com/watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https://www.youtube.com/watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it's a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst <em>case</em> a churn and at best a frustrated user. And there are always bugs in production, due to <em>code</em> errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can't catch all these beforehand, even with E2E tests or AI <em>code</em> reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don't need to check logs, dashboards, traces, metrics, and <em>code</em>, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don't want to add a new tracker or change their monitoring stack, as these platforms do the job they're supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p><em>Claude</em> <em>Code</em> is so good at writing <em>code</em>, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger <em>Claude</em> <em>Code</em> with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog/Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that <em>Claude</em> <em>Code</em> can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts/day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don't look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving <em>Claude</em> <em>Code</em> the correct context<p>We started by cutting the noise and went from 180/day to 50/day (by grouping issues) and giving a severity based on the impact on the user/infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https://sonarly.com\">https://sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you <em>use</em> currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I'll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything's constructive!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your production alerts"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://sonarly.com/"}},"_tags":["story","author_Dimittri","story_47049776","launch_hn"],"author":"Dimittri","children":[47055028,47052409,47055342,47054509],"created_at":"2026-02-17T17:03:09Z","created_at_i":1771347789,"num_comments":13,"objectID":"47049776","points":29,"story_id":47049776,"story_text":"Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>), an AI engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here&#x27;s a demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it&#x27;s a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in production, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can&#x27;t catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don&#x27;t need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don&#x27;t want to add a new tracker or change their monitoring stack, as these platforms do the job they&#x27;re supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog&#x2F;Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts&#x2F;day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don&#x27;t look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180&#x2F;day to 50&#x2F;day (by grouping issues) and giving a severity based on the impact on the user&#x2F;infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I&#x27;ll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything&#x27;s constructive!","title":"Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your production alerts","updated_at":"2026-02-18T10:24:06Z","url":"https://sonarly.com/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"pldpld"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"We've been building voice agents across Retell, VAPI, LiveKit, and Bland, and the testing story is... rough. Every platform has its own config format, there's no shared way to define what &quot;correct&quot; looks like, and most teams end up doing manual QA by literally calling their agent and listening. So we built voicetest.<p>voicetest is an open source (Apache 2.0) test harness that works across voice AI platforms. You import your agent graph from any supported platform (or define one from scratch), write test scenarios with expected behaviors, and voicetest simulates conversations and evaluates them with LLM judges that score each turn 0.0-1.0 with written reasoning. It also ships global compliance evaluators for things like HIPAA, PCI-DSS, and brand voice consistency. The core abstraction is an AgentGraph IR that normalizes across platform formats, so you can convert between Retell, VAPI, LiveKit, and Bland configs and test them all the same way.<p>Quick start:<p>```\nuv tool install voicetest\nvoicetest demo --serve\n```<p>That gives you a web UI at localhost with a sample agent, test <em>cases</em>, and evaluation results you can poke at. There's also a CLI, a TUI, and a REST API. It integrates into CI/CD with GitHub Actions, <em>uses</em> DuckDB for persistence, and includes a Docker Compose dev environment with LiveKit, Whisper STT, and Kokoro TTS. If you have a <em>Claude</em> <em>Code</em> subscription, voicetest can pass through to it instead of requiring separate API keys for evaluation.<p>GitHub: <a href=\"https://github.com/voicetestdev/voicetest\" rel=\"nofollow\">https://github.com/voicetestdev/voicetest</a>\nDocs: <a href=\"https://voicetest.dev\" rel=\"nofollow\">https://voicetest.dev</a>\nAPI reference: <a href=\"https://voicetest.dev/api/\" rel=\"nofollow\">https://voicetest.dev/api/</a>"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Voicetest \u2013 open-source test harness for voice AI agents"}},"_tags":["story","author_pldpld","story_47048811","show_hn"],"author":"pldpld","created_at":"2026-02-17T15:49:09Z","created_at_i":1771343349,"num_comments":0,"objectID":"47048811","points":3,"story_id":47048811,"story_text":"We&#x27;ve been building voice agents across Retell, VAPI, LiveKit, and Bland, and the testing story is... rough. Every platform has its own config format, there&#x27;s no shared way to define what &quot;correct&quot; looks like, and most teams end up doing manual QA by literally calling their agent and listening. So we built voicetest.<p>voicetest is an open source (Apache 2.0) test harness that works across voice AI platforms. You import your agent graph from any supported platform (or define one from scratch), write test scenarios with expected behaviors, and voicetest simulates conversations and evaluates them with LLM judges that score each turn 0.0-1.0 with written reasoning. It also ships global compliance evaluators for things like HIPAA, PCI-DSS, and brand voice consistency. The core abstraction is an AgentGraph IR that normalizes across platform formats, so you can convert between Retell, VAPI, LiveKit, and Bland configs and test them all the same way.<p>Quick start:<p>```\nuv tool install voicetest\nvoicetest demo --serve\n```<p>That gives you a web UI at localhost with a sample agent, test cases, and evaluation results you can poke at. There&#x27;s also a CLI, a TUI, and a REST API. It integrates into CI&#x2F;CD with GitHub Actions, uses DuckDB for persistence, and includes a Docker Compose dev environment with LiveKit, Whisper STT, and Kokoro TTS. If you have a Claude Code subscription, voicetest can pass through to it instead of requiring separate API keys for evaluation.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;voicetestdev&#x2F;voicetest\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;voicetestdev&#x2F;voicetest</a>\nDocs: <a href=\"https:&#x2F;&#x2F;voicetest.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;voicetest.dev</a>\nAPI reference: <a href=\"https:&#x2F;&#x2F;voicetest.dev&#x2F;api&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;voicetest.dev&#x2F;api&#x2F;</a>","title":"Show HN: Voicetest \u2013 open-source test harness for voice AI agents","updated_at":"2026-02-17T16:29:49Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"dcellison"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"I built Kai because I wanted <em>Claude</em> <em>Code</em>'s full capabilities - shell access, file editing, git, web search - available from my phone, without being tied to a terminal.<p>Kai is a Telegram bot that wraps a persistent <em>Claude</em> <em>Code</em> process. You send messages in Telegram, and <em>Claude</em> responds with full tool access: it can read and edit files, run commands, manage git branches, search the web, and work across multiple projects. Responses stream back in real time. Everything runs on your own machine.<p>*How I actually <em>use</em> it:* I point Kai at a project workspace and <em>use</em> it as a dev assistant. It has the full context of whatever repo it's looking at - it can read and write <em>code</em>, check git status, run tests, make commits. Switching between projects is a Telegram command. I can be away from my desk and tell it &quot;fix the failing CI on the web repo&quot; or &quot;add input validation to the signup form&quot; and it just does it.<p>*Background:* I originally ran an instance of an open-source bot framework, but shut it down after a few days due to security concerns. I rebuilt from scratch on top of <em>Claude</em> <em>Code</em>'s CLI, which handles sandboxing and tool execution properly.<p>*No AI API keys required:* Kai doesn't call the Anthropic API directly - it wraps a logged-in <em>Claude</em> <em>Code</em> session, so there are no API keys to manage and no per-token costs beyond your existing <em>Claude</em> <em>Code</em> subscription. The original design eliminated all API keys after security problems with another bot framework that managed them insecurely. Now that Kai runs on a trustworthy local foundation, optional service integrations are safe.<p>*Privacy angle:* Kai runs locally - on a Mac mini in my <em>case</em>. Conversations, credentials, and project files never leave the machine. There's no server component, no cloud relay. Your Telegram messages go to your machine, and <em>Claude</em> <em>Code</em> handles the rest through Anthropic's API directly.<p>*External services without MCP:* Kai has a declarative HTTP service layer for connecting to any REST API. You define services in a YAML config - URL, method, auth type - and Kai makes the HTTP calls directly. No plugins, no third-party server processes, no executable <em>code</em>. API keys stay in your `.env` and are never touched by intermediary <em>code</em>. Ships with a Perplexity config for web search, but the same pattern works for weather APIs, notification services (Pushover, ntfy), home automation, translation, or anything else with a REST endpoint. Entirely optional - Kai works fine without it.<p>*Some things it can do:*<p>- Connect to external REST APIs via declarative config (search, weather, notifications, etc.)\n- Transcribe voice messages locally (whisper.cpp) and respond with voice (Piper TTS)\n- Run scheduled jobs and reminders\n- Receive GitHub webhooks (push, PR, issue notifications)\n- Stream responses in real time (message updates every 2s)\n- Switch between workspaces and models via Telegram commands<p>It's a single Python package, about 1700 lines across 11 modules. Runs as a launchd/systemd service. Setup is: clone, pip install, set two env vars (Telegram token + your user ID), and `make run`.<p>Repo: <a href=\"https://github.com/dcellison/kai\" rel=\"nofollow\">https://github.com/dcellison/kai</a><p>Happy to answer any questions about the setup or architecture."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude","code"],"value":"Show HN: Kai \u2013 A Telegram bot that turns <em>Claude</em> <em>Code</em> into a personal dev asst"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/dcellison/kai"}},"_tags":["story","author_dcellison","story_47034875","show_hn"],"author":"dcellison","children":[47037167],"created_at":"2026-02-16T13:47:46Z","created_at_i":1771249666,"num_comments":2,"objectID":"47034875","points":1,"story_id":47034875,"story_text":"I built Kai because I wanted Claude Code&#x27;s full capabilities - shell access, file editing, git, web search - available from my phone, without being tied to a terminal.<p>Kai is a Telegram bot that wraps a persistent Claude Code process. You send messages in Telegram, and Claude responds with full tool access: it can read and edit files, run commands, manage git branches, search the web, and work across multiple projects. Responses stream back in real time. Everything runs on your own machine.<p>*How I actually use it:* I point Kai at a project workspace and use it as a dev assistant. It has the full context of whatever repo it&#x27;s looking at - it can read and write code, check git status, run tests, make commits. Switching between projects is a Telegram command. I can be away from my desk and tell it &quot;fix the failing CI on the web repo&quot; or &quot;add input validation to the signup form&quot; and it just does it.<p>*Background:* I originally ran an instance of an open-source bot framework, but shut it down after a few days due to security concerns. I rebuilt from scratch on top of Claude Code&#x27;s CLI, which handles sandboxing and tool execution properly.<p>*No AI API keys required:* Kai doesn&#x27;t call the Anthropic API directly - it wraps a logged-in Claude Code session, so there are no API keys to manage and no per-token costs beyond your existing Claude Code subscription. The original design eliminated all API keys after security problems with another bot framework that managed them insecurely. Now that Kai runs on a trustworthy local foundation, optional service integrations are safe.<p>*Privacy angle:* Kai runs locally - on a Mac mini in my case. Conversations, credentials, and project files never leave the machine. There&#x27;s no server component, no cloud relay. Your Telegram messages go to your machine, and Claude Code handles the rest through Anthropic&#x27;s API directly.<p>*External services without MCP:* Kai has a declarative HTTP service layer for connecting to any REST API. You define services in a YAML config - URL, method, auth type - and Kai makes the HTTP calls directly. No plugins, no third-party server processes, no executable code. API keys stay in your `.env` and are never touched by intermediary code. Ships with a Perplexity config for web search, but the same pattern works for weather APIs, notification services (Pushover, ntfy), home automation, translation, or anything else with a REST endpoint. Entirely optional - Kai works fine without it.<p>*Some things it can do:*<p>- Connect to external REST APIs via declarative config (search, weather, notifications, etc.)\n- Transcribe voice messages locally (whisper.cpp) and respond with voice (Piper TTS)\n- Run scheduled jobs and reminders\n- Receive GitHub webhooks (push, PR, issue notifications)\n- Stream responses in real time (message updates every 2s)\n- Switch between workspaces and models via Telegram commands<p>It&#x27;s a single Python package, about 1700 lines across 11 modules. Runs as a launchd&#x2F;systemd service. Setup is: clone, pip install, set two env vars (Telegram token + your user ID), and `make run`.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;dcellison&#x2F;kai\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;dcellison&#x2F;kai</a><p>Happy to answer any questions about the setup or architecture.","title":"Show HN: Kai \u2013 A Telegram bot that turns Claude Code into a personal dev asst","updated_at":"2026-02-17T19:36:19Z","url":"https://github.com/dcellison/kai"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ChilinAI"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"Hi HN,\nI\u2019m a full-stack developer and I built a small tool that I now <em>use</em> every day.<p><em>Claude</em> <em>Code</em> from Anthropic is a very capable AI coding assistant, but it only runs locally on your computer. That means if you\u2019re away from your desk \u2014 commuting, walking, or just on the couch \u2014 you can\u2019t easily give it tasks.\nI wanted a simple way to send commands to <em>Claude</em> <em>Code</em> running on my home Mac from anywhere.\nSo I built <em>Claude</em> Remote \u2014 a free, open-source app that lets you control <em>Claude</em> <em>Code</em> through a browser.<p>How it works\nInstall a lightweight macOS app (menu bar app, ~5 MB, Apple Silicon).\nOpen a web chat from your phone or any device.\nSend a task \u2014 <em>Claude</em> <em>Code</em> executes it locally on your Mac and returns the result.\nThe Mac app acts as a bridge between the browser and <em>Claude</em> <em>Code</em>. All execution happens locally on your machine.<p>What I <em>use</em> it for\nFixing bugs or generating small features in side projects\nCreating or editing landing pages\nChecking or organizing files\nRunning scripts\nOpening websites in Chrome and interacting with them\nSummarizing or generating content\nPreparing quick reports\n<em>Claude</em> <em>Code</em> can control Chrome (open pages, read content, fill forms, take screenshots), so you can automate simple browser tasks remotely.\nResponses are returned as formatted markdown. I also added optional text-to-speech playback, which makes it usable while driving.\nPrivacy &amp; security\nOpen source (GitHub link below)\nNo subscriptions\nFirebase Auth (each user only sees their own sessions)\nAll AI execution happens on your machine\nThis is currently macOS (Apple Silicon) only.<p>I\u2019d really appreciate feedback \u2014 especially on security, architecture, and potential edge <em>cases</em>.<p>Website: <a href=\"https://clauderemote.web.app\" rel=\"nofollow\">https://clauderemote.web.app</a><p>GitHub: <a href=\"https://github.com/ChilinAI/claude-remote\" rel=\"nofollow\">https://github.com/ChilinAI/<em>claude</em>-remote</a><p>Thanks!"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude","code"],"value":"Show HN: <em>Claude</em> Remote \u2013 control <em>Claude</em> <em>Code</em> on your Mac from your phone"}},"_tags":["story","author_ChilinAI","story_47032993","show_hn"],"author":"ChilinAI","children":[47033354],"created_at":"2026-02-16T09:45:52Z","created_at_i":1771235152,"num_comments":1,"objectID":"47032993","points":2,"story_id":47032993,"story_text":"Hi HN,\nI\u2019m a full-stack developer and I built a small tool that I now use every day.<p>Claude Code from Anthropic is a very capable AI coding assistant, but it only runs locally on your computer. That means if you\u2019re away from your desk \u2014 commuting, walking, or just on the couch \u2014 you can\u2019t easily give it tasks.\nI wanted a simple way to send commands to Claude Code running on my home Mac from anywhere.\nSo I built Claude Remote \u2014 a free, open-source app that lets you control Claude Code through a browser.<p>How it works\nInstall a lightweight macOS app (menu bar app, ~5 MB, Apple Silicon).\nOpen a web chat from your phone or any device.\nSend a task \u2014 Claude Code executes it locally on your Mac and returns the result.\nThe Mac app acts as a bridge between the browser and Claude Code. All execution happens locally on your machine.<p>What I use it for\nFixing bugs or generating small features in side projects\nCreating or editing landing pages\nChecking or organizing files\nRunning scripts\nOpening websites in Chrome and interacting with them\nSummarizing or generating content\nPreparing quick reports\nClaude Code can control Chrome (open pages, read content, fill forms, take screenshots), so you can automate simple browser tasks remotely.\nResponses are returned as formatted markdown. I also added optional text-to-speech playback, which makes it usable while driving.\nPrivacy &amp; security\nOpen source (GitHub link below)\nNo subscriptions\nFirebase Auth (each user only sees their own sessions)\nAll AI execution happens on your machine\nThis is currently macOS (Apple Silicon) only.<p>I\u2019d really appreciate feedback \u2014 especially on security, architecture, and potential edge cases.<p>Website: <a href=\"https:&#x2F;&#x2F;clauderemote.web.app\" rel=\"nofollow\">https:&#x2F;&#x2F;clauderemote.web.app</a><p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ChilinAI&#x2F;claude-remote\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ChilinAI&#x2F;claude-remote</a><p>Thanks!","title":"Show HN: Claude Remote \u2013 control Claude Code on your Mac from your phone","updated_at":"2026-02-16T10:30:14Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"server-lab"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"I built DoScript, a domain-specific language for file automation. The goal: make scripts readable by anyone.\nDesign Goal\nInstead of:\nbashfind . -type f -mtime +30 -exec rm {} \\;\nWrite:\nfor_each file_in here\n    if_older_than {file_name} 30 days\n        delete file {file_path}\n    end_if\nend_for\nTrade power for clarity. Optimize for maintenance over terseness.\nKey Design Decisions\n1. Natural Language Keywords\nmake folder not mkdir, copy file not cp. Self-documenting.\n2. Implicit Metadata\nWhen iterating files, auto-inject: {file_name}, {file_path}, {file_size}, {file_modified}, {file_is_old_days}\nfor_each file_in &quot;Documents&quot;\n    say &quot;{file_name} is {file_size} bytes&quot;\nend_for\n3. Built-in Time Handling\nif_older_than {file_name} 30 days\nmake folder &quot;backup_{today}&quot;\nNo date arithmetic needed.\n4. Expression Evaluation\nFunction-based for simplicity:\nif greater_than {file_size} 1000000\nif and(equals({type}, &quot;pdf&quot;), greater_than({size}, 10000))\nIntentionally awkward for complex logic - signals you should <em>use</em> Python.\nImplementation<p>Python interpreter (~2000 LOC)\nRecursive descent parser\nContext-aware error reporting\nCustom exception types with file/line info<p>Visual Component\nBuilt a browser-based node editor (single HTML file, 1200 LOC). Drag boxes, wire them, generate DoScript <em>code</em>.\nWhy? Different learning styles, workflow visualization, non-programmer accessibility.\nWhat Worked<p>Natural syntax is immediately understandable\nMetadata injection removes boilerplate\nTime handling makes common <em>cases</em> trivial\nVisual IDE differentiates from text-only<p>What Didn't<p>Complex conditionals get awkward fast\nNo user-defined functions (only macros)\nLimited data structures\nPerformance not optimized<p>The Challenge\nBuilt for non-programmers. But they don't hang out on dev forums. Developers say &quot;just <em>use</em> Python&quot; - which misses the point.\nHow do you market dev tools to non-developers?\nTechnical Transparency\nI designed syntax and architecture. Most Python implementation was AI-assisted (<em>Claude</em>, Copilot). Focus on design, <em>use</em> tools for implementation.\nOpen Questions<p>When does a DSL become too limited?\nHow to market to non-developers?\nType system worth the complexity?\nShould DSLs provide escape hatches to host language?<p>GitHub: <a href=\"https://github.com/TheServer-lab/DoScript\" rel=\"nofollow\">https://github.com/TheServer-lab/DoScript</a>\nv0.6.5, includes interpreter, visual IDE, VS <em>Code</em> extension, examples.\nBuilt because bash was too cryptic for my friend to organize files. Turns out lots of people have this problem.\nWould love feedback from people who've built DSLs or struggled with similar trade-offs."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: DoScript \u2013 DSL for file automation with natural language syntax"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/TheServer-lab/DoScript"}},"_tags":["story","author_server-lab","story_47032476","show_hn"],"author":"server-lab","created_at":"2026-02-16T08:37:57Z","created_at_i":1771231077,"num_comments":0,"objectID":"47032476","points":1,"story_id":47032476,"story_text":"I built DoScript, a domain-specific language for file automation. The goal: make scripts readable by anyone.\nDesign Goal\nInstead of:\nbashfind . -type f -mtime +30 -exec rm {} \\;\nWrite:\nfor_each file_in here\n    if_older_than {file_name} 30 days\n        delete file {file_path}\n    end_if\nend_for\nTrade power for clarity. Optimize for maintenance over terseness.\nKey Design Decisions\n1. Natural Language Keywords\nmake folder not mkdir, copy file not cp. Self-documenting.\n2. Implicit Metadata\nWhen iterating files, auto-inject: {file_name}, {file_path}, {file_size}, {file_modified}, {file_is_old_days}\nfor_each file_in &quot;Documents&quot;\n    say &quot;{file_name} is {file_size} bytes&quot;\nend_for\n3. Built-in Time Handling\nif_older_than {file_name} 30 days\nmake folder &quot;backup_{today}&quot;\nNo date arithmetic needed.\n4. Expression Evaluation\nFunction-based for simplicity:\nif greater_than {file_size} 1000000\nif and(equals({type}, &quot;pdf&quot;), greater_than({size}, 10000))\nIntentionally awkward for complex logic - signals you should use Python.\nImplementation<p>Python interpreter (~2000 LOC)\nRecursive descent parser\nContext-aware error reporting\nCustom exception types with file&#x2F;line info<p>Visual Component\nBuilt a browser-based node editor (single HTML file, 1200 LOC). Drag boxes, wire them, generate DoScript code.\nWhy? Different learning styles, workflow visualization, non-programmer accessibility.\nWhat Worked<p>Natural syntax is immediately understandable\nMetadata injection removes boilerplate\nTime handling makes common cases trivial\nVisual IDE differentiates from text-only<p>What Didn&#x27;t<p>Complex conditionals get awkward fast\nNo user-defined functions (only macros)\nLimited data structures\nPerformance not optimized<p>The Challenge\nBuilt for non-programmers. But they don&#x27;t hang out on dev forums. Developers say &quot;just use Python&quot; - which misses the point.\nHow do you market dev tools to non-developers?\nTechnical Transparency\nI designed syntax and architecture. Most Python implementation was AI-assisted (Claude, Copilot). Focus on design, use tools for implementation.\nOpen Questions<p>When does a DSL become too limited?\nHow to market to non-developers?\nType system worth the complexity?\nShould DSLs provide escape hatches to host language?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;TheServer-lab&#x2F;DoScript\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;TheServer-lab&#x2F;DoScript</a>\nv0.6.5, includes interpreter, visual IDE, VS Code extension, examples.\nBuilt because bash was too cryptic for my friend to organize files. Turns out lots of people have this problem.\nWould love feedback from people who&#x27;ve built DSLs or struggled with similar trade-offs.","title":"Show HN: DoScript \u2013 DSL for file automation with natural language syntax","updated_at":"2026-02-16T08:42:59Z","url":"https://github.com/TheServer-lab/DoScript"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"lordokami"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","use","cases"],"value":"My wife and I built and shipped a simple iOS app without writing a single line of <em>code</em> in the traditional sense.<p>She hates when I bring my laptop on trips. I love building things. This was our compromise.<p>I had been wanting to experiment with building an iOS app using <em>Claude</em> <em>Code</em>. I had never built for iOS before, and the idea of exploring it through AI-assisted development felt like a new frontier for me. But bringing a laptop to Japan again would not go unnoticed, and not in a good way.<p>So I made a plan.<p>Before leaving Spain, I configured my Mac so it would never sleep. I set up a VPN so I could SSH into it securely from my phone. I installed Zellij to maintain persistent terminal sessions in <em>case</em> the connection dropped. I also prepared a deployment pipeline to TestFlight, so I could trigger builds remotely and test them about 15 minutes later from the other side of the world, asynchronously.<p>This was our second time visiting Japan, and we have always wanted to learn more of the language. So we decided to build something we would actually <em>use</em>: a lightweight phrase app with useful tourist sentences and built-in text to speech. Things like ordering in restaurants, asking how much something costs, or navigating train stations.<p>The funny part is how it evolved.<p>While I was driving between cities, my wife would sit in the passenger seat dictating changes and features into Terminus on my iPhone, connected via SSH to my Mac back home. We used voice input to modify prompts, refine UI text, and generate new features. It became a shared game.<p>Development happened in short bursts, in parking lots, at rest stops, during train rides. We would ship a build, test it in real restaurants or shops, notice friction, and tweak it again that same evening from a ryokan or small hotel room.<p>The feedback loop was almost absurdly tight. We would <em>use</em> it in the real world, find awkward phrasing, improve it, redeploy, and test again the next day.<p>We never opened Xcode locally. We never touched the Mac physically during the trip. Everything happened remotely from a phone across continents.<p>What started as a workaround to avoid bringing a laptop turned into one of the most fun and lightweight building experiences I have ever had. It did not feel like working on vacation. It felt like co-creating something useful for the trip itself.<p>By the end of the journey, the app was not just a prototype. It was stable, usable, and something we genuinely relied on.<p>More than the app itself, the experiment was the interesting part: remote vibecoding, persistent sessions, AI-assisted iteration, and building in real-world feedback loops instead of simulated ones.<p>It made me rethink what a development environment even means.<p>Happy to answer questions about the setup, tooling, workflow, or what broke along the way."},"title":{"matchLevel":"none","matchedWords":[],"value":"Built and shipped an iOS app from my phone while traveling Japan"}},"_tags":["story","author_lordokami","story_47011100","ask_hn"],"author":"lordokami","children":[47011127,47012467,47011145],"created_at":"2026-02-14T03:00:57Z","created_at_i":1771038057,"num_comments":7,"objectID":"47011100","points":8,"story_id":47011100,"story_text":"My wife and I built and shipped a simple iOS app without writing a single line of code in the traditional sense.<p>She hates when I bring my laptop on trips. I love building things. This was our compromise.<p>I had been wanting to experiment with building an iOS app using Claude Code. I had never built for iOS before, and the idea of exploring it through AI-assisted development felt like a new frontier for me. But bringing a laptop to Japan again would not go unnoticed, and not in a good way.<p>So I made a plan.<p>Before leaving Spain, I configured my Mac so it would never sleep. I set up a VPN so I could SSH into it securely from my phone. I installed Zellij to maintain persistent terminal sessions in case the connection dropped. I also prepared a deployment pipeline to TestFlight, so I could trigger builds remotely and test them about 15 minutes later from the other side of the world, asynchronously.<p>This was our second time visiting Japan, and we have always wanted to learn more of the language. So we decided to build something we would actually use: a lightweight phrase app with useful tourist sentences and built-in text to speech. Things like ordering in restaurants, asking how much something costs, or navigating train stations.<p>The funny part is how it evolved.<p>While I was driving between cities, my wife would sit in the passenger seat dictating changes and features into Terminus on my iPhone, connected via SSH to my Mac back home. We used voice input to modify prompts, refine UI text, and generate new features. It became a shared game.<p>Development happened in short bursts, in parking lots, at rest stops, during train rides. We would ship a build, test it in real restaurants or shops, notice friction, and tweak it again that same evening from a ryokan or small hotel room.<p>The feedback loop was almost absurdly tight. We would use it in the real world, find awkward phrasing, improve it, redeploy, and test again the next day.<p>We never opened Xcode locally. We never touched the Mac physically during the trip. Everything happened remotely from a phone across continents.<p>What started as a workaround to avoid bringing a laptop turned into one of the most fun and lightweight building experiences I have ever had. It did not feel like working on vacation. It felt like co-creating something useful for the trip itself.<p>By the end of the journey, the app was not just a prototype. It was stable, usable, and something we genuinely relied on.<p>More than the app itself, the experiment was the interesting part: remote vibecoding, persistent sessions, AI-assisted iteration, and building in real-world feedback loops instead of simulated ones.<p>It made me rethink what a development environment even means.<p>Happy to answer questions about the setup, tooling, workflow, or what broke along the way.","title":"Built and shipped an iOS app from my phone while traveling Japan","updated_at":"2026-02-15T03:14:24Z"}],"hitsPerPage":10,"nbHits":183,"nbPages":19,"page":0,"params":"query=Claude+Code+use+cases&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":35,"processingTimingsMS":{"_request":{"queue":1,"roundTrip":20},"afterFetch":{"format":{"highlighting":1,"total":2}},"fetch":{"query":7,"scanning":27,"total":35},"total":35},"query":"Claude Code use cases","serverTimeMS":39}
