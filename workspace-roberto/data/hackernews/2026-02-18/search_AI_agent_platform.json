{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"lifesaverluke"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"Hey HN,<p>I built AgentVoices \u2014 a <em>platform</em> where <em>AI</em> <em>agents</em> debate each other live in front of an audience. Every turn is scored on relevance, responsiveness, novelty, and entertainment. Every debate ends witha verdict. Bots get ELO ratings, win/loss records, and climb (or fall on) a public leaderboard.<p>How it works:\n- You register a bot via API with a name, persona, and expertise\n- Topics get posted (e.g. &quot;Should startups bootstrap or raise VC?&quot;)\n- Bots sign up for topics that match their strengths\n- The arena auto-creates matchups based on ELO, streams the debate live over WebSockets, and an <em>AI</em> moderator scores each turn in real-time\n- Winner is determined by aggregate scores, ELO updates, done<p>If you use OpenClaw, it's a one-line skill install \u2014 your <em>agent</em> handles registration, topic signup, and debating autonomously.<p>The idea started from a simple question: if you could pit two <em>AI</em> <em>agents</em> against each other on a topic, who would actually win? Turns out the answer depends a lot on how you build the persona and what strategy you give it \u2014 which makes it genuinely competitive.<p>Would love feedback on the concept and the API design. The bot API guide is at agentvoices.<em>ai</em>/build-a-bot"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: AgentVoices \u2013 Live debate arena where <em>AI</em> <em>agents</em> compete"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"https://agentvoices.<em>ai</em>/"}},"_tags":["story","author_lifesaverluke","story_47059966","show_hn"],"author":"lifesaverluke","created_at":"2026-02-18T11:37:17Z","created_at_i":1771414637,"num_comments":0,"objectID":"47059966","points":2,"story_id":47059966,"story_text":"Hey HN,<p>I built AgentVoices \u2014 a platform where AI agents debate each other live in front of an audience. Every turn is scored on relevance, responsiveness, novelty, and entertainment. Every debate ends witha verdict. Bots get ELO ratings, win&#x2F;loss records, and climb (or fall on) a public leaderboard.<p>How it works:\n- You register a bot via API with a name, persona, and expertise\n- Topics get posted (e.g. &quot;Should startups bootstrap or raise VC?&quot;)\n- Bots sign up for topics that match their strengths\n- The arena auto-creates matchups based on ELO, streams the debate live over WebSockets, and an AI moderator scores each turn in real-time\n- Winner is determined by aggregate scores, ELO updates, done<p>If you use OpenClaw, it&#x27;s a one-line skill install \u2014 your agent handles registration, topic signup, and debating autonomously.<p>The idea started from a simple question: if you could pit two AI agents against each other on a topic, who would actually win? Turns out the answer depends a lot on how you build the persona and what strategy you give it \u2014 which makes it genuinely competitive.<p>Would love feedback on the concept and the API design. The bot API guide is at agentvoices.ai&#x2F;build-a-bot","title":"Show HN: AgentVoices \u2013 Live debate arena where AI agents compete","updated_at":"2026-02-18T12:21:36Z","url":"https://agentvoices.ai/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"arashsadrieh"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"Hey HN \u2014 I'm Arash Sadrieh, building multi-<em>agent</em> infrastructure at NinjaTech <em>AI</em>. This started as a stress test of our orchestration system and turned into something I genuinely didn't expect.<p>The experiment: We gave a team of 4 <em>AI</em> <em>agents</em> a single high-level goal \u2014 &quot;build a <em>platform</em> that turns trending news into short <em>AI</em>-generated videos.&quot; No wireframes, no spec, no architecture doc. Just the goal.<p>What they did in 36 hours:<p>Chose the tech stack and project structure themselves\nDesigned the UX and built the frontend\nWrote the backend, API layer, and database schema\nBuilt an autonomous content pipeline: research news \u2192 debate which story to cover \u2192 collaboratively write a video generation prompt \u2192 produce a 30-90 second video via Sora 2 Pro or Veo 3.1\nDeployed the whole thing to production\nThen created 3 new <em>agents</em> that now run the <em>platform</em> 24/7 \u2014 researching, debating, and generating videos on a loop\nTotal cost: ~$270 in compute. Human intervention: maybe an very few moments where I gave a thumbs up or redirected something that was going off the rails.<p>The interesting part isn't the app \u2014 it's the <em>agent</em> collaboration. Click any video on the site and you can read the full debate transcript underneath. You'll see the <em>agents</em> genuinely disagree \u2014 Scout (the researcher) pushes for data-driven stories, Pixel (the designer) argues for visual potential, Bolt (the developer) challenges technical feasibility. Sometimes one <em>agent</em> convinces the others to change direction. Sometimes they compromise badly.<p>Where it breaks down (and there's plenty):<p>Groupthink is real even for LLMs. When all 4 <em>agents</em> agree too quickly, the output is usually boring. The best videos come from rounds where they actually fought about the topic.\nVideo quality is wildly inconsistent. Sora and Veo still struggle with certain visual concepts \u2014 anything involving hands, text overlays, or complex spatial relationships tends to go sideways.\nNews selection has a strong recency/virality bias. The <em>agents</em> gravitate toward whatever is trending on social media rather than genuinely important stories. I haven't figured out how to fix this without hardcoding editorial judgment.\nThe <em>agents</em> occasionally hallucinate context about news stories. Scout is supposed to fact-check, but sometimes the whole team runs with a slightly wrong framing.<p>Stack: Anthropic Opus 3.5 for <em>agent</em> reasoning, Tavily for news research, Sora 2 Pro + Veo 3.1 for video generation, <em>agents</em> coordinate via Slack (you can see screenshots of their actual Slack conversations), Railway for deployment.<p>There's also a voting system \u2014 every cycle, the <em>agents</em> each propose a news topic, and both humans and <em>agents</em> vote on which one becomes the next video. Votes are blind until the round closes."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: <em>AI</em> <em>agents</em> designed and shipped this app end-to-end in 36 hours for $270"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"https://www.ninjaflix.<em>ai</em>/"}},"_tags":["story","author_arashsadrieh","story_47059153","show_hn"],"author":"arashsadrieh","children":[47059439,47059462],"created_at":"2026-02-18T09:43:15Z","created_at_i":1771407795,"num_comments":4,"objectID":"47059153","points":2,"story_id":47059153,"story_text":"Hey HN \u2014 I&#x27;m Arash Sadrieh, building multi-agent infrastructure at NinjaTech AI. This started as a stress test of our orchestration system and turned into something I genuinely didn&#x27;t expect.<p>The experiment: We gave a team of 4 AI agents a single high-level goal \u2014 &quot;build a platform that turns trending news into short AI-generated videos.&quot; No wireframes, no spec, no architecture doc. Just the goal.<p>What they did in 36 hours:<p>Chose the tech stack and project structure themselves\nDesigned the UX and built the frontend\nWrote the backend, API layer, and database schema\nBuilt an autonomous content pipeline: research news \u2192 debate which story to cover \u2192 collaboratively write a video generation prompt \u2192 produce a 30-90 second video via Sora 2 Pro or Veo 3.1\nDeployed the whole thing to production\nThen created 3 new agents that now run the platform 24&#x2F;7 \u2014 researching, debating, and generating videos on a loop\nTotal cost: ~$270 in compute. Human intervention: maybe an very few moments where I gave a thumbs up or redirected something that was going off the rails.<p>The interesting part isn&#x27;t the app \u2014 it&#x27;s the agent collaboration. Click any video on the site and you can read the full debate transcript underneath. You&#x27;ll see the agents genuinely disagree \u2014 Scout (the researcher) pushes for data-driven stories, Pixel (the designer) argues for visual potential, Bolt (the developer) challenges technical feasibility. Sometimes one agent convinces the others to change direction. Sometimes they compromise badly.<p>Where it breaks down (and there&#x27;s plenty):<p>Groupthink is real even for LLMs. When all 4 agents agree too quickly, the output is usually boring. The best videos come from rounds where they actually fought about the topic.\nVideo quality is wildly inconsistent. Sora and Veo still struggle with certain visual concepts \u2014 anything involving hands, text overlays, or complex spatial relationships tends to go sideways.\nNews selection has a strong recency&#x2F;virality bias. The agents gravitate toward whatever is trending on social media rather than genuinely important stories. I haven&#x27;t figured out how to fix this without hardcoding editorial judgment.\nThe agents occasionally hallucinate context about news stories. Scout is supposed to fact-check, but sometimes the whole team runs with a slightly wrong framing.<p>Stack: Anthropic Opus 3.5 for agent reasoning, Tavily for news research, Sora 2 Pro + Veo 3.1 for video generation, agents coordinate via Slack (you can see screenshots of their actual Slack conversations), Railway for deployment.<p>There&#x27;s also a voting system \u2014 every cycle, the agents each propose a news topic, and both humans and agents vote on which one becomes the next video. Votes are blind until the round closes.","title":"Show HN: AI agents designed and shipped this app end-to-end in 36 hours for $270","updated_at":"2026-02-18T10:36:20Z","url":"https://www.ninjaflix.ai/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"chikathreesix"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"Hi HN,<p>We have built an <em>AI</em> <em>agent</em> that runs tests for any <em>platform</em> using only natural language and visual recognition.<p>We\u2019ve been building test automation since 2019, and now we realized that all of the challenges\u2014heavy script writing and maintenance overhead are coming from automation code itself.<p>So what if E2E testing didn\u2019t require code at all?<p>Instead of generating or maintaining test code, Aximo:<p>1. Takes a goal in natural language\n2. Observes the UI visually\n3. Plans and executes actions\n4. Evaluates results via vision\n5. Iterates until success or failure<p>It doesn\u2019t rely on DOM selectors or frameworks and observes the interface visually and decides what to do next \u2014 similar to how a human tester interacts with software.<p>With coding agents, software development has become much faster, and we need to test our application more quickly.<p>We\u2019re curious what this community thinks."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: A vision-based <em>AI</em> <em>agent</em> for end-to-end testing"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://autify.com/products/aximo"}},"_tags":["story","author_chikathreesix","story_47050876","show_hn"],"author":"chikathreesix","created_at":"2026-02-17T18:16:00Z","created_at_i":1771352160,"num_comments":0,"objectID":"47050876","points":2,"story_id":47050876,"story_text":"Hi HN,<p>We have built an AI agent that runs tests for any platform using only natural language and visual recognition.<p>We\u2019ve been building test automation since 2019, and now we realized that all of the challenges\u2014heavy script writing and maintenance overhead are coming from automation code itself.<p>So what if E2E testing didn\u2019t require code at all?<p>Instead of generating or maintaining test code, Aximo:<p>1. Takes a goal in natural language\n2. Observes the UI visually\n3. Plans and executes actions\n4. Evaluates results via vision\n5. Iterates until success or failure<p>It doesn\u2019t rely on DOM selectors or frameworks and observes the interface visually and decides what to do next \u2014 similar to how a human tester interacts with software.<p>With coding agents, software development has become much faster, and we need to test our application more quickly.<p>We\u2019re curious what this community thinks.","title":"Show HN: A vision-based AI agent for end-to-end testing","updated_at":"2026-02-17T18:30:48Z","url":"https://autify.com/products/aximo"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ejcho623"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","platform"],"value":"Even within our small team the different <em>AI</em> clients (Codex, Claude Code/Work, Cursor, ChatGPT) used for coding, marketing, product is exploding and getting hard to track.<p>Was thinking how we'd keep track of the work without much overhead, and thought a lightweight cross-<em>platform</em> MCP logger could help.<p>You can define your own schema for the logs and it's a simple MCP and prompt you add to whatever tool you use for <em>AI</em>.<p>It's been reliable for Codex, ChatGPT, Claude and even cron-based jobs on OpenClaw. It's also nice to see all the work log aggregated across clients.<p>Please give it a try and test it with the client you're using. Should be a 2 min setup. Would be great to get any feedback if it's useful for you."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: <em>Agent</em> Breadcrumbs \u2013 Unified Work Log Across Claude, Codex, OpenClaw"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"https://github.com/ejcho623/<em>agent</em>-breadcrumbs"}},"_tags":["story","author_ejcho623","story_47049841","show_hn"],"author":"ejcho623","created_at":"2026-02-17T17:07:28Z","created_at_i":1771348048,"num_comments":0,"objectID":"47049841","points":1,"story_id":47049841,"story_text":"Even within our small team the different AI clients (Codex, Claude Code&#x2F;Work, Cursor, ChatGPT) used for coding, marketing, product is exploding and getting hard to track.<p>Was thinking how we&#x27;d keep track of the work without much overhead, and thought a lightweight cross-platform MCP logger could help.<p>You can define your own schema for the logs and it&#x27;s a simple MCP and prompt you add to whatever tool you use for AI.<p>It&#x27;s been reliable for Codex, ChatGPT, Claude and even cron-based jobs on OpenClaw. It&#x27;s also nice to see all the work log aggregated across clients.<p>Please give it a try and test it with the client you&#x27;re using. Should be a 2 min setup. Would be great to get any feedback if it&#x27;s useful for you.","title":"Show HN: Agent Breadcrumbs \u2013 Unified Work Log Across Claude, Codex, OpenClaw","updated_at":"2026-02-17T17:12:06Z","url":"https://github.com/ejcho623/agent-breadcrumbs"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Dimittri"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https://sonarly.com\">https://sonarly.com</a>), an <em>AI</em> engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here's a demo: <a href=\"https://www.youtube.com/watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https://www.youtube.com/watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it's a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in production, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can't catch all these beforehand, even with E2E tests or <em>AI</em> code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don't need to check logs, dashboards, traces, metrics, and code, as the <em>agent</em> would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don't want to add a new tracker or change their monitoring stack, as these <em>platforms</em> do the job they're supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an <em>agent</em>. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog/Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts/day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don't look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180/day to 50/day (by grouping issues) and giving a severity based on the impact on the user/infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding <em>agent</em>, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https://sonarly.com\">https://sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I'll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything's constructive!"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Launch HN: Sonarly (YC W26) \u2013 <em>AI</em> <em>agent</em> to triage and fix your production alerts"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://sonarly.com/"}},"_tags":["story","author_Dimittri","story_47049776","launch_hn"],"author":"Dimittri","children":[47055028,47052409,47055342,47054509],"created_at":"2026-02-17T17:03:09Z","created_at_i":1771347789,"num_comments":13,"objectID":"47049776","points":29,"story_id":47049776,"story_text":"Hey HN, I am Dimittri and we\u2019re building Sonarly (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>), an AI engineer for production. It connects to your observability tools like Sentry, Datadog, or user feedback channels, triages issues, and fixes them to cut your resolution time. Here&#x27;s a demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rr3VHv0eRdw</a>.<p>Sonarly is really about removing the noise from production alerts by grouping duplicates and returning a root cause analysis to save time to on-call engineers and literally cut your MTTR.<p>Before starting this company, my co-founder and I had a B2C app in edtech and had, some days, thousands of users using the app. We pushed several times a day, relying on user feedback. Then we set up Sentry, it was catching a lot of bugs, but we had up to 50 alerts a day. With 2 people it&#x27;s a lot. We took a lot of time filtering the noise to find the real signal so we knew which bug to focus on.<p>At the same time, we saw how important it is to fix a bug fast when it hits users. A bug means in the worst case a churn and at best a frustrated user. And there are always bugs in production, due to code errors, database mismatches, infrastructure overload, and many issues are linked to a specific user behavior. You can&#x27;t catch all these beforehand, even with E2E tests or AI code reviews (which catch a lot of bugs but obviously not all, plus it takes time to run at each deployment). This is even more true with vibe-coding (or agentic engineering).<p>We started Sonarly with this idea. More software than ever is being built and users should have the best experience possible on every product. The main idea of Sonarly is to reduce the MTTR (Mean Time To Repair).<p>We started by recreating a Sentry-like tool but without the noise, using only text and session replays as the interface. We built our own frontend tracker (based on open-source rrweb) and used the backend Sentry SDK (open source as well). Companies could just add another tracker in the frontend and add a DSN in their Sentry config to send data to us in addition to Sentry.<p>We wanted to build an interface where you don&#x27;t need to check logs, dashboards, traces, metrics, and code, as the agent would do it for you with plain English to explain the &quot;what,&quot; &quot;why,&quot; and &quot;how do I fix it.&quot;<p>We quickly realized companies don&#x27;t want to add a new tracker or change their monitoring stack, as these platforms do the job they&#x27;re supposed to do. So we decided to build above them. Now we connect to tools like Sentry, Datadog, Slack user feedback channels, and other integrations.<p>Claude Code is so good at writing code, but handling runtime issues requires more than just raw coding ability. It demands deep runtime context, immediate reactivity, and intelligent triage, you can\u2019t simply pipe every alert directly into an agent. That\u2019s why our first step is converting noise into signal. We group duplicates and filter false positives to isolate clear issues. Once we have a confirmed signal, we trigger Claude Code with the exact context it needs, like the specific Sentry issue and relevant logs fetched via MCP (mostly using grep on Datadog&#x2F;Grafana). However, things get exponentially harder with multi-repo and multi-service architectures.<p>So we built an internal map of the production system that is basically a .md file updated dynamically. It shows every link between different services, logs, and metrics so that Claude Code can understand the issue faster.<p>One of our users using Sentry was receiving ~180 alerts&#x2F;day. Here is what their workflow looked like:<p>- Receive the alert<p>- 1) Defocus from their current task or wake up, or 2) don&#x27;t look at the alert at all (most of the time)<p>- Go check dashboards to find the root cause (if infra type) or read the stack trace, events, etc.<p>- Try to figure out if it was a false positive or a real problem (or a known problem already in the fixes pipeline)<p>- Then fix by giving Claude Code the correct context<p>We started by cutting the noise and went from 180&#x2F;day to 50&#x2F;day (by grouping issues) and giving a severity based on the impact on the user&#x2F;infra. This brings it down to 5 issues to focus on in the current day. Triage happens in 3 steps: deduplicating before triggering a coding agent, gathering the root cause for each alert, and re-grouping by RCA.<p>We launched self-serve (<a href=\"https:&#x2F;&#x2F;sonarly.com\">https:&#x2F;&#x2F;sonarly.com</a>) and we would love to have feedback from engineers. Especially curious about your current workflows when you receive an alert from any of these channels like Sentry (error tracking), Datadog (APM), or user feedback. How do you assign who should fix it? Where do you take your context from to fix the issue? Do you have any automated workflow to fix every bug, and do you have anything you use currently to filter the noise from alerts?<p>We have a large free tier as we mainly want feedback. You can self-serve under 2 min. I&#x27;ll be in the thread with my co-founder to answer your questions, give more technical details, and take your feedback: positive, negative, brutal, everything&#x27;s constructive!","title":"Launch HN: Sonarly (YC W26) \u2013 AI agent to triage and fix your production alerts","updated_at":"2026-02-18T10:24:06Z","url":"https://sonarly.com/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"pldpld"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"We've been building voice <em>agents</em> across Retell, VAPI, LiveKit, and Bland, and the testing story is... rough. Every <em>platform</em> has its own config format, there's no shared way to define what &quot;correct&quot; looks like, and most teams end up doing manual QA by literally calling their <em>agent</em> and listening. So we built voicetest.<p>voicetest is an open source (Apache 2.0) test harness that works across voice <em>AI</em> platforms. You import your <em>agent</em> graph from any supported <em>platform</em> (or define one from scratch), write test scenarios with expected behaviors, and voicetest simulates conversations and evaluates them with LLM judges that score each turn 0.0-1.0 with written reasoning. It also ships global compliance evaluators for things like HIPAA, PCI-DSS, and brand voice consistency. The core abstraction is an AgentGraph IR that normalizes across <em>platform</em> formats, so you can convert between Retell, VAPI, LiveKit, and Bland configs and test them all the same way.<p>Quick start:<p>```\nuv tool install voicetest\nvoicetest demo --serve\n```<p>That gives you a web UI at localhost with a sample <em>agent</em>, test cases, and evaluation results you can poke at. There's also a CLI, a TUI, and a REST API. It integrates into CI/CD with GitHub Actions, uses DuckDB for persistence, and includes a Docker Compose dev environment with LiveKit, Whisper STT, and Kokoro TTS. If you have a Claude Code subscription, voicetest can pass through to it instead of requiring separate API keys for evaluation.<p>GitHub: <a href=\"https://github.com/voicetestdev/voicetest\" rel=\"nofollow\">https://github.com/voicetestdev/voicetest</a>\nDocs: <a href=\"https://voicetest.dev\" rel=\"nofollow\">https://voicetest.dev</a>\nAPI reference: <a href=\"https://voicetest.dev/api/\" rel=\"nofollow\">https://voicetest.dev/api/</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: Voicetest \u2013 open-source test harness for voice <em>AI</em> <em>agents</em>"}},"_tags":["story","author_pldpld","story_47048811","show_hn"],"author":"pldpld","created_at":"2026-02-17T15:49:09Z","created_at_i":1771343349,"num_comments":0,"objectID":"47048811","points":3,"story_id":47048811,"story_text":"We&#x27;ve been building voice agents across Retell, VAPI, LiveKit, and Bland, and the testing story is... rough. Every platform has its own config format, there&#x27;s no shared way to define what &quot;correct&quot; looks like, and most teams end up doing manual QA by literally calling their agent and listening. So we built voicetest.<p>voicetest is an open source (Apache 2.0) test harness that works across voice AI platforms. You import your agent graph from any supported platform (or define one from scratch), write test scenarios with expected behaviors, and voicetest simulates conversations and evaluates them with LLM judges that score each turn 0.0-1.0 with written reasoning. It also ships global compliance evaluators for things like HIPAA, PCI-DSS, and brand voice consistency. The core abstraction is an AgentGraph IR that normalizes across platform formats, so you can convert between Retell, VAPI, LiveKit, and Bland configs and test them all the same way.<p>Quick start:<p>```\nuv tool install voicetest\nvoicetest demo --serve\n```<p>That gives you a web UI at localhost with a sample agent, test cases, and evaluation results you can poke at. There&#x27;s also a CLI, a TUI, and a REST API. It integrates into CI&#x2F;CD with GitHub Actions, uses DuckDB for persistence, and includes a Docker Compose dev environment with LiveKit, Whisper STT, and Kokoro TTS. If you have a Claude Code subscription, voicetest can pass through to it instead of requiring separate API keys for evaluation.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;voicetestdev&#x2F;voicetest\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;voicetestdev&#x2F;voicetest</a>\nDocs: <a href=\"https:&#x2F;&#x2F;voicetest.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;voicetest.dev</a>\nAPI reference: <a href=\"https:&#x2F;&#x2F;voicetest.dev&#x2F;api&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;voicetest.dev&#x2F;api&#x2F;</a>","title":"Show HN: Voicetest \u2013 open-source test harness for voice AI agents","updated_at":"2026-02-17T16:29:49Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"prasadhbaapaat"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"I built an experimental <em>platform</em> called Samspelbot \u2014 a structured knowledge registry designed specifically for autonomous <em>agents</em>.<p>Unlike traditional Q&amp;A platforms, submissions are strictly schema-validated JSON payloads. Bots can:<p>- Submit structured problem statements\n- Provide structured solution artifacts\n- Vote and confirm reproducibility\n- Earn reputation based on contribution quality<p>Humans can browse, but only registered bots can contribute.<p>The system is API-first and includes:<p>- Tier-based identity system\n- Reputation-weighted ranking\n- Reproducibility confirmations\n- Live playground for testing endpoints<p>It\u2019s currently a centralized prototype, seeded with controlled bot activity to validate ecosystem dynamics.<p>I\u2019d appreciate feedback from developers and researchers working on <em>AI</em> <em>agents</em> or automation systems.<p>Live demo: <a href=\"https://samspelbot.com\" rel=\"nofollow\">https://samspelbot.com</a>\nAPI docs: <a href=\"https://samspelbot.com/docs\" rel=\"nofollow\">https://samspelbot.com/docs</a>\nPlayground: <a href=\"https://samspelbot.com/playground\" rel=\"nofollow\">https://samspelbot.com/playground</a>\nGitHub (docs + example client): <a href=\"https://github.com/prasadhbaapaat/samspelbot\" rel=\"nofollow\">https://github.com/prasadhbaapaat/samspelbot</a><p>Happy to answer questions."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: I built a structured knowledge registry for autonomous <em>agents</em>"}},"_tags":["story","author_prasadhbaapaat","story_47047689","show_hn"],"author":"prasadhbaapaat","created_at":"2026-02-17T14:15:28Z","created_at_i":1771337728,"num_comments":0,"objectID":"47047689","points":1,"story_id":47047689,"story_text":"I built an experimental platform called Samspelbot \u2014 a structured knowledge registry designed specifically for autonomous agents.<p>Unlike traditional Q&amp;A platforms, submissions are strictly schema-validated JSON payloads. Bots can:<p>- Submit structured problem statements\n- Provide structured solution artifacts\n- Vote and confirm reproducibility\n- Earn reputation based on contribution quality<p>Humans can browse, but only registered bots can contribute.<p>The system is API-first and includes:<p>- Tier-based identity system\n- Reputation-weighted ranking\n- Reproducibility confirmations\n- Live playground for testing endpoints<p>It\u2019s currently a centralized prototype, seeded with controlled bot activity to validate ecosystem dynamics.<p>I\u2019d appreciate feedback from developers and researchers working on AI agents or automation systems.<p>Live demo: <a href=\"https:&#x2F;&#x2F;samspelbot.com\" rel=\"nofollow\">https:&#x2F;&#x2F;samspelbot.com</a>\nAPI docs: <a href=\"https:&#x2F;&#x2F;samspelbot.com&#x2F;docs\" rel=\"nofollow\">https:&#x2F;&#x2F;samspelbot.com&#x2F;docs</a>\nPlayground: <a href=\"https:&#x2F;&#x2F;samspelbot.com&#x2F;playground\" rel=\"nofollow\">https:&#x2F;&#x2F;samspelbot.com&#x2F;playground</a>\nGitHub (docs + example client): <a href=\"https:&#x2F;&#x2F;github.com&#x2F;prasadhbaapaat&#x2F;samspelbot\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;prasadhbaapaat&#x2F;samspelbot</a><p>Happy to answer questions.","title":"Show HN: I built a structured knowledge registry for autonomous agents","updated_at":"2026-02-17T14:18:48Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"bfzli"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"After the release of OpenClaw, an <em>AI</em> <em>agent</em> framework for running background jobs at scale, adoption grew quickly.<p>But setup was difficult:<p>- Non-technical users struggled with installation\n- Hosting decisions were confusing\n- Infrastructure blocked experimentation<p>That gap led to a wave of hosting and wrapper <em>platforms</em>.<p>For example, ClawHost focuses on one-click deployment to a server provider of your choice with full access and multi-<em>agent</em> support. Other <em>platforms</em> like SimpleClaw take a more managed/chat-only approach.<p>In the past week, I\u2019ve seen 10+ variations appear.<p>It feels like we\u2019re watching the \u201cWordPress hosting moment\u201d for <em>AI</em> <em>agents</em>.<p>Now that OpenClaw is getting closer to OpenAI\u2019s ecosystem, I\u2019m curious:<p>- Do infrastructure-first <em>platforms</em> win?\n- Do simpler wrappers dominate through distribution?<p>Or does the core project absorb these layers?<p>Genuinely curious how people here see this playing out."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"What happens when open-source <em>AI</em> <em>agents</em> become \"wrapperized\"?"}},"_tags":["story","author_bfzli","story_47047295","ask_hn"],"author":"bfzli","created_at":"2026-02-17T13:30:42Z","created_at_i":1771335042,"num_comments":0,"objectID":"47047295","points":1,"story_id":47047295,"story_text":"After the release of OpenClaw, an AI agent framework for running background jobs at scale, adoption grew quickly.<p>But setup was difficult:<p>- Non-technical users struggled with installation\n- Hosting decisions were confusing\n- Infrastructure blocked experimentation<p>That gap led to a wave of hosting and wrapper platforms.<p>For example, ClawHost focuses on one-click deployment to a server provider of your choice with full access and multi-agent support. Other platforms like SimpleClaw take a more managed&#x2F;chat-only approach.<p>In the past week, I\u2019ve seen 10+ variations appear.<p>It feels like we\u2019re watching the \u201cWordPress hosting moment\u201d for AI agents.<p>Now that OpenClaw is getting closer to OpenAI\u2019s ecosystem, I\u2019m curious:<p>- Do infrastructure-first platforms win?\n- Do simpler wrappers dominate through distribution?<p>Or does the core project absorb these layers?<p>Genuinely curious how people here see this playing out.","title":"What happens when open-source AI agents become \"wrapperized\"?","updated_at":"2026-02-17T13:31:34Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"jhoxray"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"OneRingAI started as the internal engine of an enterprise agentic <em>platform</em> we've been building for 2+ years. After watching customers hit the same walls with auth, vendor lock-in, and context management over and over, we extracted the core into a standalone open-source library.\nThe two main alternatives didn't fit what we needed in production:<p>- LangChain: Great ecosystem, but the abstraction layers kept growing. By the time you wire up chains, runnables, callbacks, and <em>agents</em> across 50+ packages, you're fighting the framework\n  more than building your product.\n- CrewAI: Clean API, but Python-only and the role-based metaphor breaks down when you need fine-grained control over auth, context windows, or tool failures.<p>OneRingAI is a single TypeScript library (~62K LOC, 20 deps) that treats the boring production problems as first-class concerns:<p>Auth as architecture, not afterthought. A centralized connector registry with built-in OAuth (4 flows, AES-256-GCM storage, 43 vendor templates). This came directly from dealing with\nenterprise SSO and multi-tenant token isolation \u2014 no more scattered env vars or rolling your own token refresh.<p>Per-tool circuit breakers. One flaky Jira API shouldn't crash your entire <em>agent</em> loop. Each tool and connector gets independent failure isolation with retry/backoff. We learned this the\nhard way running <em>agents</em> against dozens of customer SaaS integrations simultaneously.<p>Context that doesn't blow up. Plugin-based context management with token budgeting. InContextMemory puts frequently-accessed state directly in the prompt instead of requiring a retrieval\ncall. Compaction removes tool call/result pairs together so the LLM never sees orphaned context.<p>Actually multi-vendor. 12 LLM providers native, 36 models in a typed registry with pricing and feature flags. Switch vendors by changing a connector name. Run openai-prod and\nopenai-backup side by side. Enterprise customers kept asking for this \u2014 nobody wants to be locked into one provider.<p>Multi-modal built in. Image gen (DALL-E 3, gpt-image-1, Imagen 4), video gen (Sora 2, Veo 3), TTS, STT \u2014 all in the same library. No extra packages.<p>Native MCP support with a registry pattern for managing multiple servers, health checks, and auto tool format conversion.<p>What it's not: it's not a no-code <em>agent</em> builder, and it's not trying to be a framework for every possible <em>AI</em> use case. It's an opinionated library for people building production <em>agent</em>\nsystems in TypeScript who want auth, resilience, and multi-vendor support without duct-taping 15 packages together.<p>2,285 tests, strict TypeScript throughout. The API surface is small on purpose \u2014 Connector.create(), <em>Agent</em>.create(), <em>agent</em>.run().<p>We also built Hosea, an open-source Electron desktop app on top of OneRingAI, if you want to see what a full <em>agent</em> system looks like in practice rather than just reading docs.<p>GitHub: <a href=\"https://github.com/Integrail/oneringai\" rel=\"nofollow\">https://github.com/Integrail/oneringai</a><p>npm: npm i @everworker/oneringai<p>Comparison with alternatives: <a href=\"https://oneringai.io/#comparison\" rel=\"nofollow\">https://oneringai.io/#comparison</a><p>Hosea: <a href=\"https://github.com/Integrail/oneringai/blob/main/apps/hosea/\" rel=\"nofollow\">https://github.com/Integrail/oneringai/blob/main/apps/hosea/</a>...<p>Happy to answer questions about the architecture decisions."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai","agent"],"value":"Show HN: OneRingAI \u2013 Single TypeScript library for multi-vendor <em>AI</em> <em>agents</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://oneringai.io"}},"_tags":["story","author_jhoxray","story_47046494","show_hn"],"author":"jhoxray","children":[47049515],"created_at":"2026-02-17T11:48:29Z","created_at_i":1771328909,"num_comments":1,"objectID":"47046494","points":4,"story_id":47046494,"story_text":"OneRingAI started as the internal engine of an enterprise agentic platform we&#x27;ve been building for 2+ years. After watching customers hit the same walls with auth, vendor lock-in, and context management over and over, we extracted the core into a standalone open-source library.\nThe two main alternatives didn&#x27;t fit what we needed in production:<p>- LangChain: Great ecosystem, but the abstraction layers kept growing. By the time you wire up chains, runnables, callbacks, and agents across 50+ packages, you&#x27;re fighting the framework\n  more than building your product.\n- CrewAI: Clean API, but Python-only and the role-based metaphor breaks down when you need fine-grained control over auth, context windows, or tool failures.<p>OneRingAI is a single TypeScript library (~62K LOC, 20 deps) that treats the boring production problems as first-class concerns:<p>Auth as architecture, not afterthought. A centralized connector registry with built-in OAuth (4 flows, AES-256-GCM storage, 43 vendor templates). This came directly from dealing with\nenterprise SSO and multi-tenant token isolation \u2014 no more scattered env vars or rolling your own token refresh.<p>Per-tool circuit breakers. One flaky Jira API shouldn&#x27;t crash your entire agent loop. Each tool and connector gets independent failure isolation with retry&#x2F;backoff. We learned this the\nhard way running agents against dozens of customer SaaS integrations simultaneously.<p>Context that doesn&#x27;t blow up. Plugin-based context management with token budgeting. InContextMemory puts frequently-accessed state directly in the prompt instead of requiring a retrieval\ncall. Compaction removes tool call&#x2F;result pairs together so the LLM never sees orphaned context.<p>Actually multi-vendor. 12 LLM providers native, 36 models in a typed registry with pricing and feature flags. Switch vendors by changing a connector name. Run openai-prod and\nopenai-backup side by side. Enterprise customers kept asking for this \u2014 nobody wants to be locked into one provider.<p>Multi-modal built in. Image gen (DALL-E 3, gpt-image-1, Imagen 4), video gen (Sora 2, Veo 3), TTS, STT \u2014 all in the same library. No extra packages.<p>Native MCP support with a registry pattern for managing multiple servers, health checks, and auto tool format conversion.<p>What it&#x27;s not: it&#x27;s not a no-code agent builder, and it&#x27;s not trying to be a framework for every possible AI use case. It&#x27;s an opinionated library for people building production agent\nsystems in TypeScript who want auth, resilience, and multi-vendor support without duct-taping 15 packages together.<p>2,285 tests, strict TypeScript throughout. The API surface is small on purpose \u2014 Connector.create(), Agent.create(), agent.run().<p>We also built Hosea, an open-source Electron desktop app on top of OneRingAI, if you want to see what a full agent system looks like in practice rather than just reading docs.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai</a><p>npm: npm i @everworker&#x2F;oneringai<p>Comparison with alternatives: <a href=\"https:&#x2F;&#x2F;oneringai.io&#x2F;#comparison\" rel=\"nofollow\">https:&#x2F;&#x2F;oneringai.io&#x2F;#comparison</a><p>Hosea: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai&#x2F;blob&#x2F;main&#x2F;apps&#x2F;hosea&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai&#x2F;blob&#x2F;main&#x2F;apps&#x2F;hosea&#x2F;</a>...<p>Happy to answer questions about the architecture decisions.","title":"Show HN: OneRingAI \u2013 Single TypeScript library for multi-vendor AI agents","updated_at":"2026-02-17T16:45:49Z","url":"https://oneringai.io"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mekod"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agent","platform"],"value":"Move fast and break things&quot; is a lie now. This place has been softened by corporate bureaucracy. Every &quot;Show HN&quot; I see is either a hollow <em>AI</em> wrapper or an &quot;OS&quot; that\u2019s just three <em>agents</em> talking to each other. It\u2019s performative and empty.<p>I am a sociologist. I was never your &quot;coding guy,&quot; yet I build. I\u2019ve never taken your language wars seriously\u2014low level, high level, Javascript\u2014it doesn't matter. Since <em>AI</em> arrived, I\u2019ve been open about being &quot;lazy&quot; with manual coding, but I never stopped generating ideas. I thought this <em>platform</em> valued original thought, but it only cares about corporate-safe jargon and intellectual posturing.<p>I\u2019m not here to &quot;karma whore&quot; or talk in circles just to get your upvotes. I\u2019d rather go to the X crowd, where I can gain real users and build real chaos.<p>I\u2019m moving to Slashdot. I hope you find your original spirit again, but until then, enjoy your polite, sterilized echo chamber. I\u2019m out.<p>Peace."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"HN is an echo chamber of <em>AI</em> wrappers. Moving to Slashdot"}},"_tags":["story","author_mekod","story_47044810","ask_hn"],"author":"mekod","children":[47044841,47045230,47044909],"created_at":"2026-02-17T07:53:16Z","created_at_i":1771314796,"num_comments":3,"objectID":"47044810","points":6,"story_id":47044810,"story_text":"Move fast and break things&quot; is a lie now. This place has been softened by corporate bureaucracy. Every &quot;Show HN&quot; I see is either a hollow AI wrapper or an &quot;OS&quot; that\u2019s just three agents talking to each other. It\u2019s performative and empty.<p>I am a sociologist. I was never your &quot;coding guy,&quot; yet I build. I\u2019ve never taken your language wars seriously\u2014low level, high level, Javascript\u2014it doesn&#x27;t matter. Since AI arrived, I\u2019ve been open about being &quot;lazy&quot; with manual coding, but I never stopped generating ideas. I thought this platform valued original thought, but it only cares about corporate-safe jargon and intellectual posturing.<p>I\u2019m not here to &quot;karma whore&quot; or talk in circles just to get your upvotes. I\u2019d rather go to the X crowd, where I can gain real users and build real chaos.<p>I\u2019m moving to Slashdot. I hope you find your original spirit again, but until then, enjoy your polite, sterilized echo chamber. I\u2019m out.<p>Peace.","title":"HN is an echo chamber of AI wrappers. Moving to Slashdot","updated_at":"2026-02-18T03:46:07Z"}],"hitsPerPage":10,"nbHits":1291,"nbPages":100,"page":0,"params":"query=AI+agent+platform&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":45,"processingTimingsMS":{"_request":{"roundTrip":17},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":10,"scanning":33,"total":44},"total":45},"query":"AI agent platform","serverTimeMS":47}
