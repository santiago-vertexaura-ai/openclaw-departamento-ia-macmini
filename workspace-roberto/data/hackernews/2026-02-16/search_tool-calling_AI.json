{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"StanAngeloff"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["tool","calling","ai"],"value":"Hey HN, I posted Flemma back in October 2025 with no context. Since then I've shipped &gt;100 commits and used it daily as my primary <em>AI</em> workspace so I figured a proper update was due.<p>The core idea: a .chat file IS the conversation. No SQLite, no JSON logs, no shadow state. What you see in the buffer is exactly what the model receives. Edit an assistant reply to fix a hallucination, delete a tangent, fork by duplicating the file - it all works because there's nothing to fall out of sync.<p>What's new since October:<p>- <em>Tool calling</em>. Models can run shell commands, read/edit/write files (same as Pi, just 4 tools). Results go straight into the buffer. There's an approval flow (Ctrl-] cycles: preview -&gt; execute -&gt; send) so nothing runs without your say-so. Parallel tool use also works.<p>- Prompt caching for Anthropic, OpenAI and Vertex <em>AI</em>. Flemma places cache breakpoints automatically. Long conversations are now significantly cheaper (this was a major pain point for me).<p>- Extended thinking / reasoning support for all 3 providers.<p>- Per-buffer overrides via frontmatter. `flemma.opt` lets you pick which tools a buffer can use, set provider parameters, switch models - all scoped to that one file.<p>- Open registration APIs for both providers and tools. Custom tools can resolve definitions asynchronously from CLI subprocesses or remote APIs. I plan on adding mcporter support at some point.<p>Flemma works with Anthropic, OpenAI and Vertex <em>AI</em>. You get cost tracking, presets, Lua template expressions, file attachments and a lualine.nvim component.<p>One thing I want to be upfront about: nearly every line of code in Flemma was written by <em>AI</em> (Claude Code as of late, Amp and Aider in the past). It says so in the README. Every change was personally architected, reviewed and tested by me. I decide what gets built and I vet every diff. I think this is where a lot of software development is heading and I'd rather be honest about it than pretend otherwise.<p>I'm @StanAngeloff on GitHub - long-time Neovim user and open source enthusiast. Happy to answer questions.<p><a href=\"https://github.com/Flemma-Dev/flemma.nvim\" rel=\"nofollow\">https://github.com/Flemma-Dev/flemma.nvim</a>"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Flemma \u2013 a Neovim plugin where the .chat buffer is the conversation"}},"_tags":["story","author_StanAngeloff","story_47004647","show_hn"],"author":"StanAngeloff","created_at":"2026-02-13T16:38:25Z","created_at_i":1771000705,"num_comments":0,"objectID":"47004647","points":2,"story_id":47004647,"story_text":"Hey HN, I posted Flemma back in October 2025 with no context. Since then I&#x27;ve shipped &gt;100 commits and used it daily as my primary AI workspace so I figured a proper update was due.<p>The core idea: a .chat file IS the conversation. No SQLite, no JSON logs, no shadow state. What you see in the buffer is exactly what the model receives. Edit an assistant reply to fix a hallucination, delete a tangent, fork by duplicating the file - it all works because there&#x27;s nothing to fall out of sync.<p>What&#x27;s new since October:<p>- Tool calling. Models can run shell commands, read&#x2F;edit&#x2F;write files (same as Pi, just 4 tools). Results go straight into the buffer. There&#x27;s an approval flow (Ctrl-] cycles: preview -&gt; execute -&gt; send) so nothing runs without your say-so. Parallel tool use also works.<p>- Prompt caching for Anthropic, OpenAI and Vertex AI. Flemma places cache breakpoints automatically. Long conversations are now significantly cheaper (this was a major pain point for me).<p>- Extended thinking &#x2F; reasoning support for all 3 providers.<p>- Per-buffer overrides via frontmatter. `flemma.opt` lets you pick which tools a buffer can use, set provider parameters, switch models - all scoped to that one file.<p>- Open registration APIs for both providers and tools. Custom tools can resolve definitions asynchronously from CLI subprocesses or remote APIs. I plan on adding mcporter support at some point.<p>Flemma works with Anthropic, OpenAI and Vertex AI. You get cost tracking, presets, Lua template expressions, file attachments and a lualine.nvim component.<p>One thing I want to be upfront about: nearly every line of code in Flemma was written by AI (Claude Code as of late, Amp and Aider in the past). It says so in the README. Every change was personally architected, reviewed and tested by me. I decide what gets built and I vet every diff. I think this is where a lot of software development is heading and I&#x27;d rather be honest about it than pretend otherwise.<p>I&#x27;m @StanAngeloff on GitHub - long-time Neovim user and open source enthusiast. Happy to answer questions.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;Flemma-Dev&#x2F;flemma.nvim\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Flemma-Dev&#x2F;flemma.nvim</a>","title":"Show HN: Flemma \u2013 a Neovim plugin where the .chat buffer is the conversation","updated_at":"2026-02-13T17:39:09Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"sebringj"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["tool","calling","ai"],"value":"<em>AI</em> agents (Cursor, Claude computer-use, Copilot agent mode, etc.) have gotten stupidly good at spitting out code. Prompt \u2192 boom, clean code. The marketing says &quot;it just works.&quot;<p>It fucking doesn't.<p>You run it in a real app and immediately hit the same bullshit wall every time:\n- Hallucinated logic only reveals itself under real data or edge cases\n- UI updates magically forget to sync across devices (mobile \u2192 web = sad trombone)\n- API calls quietly return 401s or other crap that gets swallowed in some lazy try-catch\n- Vision-based agents crawl like molasses (2\u201310s per action) and torch tokens like it's free\n- Background pings and unrelated fetches make it impossible to tell what actually caused what<p>I tried pretty much everything out there and none of it quite scratched the itch I had: fast, structured, cross-platform runtime visibility without vision bloat or having to wire up a ton of hooks.<p>Quick rundown of the usual suspects:<p>- Pure vision/computer-use (Claude 3.5/4, ADEPT-style): zero setup, works on anything \u2014 but latency from hell and token burn is brutal for anything longer than a demo\n- Playwright / browser MCP servers: fast and structured for web \u2014 but web-only, selectors shatter like glass, no native mobile\n- Appium + vision hybrids: cross-platform on paper \u2014 but still vision-dependent and setup is a pain\n- Sandboxed agents (OpenHands, SWE-agent): decent for repo tasks and shell stuff \u2014 not so much for live app UI/network state\n- Explicit hooks/bridges: precise when you bother adding them \u2014 but requires code changes, which sucks<p>Couldn't find anything that gave me low-latency structured JSON state (UI elements, network, errors, logs) across platforms, local-first, without the usual trade-offs. So yeah, I got fed up and built a small local MCP server to solve it for myself.<p>Full disclosure: it's called Autonomo MCP https://github.com/sebringj/autonomo \u2014 very early, just launched.<p>I don't usually do this &quot;I built a thing&quot; thing \u2014 my open-source contributions are mostly small fixes and PRs \u2014 but I honestly couldn't see a better way in the current landscape.<p>It is my hope that Anthropic (or someone) will eventually ship a clean native solution for this. They already fixed BM25 <em>tool calling</em> to shrink context like crazy; I'd love to see them (or the industry) make runtime validation &quot;just work&quot; out of the box too.<p>Sometimes when you code in a vacuum you think your shit smells good. lmk if I'm off base here, I grew up with a mean grandpa so I'm cool with it."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Runtime validation is still fucked in <em>AI</em> coding agents"}},"_tags":["story","author_sebringj","story_46963340","ask_hn"],"author":"sebringj","children":[46963467],"created_at":"2026-02-10T17:22:50Z","created_at_i":1770744170,"num_comments":2,"objectID":"46963340","points":1,"story_id":46963340,"story_text":"AI agents (Cursor, Claude computer-use, Copilot agent mode, etc.) have gotten stupidly good at spitting out code. Prompt \u2192 boom, clean code. The marketing says &quot;it just works.&quot;<p>It fucking doesn&#x27;t.<p>You run it in a real app and immediately hit the same bullshit wall every time:\n- Hallucinated logic only reveals itself under real data or edge cases\n- UI updates magically forget to sync across devices (mobile \u2192 web = sad trombone)\n- API calls quietly return 401s or other crap that gets swallowed in some lazy try-catch\n- Vision-based agents crawl like molasses (2\u201310s per action) and torch tokens like it&#x27;s free\n- Background pings and unrelated fetches make it impossible to tell what actually caused what<p>I tried pretty much everything out there and none of it quite scratched the itch I had: fast, structured, cross-platform runtime visibility without vision bloat or having to wire up a ton of hooks.<p>Quick rundown of the usual suspects:<p>- Pure vision&#x2F;computer-use (Claude 3.5&#x2F;4, ADEPT-style): zero setup, works on anything \u2014 but latency from hell and token burn is brutal for anything longer than a demo\n- Playwright &#x2F; browser MCP servers: fast and structured for web \u2014 but web-only, selectors shatter like glass, no native mobile\n- Appium + vision hybrids: cross-platform on paper \u2014 but still vision-dependent and setup is a pain\n- Sandboxed agents (OpenHands, SWE-agent): decent for repo tasks and shell stuff \u2014 not so much for live app UI&#x2F;network state\n- Explicit hooks&#x2F;bridges: precise when you bother adding them \u2014 but requires code changes, which sucks<p>Couldn&#x27;t find anything that gave me low-latency structured JSON state (UI elements, network, errors, logs) across platforms, local-first, without the usual trade-offs. So yeah, I got fed up and built a small local MCP server to solve it for myself.<p>Full disclosure: it&#x27;s called Autonomo MCP https:&#x2F;&#x2F;github.com&#x2F;sebringj&#x2F;autonomo \u2014 very early, just launched.<p>I don&#x27;t usually do this &quot;I built a thing&quot; thing \u2014 my open-source contributions are mostly small fixes and PRs \u2014 but I honestly couldn&#x27;t see a better way in the current landscape.<p>It is my hope that Anthropic (or someone) will eventually ship a clean native solution for this. They already fixed BM25 tool calling to shrink context like crazy; I&#x27;d love to see them (or the industry) make runtime validation &quot;just work&quot; out of the box too.<p>Sometimes when you code in a vacuum you think your shit smells good. lmk if I&#x27;m off base here, I grew up with a mean grandpa so I&#x27;m cool with it.","title":"Runtime validation is still fucked in AI coding agents","updated_at":"2026-02-10T17:53:41Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ankit219"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["tool","calling","ai"],"value":"GitHub: <a href=\"https://github.com/ClioAI/kw-sdk\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk</a><p>Most <em>AI</em> agent frameworks target code. Write code, run tests, fix errors, repeat. That works because code has a natural verification signal. It works or it doesn't.<p>This SDK treats knowledge work like an engineering problem:<p>Task \u2192 Brief \u2192 Rubric (hidden from executor) \u2192 Work \u2192 Verify \u2192 Fail? \u2192 Retry \u2192 Pass \u2192 Submit<p>The orchestrator coordinates subagents, web search, code execution, and file I/O. then checks its own work against criteria it can't game (the rubric is generated in a separate call and the executor never sees it directly).<p>We originally built this as a harness for RL training on knowledge tasks. The rubric is the reward function. If you're training models on knowledge work, the brief\u2192rubric\u2192execute\u2192verify loop gives you a structured reward signal for tasks that normally don't have one.<p>What makes Knowledge work different from code? (apart from feedback loop)\nI believe there is some functionality missing from today's agents when it comes to knowledge work. I tried to include that in this release. Example:<p>Explore mode: Mapping the solution space, identifying the set level gaps, and giving options.<p>Most agents optimize for a single answer, and end up with a median one. For strategy, design, creative problems, you want to see the options, what are the tradeoffs, and what can you do? Explore mode generates N distinct approaches, each with explicit assumptions and counterfactuals (&quot;this works if X, breaks if Y&quot;). The output ends with set-level gaps ie what angles the entire set missed. The gaps are often more valuable than the takes. I think this is what many of us do on a daily basis, but no agent directly captures it today. See <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/explore_mode.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/explore_...</a> and the output for a sense of how this is different.<p>Checkpointing: With many <em>ai</em> agents and especially multi agent systems, i can see where it went wrong, but cant run inference from same stage. (or you may want multiple explorations once an agent has done some tasks like search and is now looking at ideas). I used this for rollouts a lot, and think its a great feature to run again, or fork from a specific checkpoint.<p>A note on Verification loop:\nThe verify step is where the real leverage is. A model that can accurately assess its own work against a rubric is more valuable than one that generates slightly better first drafts. The rubric makes quality legible \u2014 to the agent, to the human, and potentially to a training signal.<p>Some things i like about this: \n- You can pass a remote execution environment (including your browser as a sandbox) and it would work. It can be docker, e2b, your local env, anything, the model will execute commands in your context, and will iterate based on feedback loop. Code execution is a protocol here.<p>- <em>Tool calling</em>: I realize you don't need complex functions. Models are good at writing terminal code, and can iterate based on feedback, so you can just pass either functions in context and model will execute or you can pass docs and model will write the code. (same as anthropic's programmatic <em>tool calling</em>). Details: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/TOOL_CALLING_GUIDE.md\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/<em>TOOL_CALLING</em>_GUID...</a><p>Lastly, some guides: \n- SDK guide: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/SDK_GUIDE.md\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/SDK_GUIDE.md</a>\n- Extensible. See bizarro example where i add a new mode: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/custom_mode_bizarro.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/custom_m...</a>\n- working with files: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/with_files.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/with_fil...</a> \n- this is simple but i love the csv example: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/csv_research_and_calc.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/csv_rese...</a>\n- remote execution: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/with_custom_executor.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/with_cus...</a><p>And a lot more. This was completely refactored by opus and given the rework, probably would have taken a lot of time to release it.<p>MIT licensed. Would love your feedback."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Show HN: Open-Source SDK for <em>AI</em> Knowledge Work"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/ClioAI/kw-sdk"}},"_tags":["story","author_ankit219","story_46963026","show_hn"],"author":"ankit219","children":[46963327],"created_at":"2026-02-10T17:06:00Z","created_at_i":1770743160,"num_comments":1,"objectID":"46963026","points":21,"story_id":46963026,"story_text":"GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk</a><p>Most AI agent frameworks target code. Write code, run tests, fix errors, repeat. That works because code has a natural verification signal. It works or it doesn&#x27;t.<p>This SDK treats knowledge work like an engineering problem:<p>Task \u2192 Brief \u2192 Rubric (hidden from executor) \u2192 Work \u2192 Verify \u2192 Fail? \u2192 Retry \u2192 Pass \u2192 Submit<p>The orchestrator coordinates subagents, web search, code execution, and file I&#x2F;O. then checks its own work against criteria it can&#x27;t game (the rubric is generated in a separate call and the executor never sees it directly).<p>We originally built this as a harness for RL training on knowledge tasks. The rubric is the reward function. If you&#x27;re training models on knowledge work, the brief\u2192rubric\u2192execute\u2192verify loop gives you a structured reward signal for tasks that normally don&#x27;t have one.<p>What makes Knowledge work different from code? (apart from feedback loop)\nI believe there is some functionality missing from today&#x27;s agents when it comes to knowledge work. I tried to include that in this release. Example:<p>Explore mode: Mapping the solution space, identifying the set level gaps, and giving options.<p>Most agents optimize for a single answer, and end up with a median one. For strategy, design, creative problems, you want to see the options, what are the tradeoffs, and what can you do? Explore mode generates N distinct approaches, each with explicit assumptions and counterfactuals (&quot;this works if X, breaks if Y&quot;). The output ends with set-level gaps ie what angles the entire set missed. The gaps are often more valuable than the takes. I think this is what many of us do on a daily basis, but no agent directly captures it today. See <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;explore_mode.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;explore_...</a> and the output for a sense of how this is different.<p>Checkpointing: With many ai agents and especially multi agent systems, i can see where it went wrong, but cant run inference from same stage. (or you may want multiple explorations once an agent has done some tasks like search and is now looking at ideas). I used this for rollouts a lot, and think its a great feature to run again, or fork from a specific checkpoint.<p>A note on Verification loop:\nThe verify step is where the real leverage is. A model that can accurately assess its own work against a rubric is more valuable than one that generates slightly better first drafts. The rubric makes quality legible \u2014 to the agent, to the human, and potentially to a training signal.<p>Some things i like about this: \n- You can pass a remote execution environment (including your browser as a sandbox) and it would work. It can be docker, e2b, your local env, anything, the model will execute commands in your context, and will iterate based on feedback loop. Code execution is a protocol here.<p>- Tool calling: I realize you don&#x27;t need complex functions. Models are good at writing terminal code, and can iterate based on feedback, so you can just pass either functions in context and model will execute or you can pass docs and model will write the code. (same as anthropic&#x27;s programmatic tool calling). Details: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;TOOL_CALLING_GUIDE.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;TOOL_CALLING_GUID...</a><p>Lastly, some guides: \n- SDK guide: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;SDK_GUIDE.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;SDK_GUIDE.md</a>\n- Extensible. See bizarro example where i add a new mode: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;custom_mode_bizarro.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;custom_m...</a>\n- working with files: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_files.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_fil...</a> \n- this is simple but i love the csv example: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;csv_research_and_calc.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;csv_rese...</a>\n- remote execution: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_custom_executor.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_cus...</a><p>And a lot more. This was completely refactored by opus and given the rework, probably would have taken a lot of time to release it.<p>MIT licensed. Would love your feedback.","title":"Show HN: Open-Source SDK for AI Knowledge Work","updated_at":"2026-02-11T15:35:15Z","url":"https://github.com/ClioAI/kw-sdk"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"samrith"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["tool","calling","ai"],"value":"Hi HN,<p>I'm Samrith, the founder of Hyperterse.<p>We built Hyperterse because I kept running into the same problem when building <em>AI</em> agents: safely connecting databases to LLMs requires writing endless API endpoints, managing boilerplate validation, and building custom integrations for each <em>AI</em> framework, build custom MCP integrations for each <em>AI</em> framework, or worry about exposing SQL or connection strings to clients.<p>We built a runtime server that consolidates this into a single declarative configuration file. You define your queries once, and Hyperterse automatically generates typed REST endpoints, MCP tools, OpenAPI specs, and LLM-friendly documentation.<p>Instead of writing API endpoints, you define queries in a simple config file. Hyperterse handles:<p>- Query Definition: Write queries once with typed inputs<p>- Endpoint Generation: Each query becomes a REST endpoint<p>- MCP Integration: Queries are automatically exposed as MCP tools for <em>AI</em> agents<p>- Documentation: OpenAPI 3.0 specs and LLM-readable docs generated automatically<p>- Security: SQL and connection strings stay server-side\u2014never exposed to clients<p>Here's what a typical query looks like. This replaces about 500-1000 lines of boilerplate API code:<p>adapters:<p><pre><code>  my_db:\n\n    connector: postgres\n\n    connection_string: &quot;postgresql://user:pass@localhost:5432/db&quot;\n</code></pre>\nqueries:<p><pre><code>  get-user:\n\n    use: my_db\n\n    description: &quot;Retrieve a user by email&quot;\n\n    statement: |\n\n      SELECT id, name, email, created_at\n\n      FROM users\n\n      WHERE email = {{ inputs.email }}\n\n    inputs:\n\n      email:\n\n        type: string\n\n        description: &quot;User email address&quot;\n</code></pre>\nSupports PostgreSQL, MySQL, and Redis (at the moment, more connectors coming), hot reloading in dev mode, type-safe input validation (string, int, float, boolean, datetime), self-contained runtime\u2014deploy anywhere, no ORMs or query builders required.<p>You can use Hyperterse for a variety of use cases, with the most prominent being:<p>- <em>AI</em> agents and LLM <em>tool calling</em><p>- RAG applications with structured database queries<p>- Rapid API prototyping<p>- Multi-agent systems sharing database access"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Hyperterse \u2013 a super fast runtime to connect your data to your agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/hyperterse/hyperterse"}},"_tags":["story","author_samrith","story_46812366","show_hn"],"author":"samrith","children":[46825058],"created_at":"2026-01-29T16:24:47Z","created_at_i":1769703887,"num_comments":1,"objectID":"46812366","points":1,"story_id":46812366,"story_text":"Hi HN,<p>I&#x27;m Samrith, the founder of Hyperterse.<p>We built Hyperterse because I kept running into the same problem when building AI agents: safely connecting databases to LLMs requires writing endless API endpoints, managing boilerplate validation, and building custom integrations for each AI framework, build custom MCP integrations for each AI framework, or worry about exposing SQL or connection strings to clients.<p>We built a runtime server that consolidates this into a single declarative configuration file. You define your queries once, and Hyperterse automatically generates typed REST endpoints, MCP tools, OpenAPI specs, and LLM-friendly documentation.<p>Instead of writing API endpoints, you define queries in a simple config file. Hyperterse handles:<p>- Query Definition: Write queries once with typed inputs<p>- Endpoint Generation: Each query becomes a REST endpoint<p>- MCP Integration: Queries are automatically exposed as MCP tools for AI agents<p>- Documentation: OpenAPI 3.0 specs and LLM-readable docs generated automatically<p>- Security: SQL and connection strings stay server-side\u2014never exposed to clients<p>Here&#x27;s what a typical query looks like. This replaces about 500-1000 lines of boilerplate API code:<p>adapters:<p><pre><code>  my_db:\n\n    connector: postgres\n\n    connection_string: &quot;postgresql:&#x2F;&#x2F;user:pass@localhost:5432&#x2F;db&quot;\n</code></pre>\nqueries:<p><pre><code>  get-user:\n\n    use: my_db\n\n    description: &quot;Retrieve a user by email&quot;\n\n    statement: |\n\n      SELECT id, name, email, created_at\n\n      FROM users\n\n      WHERE email = {{ inputs.email }}\n\n    inputs:\n\n      email:\n\n        type: string\n\n        description: &quot;User email address&quot;\n</code></pre>\nSupports PostgreSQL, MySQL, and Redis (at the moment, more connectors coming), hot reloading in dev mode, type-safe input validation (string, int, float, boolean, datetime), self-contained runtime\u2014deploy anywhere, no ORMs or query builders required.<p>You can use Hyperterse for a variety of use cases, with the most prominent being:<p>- AI agents and LLM tool calling<p>- RAG applications with structured database queries<p>- Rapid API prototyping<p>- Multi-agent systems sharing database access","title":"Show HN: Hyperterse \u2013 a super fast runtime to connect your data to your agents","updated_at":"2026-01-30T23:27:41Z","url":"https://github.com/hyperterse/hyperterse"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"SilasYee"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["tool","calling","ai"],"value":"Alibaba has officially launched Qwen3-Max-Thinking, a trillion-parameter MoE flagship LLM pretrained on 36T tokens\u2014double the corpus of Qwen 2.5\u2014and it\u2019s already matching or outperforming top-tier models like GPT-5.2-Thinking, Claude-Opus-4.5, and Gemini 3 Pro across 19 authoritative benchmarks. Its two core technical breakthroughs are what truly set it apart.<p>First, Adaptive <em>Tool Calling</em>: No manual prompts are needed\u2014it autonomously invokes search engines, memory tools, and code interpreters based on task demands. This cuts down on hallucinations and boosts real-time problem-solving; for instance, coding tasks trigger automatic error correction loops, while research tasks combine search with context synthesis. Second, Test-Time Scaling (TTS): It outperforms standard parallel sampling by refining reasoning through iterative insights, with measurable jumps in key benchmarks\u2014GPQA rose from 90.3 to 92.8, LiveCodeBench v6 hit 91.4 from 88.0, and IMO-AnswerBench climbed to 91.5 from 89.5.<p>Notably, its preview version even achieved 100% accuracy in tough math contests like <em>AI</em>ME 25 and HMMT 25. The model runs smoothly on web/desktop demos, and its API is production-ready with adjustable thinking budgets (up to 80K tokens by default) to balance depth and speed. This isn\u2019t just an incremental update\u2014it\u2019s a leap that closes the gap in reasoning and tool integration for real-world academic and engineering tasks.<p>Check it out: https://chat.qwen.<em>ai</em>/"},"title":{"matchLevel":"none","matchedWords":[],"value":"Qwen3-Max-Thinking Drops: 36T Tokens"}},"_tags":["story","author_SilasYee","story_46769281","ask_hn"],"author":"SilasYee","children":[46769582,46769403],"created_at":"2026-01-26T18:10:37Z","created_at_i":1769451037,"num_comments":2,"objectID":"46769281","points":4,"story_id":46769281,"story_text":"Alibaba has officially launched Qwen3-Max-Thinking, a trillion-parameter MoE flagship LLM pretrained on 36T tokens\u2014double the corpus of Qwen 2.5\u2014and it\u2019s already matching or outperforming top-tier models like GPT-5.2-Thinking, Claude-Opus-4.5, and Gemini 3 Pro across 19 authoritative benchmarks. Its two core technical breakthroughs are what truly set it apart.<p>First, Adaptive Tool Calling: No manual prompts are needed\u2014it autonomously invokes search engines, memory tools, and code interpreters based on task demands. This cuts down on hallucinations and boosts real-time problem-solving; for instance, coding tasks trigger automatic error correction loops, while research tasks combine search with context synthesis. Second, Test-Time Scaling (TTS): It outperforms standard parallel sampling by refining reasoning through iterative insights, with measurable jumps in key benchmarks\u2014GPQA rose from 90.3 to 92.8, LiveCodeBench v6 hit 91.4 from 88.0, and IMO-AnswerBench climbed to 91.5 from 89.5.<p>Notably, its preview version even achieved 100% accuracy in tough math contests like AIME 25 and HMMT 25. The model runs smoothly on web&#x2F;desktop demos, and its API is production-ready with adjustable thinking budgets (up to 80K tokens by default) to balance depth and speed. This isn\u2019t just an incremental update\u2014it\u2019s a leap that closes the gap in reasoning and tool integration for real-world academic and engineering tasks.<p>Check it out: https:&#x2F;&#x2F;chat.qwen.ai&#x2F;","title":"Qwen3-Max-Thinking Drops: 36T Tokens","updated_at":"2026-01-29T16:10:37Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"loffloff"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["tool","calling","ai"],"value":"I built a React hook that makes it easy to add real-time <em>AI</em> conversations with screen sharing to any app.<p>GitHub: <a href=\"https://github.com/loffloff/gemini-live-react\" rel=\"nofollow\">https://github.com/loffloff/gemini-live-react</a><p>What it does:                                                                                                                                                    \n- Stream mic audio to Gemini, get voice responses back                                                                                                           \n- Share your screen so <em>AI</em> can see what you're doing                                                                                                              \n- Real-time transcripts for both sides                                                                                                                           \n- <em>Tool calling</em> support<p>Just added: useCapturedSurfaceControl hook for Chrome 124+ that lets you programmatically scroll/zoom a captured tab without injecting scripts.<p>Built this for deflectionrate.com (<em>AI</em> support that resolves issues before they become tickets), extracted the core into this package.<p>npm install gemini-live-react<p>Happy to answer questions.<p>Want me to tweak anything?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: React hook for Gemini Live API \u2013 real-time voice and screen sharing"}},"_tags":["story","author_loffloff","story_46654284","show_hn"],"author":"loffloff","created_at":"2026-01-17T01:02:26Z","created_at_i":1768611746,"num_comments":0,"objectID":"46654284","points":1,"story_id":46654284,"story_text":"I built a React hook that makes it easy to add real-time AI conversations with screen sharing to any app.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;loffloff&#x2F;gemini-live-react\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;loffloff&#x2F;gemini-live-react</a><p>What it does:                                                                                                                                                    \n- Stream mic audio to Gemini, get voice responses back                                                                                                           \n- Share your screen so AI can see what you&#x27;re doing                                                                                                              \n- Real-time transcripts for both sides                                                                                                                           \n- Tool calling support<p>Just added: useCapturedSurfaceControl hook for Chrome 124+ that lets you programmatically scroll&#x2F;zoom a captured tab without injecting scripts.<p>Built this for deflectionrate.com (AI support that resolves issues before they become tickets), extracted the core into this package.<p>npm install gemini-live-react<p>Happy to answer questions.<p>Want me to tweak anything?","title":"Show HN: React hook for Gemini Live API \u2013 real-time voice and screen sharing","updated_at":"2026-01-17T01:03:18Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"loffloff"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["tool","calling","ai"],"value":"I built a React hook for real-time voice conversations with Google's Gemini Live API and I'm confused why more people aren't doing this.<p>Gemini Live is capable - full-duplex audio (interrupt the <em>AI</em> mid-sentence), screen sharing so the <em>AI</em> sees what you're looking at, <em>tool calling</em>, built-in VAD. But using it from a browser is painful:<p>- Browser audio is 48kHz, Gemini wants 16kHz in and sends 24kHz out                                                                                              \n- PCM16 endianness conversions                                                                                                                                   \n- Buffer management to avoid clicks and gaps                                                                                                                     \n- Keeping your API key out of client code<p>So I wrapped it into a single hook:<p>const { connect, transcripts, isConnected } = useGeminiLive({                                                                                                    \n  proxyUrl: 'wss://your-project.supabase.co/functions/v1/gemini-live-proxy'                                                                                      \n});<p>Includes a Supabase Edge Function proxy, screen sharing, auto-reconnection, real-time transcription, full TypeScript.<p>GitHub: <a href=\"https://github.com/loffloff/gemini-live-react\" rel=\"nofollow\">https://github.com/loffloff/gemini-live-react</a><p>Everyone builds voice <em>AI</em> with OpenAI's Realtime API, but Gemini Live is cheaper and screen sharing is underrated. Am I missing something?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Why is nobody using this? Full-duplex voice streaming with Gemini Live in React"}},"_tags":["story","author_loffloff","story_46650679","ask_hn"],"author":"loffloff","children":[46655092],"created_at":"2026-01-16T19:08:32Z","created_at_i":1768590512,"num_comments":1,"objectID":"46650679","points":3,"story_id":46650679,"story_text":"I built a React hook for real-time voice conversations with Google&#x27;s Gemini Live API and I&#x27;m confused why more people aren&#x27;t doing this.<p>Gemini Live is capable - full-duplex audio (interrupt the AI mid-sentence), screen sharing so the AI sees what you&#x27;re looking at, tool calling, built-in VAD. But using it from a browser is painful:<p>- Browser audio is 48kHz, Gemini wants 16kHz in and sends 24kHz out                                                                                              \n- PCM16 endianness conversions                                                                                                                                   \n- Buffer management to avoid clicks and gaps                                                                                                                     \n- Keeping your API key out of client code<p>So I wrapped it into a single hook:<p>const { connect, transcripts, isConnected } = useGeminiLive({                                                                                                    \n  proxyUrl: &#x27;wss:&#x2F;&#x2F;your-project.supabase.co&#x2F;functions&#x2F;v1&#x2F;gemini-live-proxy&#x27;                                                                                      \n});<p>Includes a Supabase Edge Function proxy, screen sharing, auto-reconnection, real-time transcription, full TypeScript.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;loffloff&#x2F;gemini-live-react\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;loffloff&#x2F;gemini-live-react</a><p>Everyone builds voice AI with OpenAI&#x27;s Realtime API, but Gemini Live is cheaper and screen sharing is underrated. Am I missing something?","title":"Why is nobody using this? Full-duplex voice streaming with Gemini Live in React","updated_at":"2026-01-17T03:58:48Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"randall"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["tool","calling"],"value":"Hey HN!<p>Wanted to show our open source agent harness called Gambit.<p>If you\u2019re not familiar, agent harnesses are sort of like an operating system for an agent... they handle <em>tool calling</em>, planning, context window management, and don\u2019t require as much developer orchestration.<p>Normally you might see an agent orchestration framework pipeline like:<p>compute -&gt; compute -&gt; compute -&gt; LLM -&gt; compute -&gt; compute -&gt; LLM<p>we invert this so with an agent harness, it\u2019s more like:<p>LLM -&gt; LLM -&gt; LLM -&gt; compute -&gt; LLM -&gt; LLM -&gt; compute -&gt; LLM<p>Essentially you describe each agent in either a self contained markdown file, or as a typescript program. Your root agent can bring in other agents as needed, and we create a typesafe way for you to define the interfaces between those agents. We call these decks.<p>Agents can call agents, and each agent can be designed with whatever model params make sense for your task.<p>Additionally, each step of the chain gets automatic evals, we call graders. A grader is another deck type\u2026 but it\u2019s designed to evaluate and score conversations (or individual conversation turns).<p>We also have test agents you can define on a deck-by-deck basis, that are designed to mimic scenarios your agent would face and generate synthetic data for either humans or graders to grade.<p>Prior to Gambit, we had built an LLM based video editor, and we weren\u2019t happy with the results, which is what brought us down this path of improving inference time LLM quality.<p>We know it\u2019s missing some obvious parts, but we wanted to get this out there to see how it could help people or start conversations. We\u2019re really happy with how it\u2019s working with some of our early design partners, and we think it\u2019s a way to implement a lot of interesting applications:<p>- Truly open source agents and assistants, where logic, code, and prompts can be easily shared with the community.<p>- Rubric based grading to guarantee you (for instance) don\u2019t leak PII accidentally<p>- Spin up a usable bot in minutes and have Codex or Claude Code use our command line runner / graders to build a first version that is pretty good w/ very little human intervention.<p>We\u2019ll be around if ya\u2019ll have any questions or thoughts. Thanks for checking us out!<p>Walkthrough video: <a href=\"https://youtu.be/J_hQ2L_yy60\" rel=\"nofollow\">https://youtu.be/J_hQ2L_yy60</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Show HN: Gambit, an open-source agent harness for building reliable <em>AI</em> agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/bolt-foundry/gambit"}},"_tags":["story","author_randall","story_46641362","show_hn"],"author":"randall","children":[46658241,46681251,46648780,46642428,46685068,46656955,46659076,46642393,46655546,46643710,46643407],"created_at":"2026-01-16T00:13:25Z","created_at_i":1768522405,"num_comments":27,"objectID":"46641362","points":91,"story_id":46641362,"story_text":"Hey HN!<p>Wanted to show our open source agent harness called Gambit.<p>If you\u2019re not familiar, agent harnesses are sort of like an operating system for an agent... they handle tool calling, planning, context window management, and don\u2019t require as much developer orchestration.<p>Normally you might see an agent orchestration framework pipeline like:<p>compute -&gt; compute -&gt; compute -&gt; LLM -&gt; compute -&gt; compute -&gt; LLM<p>we invert this so with an agent harness, it\u2019s more like:<p>LLM -&gt; LLM -&gt; LLM -&gt; compute -&gt; LLM -&gt; LLM -&gt; compute -&gt; LLM<p>Essentially you describe each agent in either a self contained markdown file, or as a typescript program. Your root agent can bring in other agents as needed, and we create a typesafe way for you to define the interfaces between those agents. We call these decks.<p>Agents can call agents, and each agent can be designed with whatever model params make sense for your task.<p>Additionally, each step of the chain gets automatic evals, we call graders. A grader is another deck type\u2026 but it\u2019s designed to evaluate and score conversations (or individual conversation turns).<p>We also have test agents you can define on a deck-by-deck basis, that are designed to mimic scenarios your agent would face and generate synthetic data for either humans or graders to grade.<p>Prior to Gambit, we had built an LLM based video editor, and we weren\u2019t happy with the results, which is what brought us down this path of improving inference time LLM quality.<p>We know it\u2019s missing some obvious parts, but we wanted to get this out there to see how it could help people or start conversations. We\u2019re really happy with how it\u2019s working with some of our early design partners, and we think it\u2019s a way to implement a lot of interesting applications:<p>- Truly open source agents and assistants, where logic, code, and prompts can be easily shared with the community.<p>- Rubric based grading to guarantee you (for instance) don\u2019t leak PII accidentally<p>- Spin up a usable bot in minutes and have Codex or Claude Code use our command line runner &#x2F; graders to build a first version that is pretty good w&#x2F; very little human intervention.<p>We\u2019ll be around if ya\u2019ll have any questions or thoughts. Thanks for checking us out!<p>Walkthrough video: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;J_hQ2L_yy60\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;J_hQ2L_yy60</a>","title":"Show HN: Gambit, an open-source agent harness for building reliable AI agents","updated_at":"2026-01-27T02:48:26Z","url":"https://github.com/bolt-foundry/gambit"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"weli"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["tool","calling","ai"],"value":"Our use case is not uncommon, we are developing tools so that people can install LLM's on their e-commerces.<p>But there are some interesting challenges that I feel can't be solved unless inference providers allow us to include the concept additional entities in a conversation.<p>As far as I know the three most basic ones shared alongside all providers are:<p>- System<p>- Assistant<p>- User<p>That's fine and it allows for simple conversational-based approaches (ChatGPT, Claude, Gemini, etc). However in our use case we allow our customers (not the final user who is talking with the <em>AI</em>) to configure the <em>AI</em> in different ways (personality, RAG, etc), which poses a problem.<p>If we inject those customer settings in the System prompt then that's a risk because there might be conflicting prompts with our internal rules. So the easiest option is to &quot;clean&quot; the customer prompts before injecting them, but that feels hacky and just adds one more level of indirection. Cleaning the prompt and injecting it with common patterns like XML tags seems to help a bit but still feels extremely risky for some reason.<p>Injecting it in the assistant or user also seems flaky and prone to prompt injection.<p>Creating a fake tool call and result like &quot;getPersonalityConfiguration&quot; seems to work the best, from our testing it is treated as something between the System and Assistant roles. And our top system prompt rules are still respected while allowing the customer some freedom to configure the <em>AI</em>.<p>The problem comes when you need to add more parties to what essentially is a 2 entity conversation. Sometimes we want external agents to chime in a conversation (via subagents or other methods) and there is no good way to do that AFAIK. It gets the occasional confusion and starts mixing up who is who.<p>One of our typical scenarios that we need to model:<p>System: Your rules are: You will never use foul language...<p>Store owner: You are John the customer agent for store Foo...<p>User: Do you have snowboards in stock?<p>Assistant-&gt;User: Let me check with the team. I'll get back to you soon.<p>System-&gt;Team: User is asking if we have snowboards in stock. Do we?<p>Team: We do have snowboards in stock.<p>Team-&gt;User: Yes we do have snowboards in stock!<p>User: Perfect, if I buy them will the courier send it to my country? [country name].<p>Assistant-&gt;User: Let me check, I need to see if our courier can ship a snowboard to your country.<p>Assistant-&gt;Third party logistics: I have a user from [country] interested in buying a snowboard. The dimensions are X by Y and the weight is Z. We would send it from our logistics center located at [address].<p>Third party logistics -&gt; Assistant: Yes we can do it, it will be 29.99 for the shipping.<p>Assistant-&gt;User: Yes they can ship it to [country] but it does incur in 29.99 extra charge...<p>I obviated tool calls and responses, but that's basically the gist of it. Spawning sub-agents that have the context of the main conversation works but at some point it is limiting (we need to copy all personality traits and relevant information via summarization or injecting the conversation in a manner that the sub-agent won't get confused). It feels like an anti-pattern and trying to fight the intended use case of LLM's, which seems to be focused in conversation between two entities with the occasional external information going in through System or <em>tool calling</em>.<p>It would be amazing if we could add custom roles to model messages, still with special cases like agent or assistant.<p>Has anyone worked with similar problems? How did you solve it? Is this solved in the model lab or at the inference provider level (post-training)?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: How to overcome the limit of roles in LLM's"}},"_tags":["story","author_weli","story_46634915","ask_hn"],"author":"weli","children":[46664392],"created_at":"2026-01-15T16:23:35Z","created_at_i":1768494215,"num_comments":1,"objectID":"46634915","points":2,"story_id":46634915,"story_text":"Our use case is not uncommon, we are developing tools so that people can install LLM&#x27;s on their e-commerces.<p>But there are some interesting challenges that I feel can&#x27;t be solved unless inference providers allow us to include the concept additional entities in a conversation.<p>As far as I know the three most basic ones shared alongside all providers are:<p>- System<p>- Assistant<p>- User<p>That&#x27;s fine and it allows for simple conversational-based approaches (ChatGPT, Claude, Gemini, etc). However in our use case we allow our customers (not the final user who is talking with the AI) to configure the AI in different ways (personality, RAG, etc), which poses a problem.<p>If we inject those customer settings in the System prompt then that&#x27;s a risk because there might be conflicting prompts with our internal rules. So the easiest option is to &quot;clean&quot; the customer prompts before injecting them, but that feels hacky and just adds one more level of indirection. Cleaning the prompt and injecting it with common patterns like XML tags seems to help a bit but still feels extremely risky for some reason.<p>Injecting it in the assistant or user also seems flaky and prone to prompt injection.<p>Creating a fake tool call and result like &quot;getPersonalityConfiguration&quot; seems to work the best, from our testing it is treated as something between the System and Assistant roles. And our top system prompt rules are still respected while allowing the customer some freedom to configure the AI.<p>The problem comes when you need to add more parties to what essentially is a 2 entity conversation. Sometimes we want external agents to chime in a conversation (via subagents or other methods) and there is no good way to do that AFAIK. It gets the occasional confusion and starts mixing up who is who.<p>One of our typical scenarios that we need to model:<p>System: Your rules are: You will never use foul language...<p>Store owner: You are John the customer agent for store Foo...<p>User: Do you have snowboards in stock?<p>Assistant-&gt;User: Let me check with the team. I&#x27;ll get back to you soon.<p>System-&gt;Team: User is asking if we have snowboards in stock. Do we?<p>Team: We do have snowboards in stock.<p>Team-&gt;User: Yes we do have snowboards in stock!<p>User: Perfect, if I buy them will the courier send it to my country? [country name].<p>Assistant-&gt;User: Let me check, I need to see if our courier can ship a snowboard to your country.<p>Assistant-&gt;Third party logistics: I have a user from [country] interested in buying a snowboard. The dimensions are X by Y and the weight is Z. We would send it from our logistics center located at [address].<p>Third party logistics -&gt; Assistant: Yes we can do it, it will be 29.99 for the shipping.<p>Assistant-&gt;User: Yes they can ship it to [country] but it does incur in 29.99 extra charge...<p>I obviated tool calls and responses, but that&#x27;s basically the gist of it. Spawning sub-agents that have the context of the main conversation works but at some point it is limiting (we need to copy all personality traits and relevant information via summarization or injecting the conversation in a manner that the sub-agent won&#x27;t get confused). It feels like an anti-pattern and trying to fight the intended use case of LLM&#x27;s, which seems to be focused in conversation between two entities with the occasional external information going in through System or tool calling.<p>It would be amazing if we could add custom roles to model messages, still with special cases like agent or assistant.<p>Has anyone worked with similar problems? How did you solve it? Is this solved in the model lab or at the inference provider level (post-training)?","title":"Ask HN: How to overcome the limit of roles in LLM's","updated_at":"2026-01-18T02:49:23Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mjomaa"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["tool","calling","ai"],"value":"Hey HN,<p>I've been building SaaS apps for 12 years and got tired of rebuilding the same things every time. Not only for products, but also internal tools in our organization.<p>Things like admin panels, billing/credits, multi-organization support and even marketing pages are still surprisingly annoying and time-consuming to get right. Having these solved upfront makes a huge difference and you can use your Claude tokens for features right away.<p>What's in it:<p>- Authentication (Better Auth) - email/password, social, MFA, account linking, profile management, session management<p>- Multi-tenancy - organizations, invitations, roles, ownership transfer<p>- Billing \u2013 subscriptions, per-seat pricing, lifetime purchases,  trials, huge amount of webhooks supported<p>- Credits \u2013 usage-based billing for <em>AI</em> features<p>- Admin panel \u2013 user management, impersonation, ban user, manual email verification, billing sync<p>- <em>AI</em> chatbot \u2013 Vercel <em>AI</em> SDK with streaming and <em>tool calling</em><p>- Marketing pages \u2013 landing, pricing, blog, docs (fumadocs), changelog, careers, about, legal pages<p>- Emails - all the email templates ranging from &quot;forgot password&quot; to &quot;your trial has ended&quot;, etc.<p>Tech stack:<p>- Next.js 16 and React 19<p>- tRPC for end-to-end type safety<p>- Better Auth for authentication<p>- Prisma or Drizzle ORM (two separate versions)<p>- Tailwind CSS + shadcn/ui<p>- TypeScript with strict mode<p>Two ORM?<p>It's two versions, not at the same time. Prisma and Drizzle. Both versions have feature parity - pick what fits your workflow.<p>Pricing:<p>One-time purchase with lifetime team access. Use it for unlimited projects including client work.<p>Happy to answer any questions about the architecture, design decisions or specific features.<p>- Website: <a href=\"https://www.achromatic.dev\" rel=\"nofollow\">https://www.achromatic.dev</a><p>- Demo: <a href=\"https://demo.achromatic.dev\" rel=\"nofollow\">https://demo.achromatic.dev</a><p>I\u2019m planning to add TanStack-based variants and more opinionated, use-case specific starter kit included in the same license, such as:<p>- CRM starter kit<p>- Workflow builder starter kit<p>- Support/helpdesk starter kit<p>- etc.<p>Happy to hear feedback or answer questions."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Show HN: Achromatic \u2013 <em>AI</em> Ready Next.js 16 Starter Kit"}},"_tags":["story","author_mjomaa","story_46621156","show_hn"],"author":"mjomaa","children":[46661964],"created_at":"2026-01-14T19:14:34Z","created_at_i":1768418074,"num_comments":1,"objectID":"46621156","points":1,"story_id":46621156,"story_text":"Hey HN,<p>I&#x27;ve been building SaaS apps for 12 years and got tired of rebuilding the same things every time. Not only for products, but also internal tools in our organization.<p>Things like admin panels, billing&#x2F;credits, multi-organization support and even marketing pages are still surprisingly annoying and time-consuming to get right. Having these solved upfront makes a huge difference and you can use your Claude tokens for features right away.<p>What&#x27;s in it:<p>- Authentication (Better Auth) - email&#x2F;password, social, MFA, account linking, profile management, session management<p>- Multi-tenancy - organizations, invitations, roles, ownership transfer<p>- Billing \u2013 subscriptions, per-seat pricing, lifetime purchases,  trials, huge amount of webhooks supported<p>- Credits \u2013 usage-based billing for AI features<p>- Admin panel \u2013 user management, impersonation, ban user, manual email verification, billing sync<p>- AI chatbot \u2013 Vercel AI SDK with streaming and tool calling<p>- Marketing pages \u2013 landing, pricing, blog, docs (fumadocs), changelog, careers, about, legal pages<p>- Emails - all the email templates ranging from &quot;forgot password&quot; to &quot;your trial has ended&quot;, etc.<p>Tech stack:<p>- Next.js 16 and React 19<p>- tRPC for end-to-end type safety<p>- Better Auth for authentication<p>- Prisma or Drizzle ORM (two separate versions)<p>- Tailwind CSS + shadcn&#x2F;ui<p>- TypeScript with strict mode<p>Two ORM?<p>It&#x27;s two versions, not at the same time. Prisma and Drizzle. Both versions have feature parity - pick what fits your workflow.<p>Pricing:<p>One-time purchase with lifetime team access. Use it for unlimited projects including client work.<p>Happy to answer any questions about the architecture, design decisions or specific features.<p>- Website: <a href=\"https:&#x2F;&#x2F;www.achromatic.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;www.achromatic.dev</a><p>- Demo: <a href=\"https:&#x2F;&#x2F;demo.achromatic.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;demo.achromatic.dev</a><p>I\u2019m planning to add TanStack-based variants and more opinionated, use-case specific starter kit included in the same license, such as:<p>- CRM starter kit<p>- Workflow builder starter kit<p>- Support&#x2F;helpdesk starter kit<p>- etc.<p>Happy to hear feedback or answer questions.","title":"Show HN: Achromatic \u2013 AI Ready Next.js 16 Starter Kit","updated_at":"2026-01-17T20:52:37Z"}],"hitsPerPage":10,"nbHits":82,"nbPages":9,"page":0,"params":"query=tool-calling+AI&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":21,"processingTimingsMS":{"_request":{"roundTrip":19},"afterFetch":{"format":{"highlighting":2,"total":2}},"fetch":{"query":3,"scanning":16,"total":20},"total":21},"query":"tool-calling AI","serverTimeMS":23}
