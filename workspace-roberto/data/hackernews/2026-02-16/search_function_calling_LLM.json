{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"simranmultani"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Hey HN,<p>I've been building <em>LLM</em>-based agents for a while and two things kept biting me.<p>1. Loops \u2014 an agent node would get stuck <em>calling</em> the same thing over and over, and I wouldn't notice until the API bill showed up. Lost $200+ on one run.\n2. <em>LLM</em> would return garbage that didn't match what downstream code expected, and everything would just crash.<p>I looked around and couldn't find something simple that handled both. Most frameworks assume your node <em>function</em> just works. In practice it doesn't \u2014 <em>LLM</em> calls fail, JSON comes back broken, state gets weird.<p>So I built AgentCircuit. It's a Python decorator that wraps your agent <em>functions</em> with circuit breaker-style protections:<p><pre><code>    from agentcircuit import reliable\n    from pydantic import BaseModel\n\n    class Output(BaseModel):\n        name: str\n        age: int\n\n    @reliable(sentinel_schema=Output)\n    def extract_data(state):\n        return call_<em>llm</em>(state[&quot;text&quot;])\n</code></pre>\nThat's it. Under the hood it:<p>- Fuse \u2014 detects when a node keeps seeing the same input and kills the loop\n- Sentinel \u2014 validates every output against a Pydantic schema\n- Medic \u2014 auto-repairs bad outputs using an <em>LLM</em>\n- Budget \u2014 per-node and global dollar/time limits so you never get a surprise bill\n- Pricing \u2014 built-in cost tracking for 40+ models (GPT-5, Claude 4.x, Gemini 3, Llama, etc.)<p>There's no server, no config files, no framework lock-in. It works at the <em>function</em> boundary so it composes with LangGraph, LangChain, CrewAI, AutoGen, or just plain <em>functions</em>.<p>GitHub: <a href=\"https://github.com/simranmultani197/AgentCircuit\" rel=\"nofollow\">https://github.com/simranmultani197/AgentCircuit</a>\nPyPI: <a href=\"https://pypi.org/project/agentcircuit/\" rel=\"nofollow\">https://pypi.org/project/agentcircuit/</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function"],"value":"Show HN: AgentCircuit \u2013 Circuit breaker for AI agent <em>functions</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/simranmultani197/AgentCircuit"}},"_tags":["story","author_simranmultani","story_46899775","show_hn"],"author":"simranmultani","children":[46947900],"created_at":"2026-02-05T14:07:27Z","created_at_i":1770300447,"num_comments":1,"objectID":"46899775","points":1,"story_id":46899775,"story_text":"Hey HN,<p>I&#x27;ve been building LLM-based agents for a while and two things kept biting me.<p>1. Loops \u2014 an agent node would get stuck calling the same thing over and over, and I wouldn&#x27;t notice until the API bill showed up. Lost $200+ on one run.\n2. LLM would return garbage that didn&#x27;t match what downstream code expected, and everything would just crash.<p>I looked around and couldn&#x27;t find something simple that handled both. Most frameworks assume your node function just works. In practice it doesn&#x27;t \u2014 LLM calls fail, JSON comes back broken, state gets weird.<p>So I built AgentCircuit. It&#x27;s a Python decorator that wraps your agent functions with circuit breaker-style protections:<p><pre><code>    from agentcircuit import reliable\n    from pydantic import BaseModel\n\n    class Output(BaseModel):\n        name: str\n        age: int\n\n    @reliable(sentinel_schema=Output)\n    def extract_data(state):\n        return call_llm(state[&quot;text&quot;])\n</code></pre>\nThat&#x27;s it. Under the hood it:<p>- Fuse \u2014 detects when a node keeps seeing the same input and kills the loop\n- Sentinel \u2014 validates every output against a Pydantic schema\n- Medic \u2014 auto-repairs bad outputs using an LLM\n- Budget \u2014 per-node and global dollar&#x2F;time limits so you never get a surprise bill\n- Pricing \u2014 built-in cost tracking for 40+ models (GPT-5, Claude 4.x, Gemini 3, Llama, etc.)<p>There&#x27;s no server, no config files, no framework lock-in. It works at the function boundary so it composes with LangGraph, LangChain, CrewAI, AutoGen, or just plain functions.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;simranmultani197&#x2F;AgentCircuit\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;simranmultani197&#x2F;AgentCircuit</a>\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;agentcircuit&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;agentcircuit&#x2F;</a>","title":"Show HN: AgentCircuit \u2013 Circuit breaker for AI agent functions","updated_at":"2026-02-10T11:38:37Z","url":"https://github.com/simranmultani197/AgentCircuit"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Hannah203"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Over the past year, many SaaS products have added AI chatbots to answer questions and reduce support load. It helped initially, but most of these systems still live in a chat window with no awareness of what\u2019s happening inside the application.\nThey don\u2019t know the current page, selected data, user permissions, or workflow state. Users end up repeating context the product already has.\nI recently came across an open-source Copilot SDK that approaches this differently by injecting live application state directly into the AI and letting it execute real frontend and backend <em>functions</em> instead of just responding with text.\nWhat it does:\n- Understands application state including current page, selected data, and user permissions\n- Executes backend and frontend <em>functions</em> instead of only responding with text\n- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons\n- Understands user workflow and intent based on in-product context\n- Maintains session context so interactions remain consistent\nExample: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.\nTechnical details:\n- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)\n- <em>LLM</em>-agnostic (bring your own model)\n- State injection via context providers\n- Tool execution layer for safe <em>function</em> <em>calling</em>\n- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https://copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https://copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: YourGPT Copilot SDK Open-source SDK for product-level intelligence"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://copilot-sdk.yourgpt.ai/docs"}},"_tags":["story","author_Hannah203","story_46885205","show_hn"],"author":"Hannah203","created_at":"2026-02-04T12:47:39Z","created_at_i":1770209259,"num_comments":0,"objectID":"46885205","points":2,"story_id":46885205,"story_text":"Over the past year, many SaaS products have added AI chatbots to answer questions and reduce support load. It helped initially, but most of these systems still live in a chat window with no awareness of what\u2019s happening inside the application.\nThey don\u2019t know the current page, selected data, user permissions, or workflow state. Users end up repeating context the product already has.\nI recently came across an open-source Copilot SDK that approaches this differently by injecting live application state directly into the AI and letting it execute real frontend and backend functions instead of just responding with text.\nWhat it does:\n- Understands application state including current page, selected data, and user permissions\n- Executes backend and frontend functions instead of only responding with text\n- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons\n- Understands user workflow and intent based on in-product context\n- Maintains session context so interactions remain consistent\nExample: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.\nTechnical details:\n- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)\n- LLM-agnostic (bring your own model)\n- State injection via context providers\n- Tool execution layer for safe function calling\n- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases.","title":"Show HN: YourGPT Copilot SDK Open-source SDK for product-level intelligence","updated_at":"2026-02-04T13:18:11Z","url":"https://copilot-sdk.yourgpt.ai/docs"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Roshni1990r"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Over the past year, many SaaS products have added AI chatbots. They answer questions, guide users, and reduce some support load. That was a useful first step, but it is no longer enough.<p>Most AI still live in a chat window with no awareness of the application. They don't understand product state, selected data, user permissions, or the current workflow, so users end up restating context the system already has.<p>That approach does not scale well for real products.<p>We\u2019ve always believed copilots are the way to deliver the best customer experience\u2014not by answering questions, but by actually doing things<p>We are releasing Copilot SDK as an open-source toolkit to explore this idea and make context-aware, action-driven copilots for product teams.<p>What it does:<p>- Understands application state including current page, selected data, and user permissions<p>- Executes backend and frontend <em>functions</em> instead of only responding with text<p>- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons<p>- Understands user workflow and intent based on in-product context<p>- Maintains session context so interactions remain consistent<p>Example: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.<p>Technical details:<p>- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)<p>- <em>LLM</em>-agnostic (bring your own model)<p>- State injection via context providers<p>- Tool execution layer for safe <em>function</em> <em>calling</em><p>- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https://copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https://copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: YourGPT Copilot SDK \u2013 Open-source toolkit for product-aware AI agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://copilot-sdk.yourgpt.ai/docs"}},"_tags":["story","author_Roshni1990r","story_46870742","show_hn"],"author":"Roshni1990r","created_at":"2026-02-03T13:31:12Z","created_at_i":1770125472,"num_comments":0,"objectID":"46870742","points":1,"story_id":46870742,"story_text":"Over the past year, many SaaS products have added AI chatbots. They answer questions, guide users, and reduce some support load. That was a useful first step, but it is no longer enough.<p>Most AI still live in a chat window with no awareness of the application. They don&#x27;t understand product state, selected data, user permissions, or the current workflow, so users end up restating context the system already has.<p>That approach does not scale well for real products.<p>We\u2019ve always believed copilots are the way to deliver the best customer experience\u2014not by answering questions, but by actually doing things<p>We are releasing Copilot SDK as an open-source toolkit to explore this idea and make context-aware, action-driven copilots for product teams.<p>What it does:<p>- Understands application state including current page, selected data, and user permissions<p>- Executes backend and frontend functions instead of only responding with text<p>- Delivers richer product experiences through generative UI such as tables, forms, and interactive buttons<p>- Understands user workflow and intent based on in-product context<p>- Maintains session context so interactions remain consistent<p>Example: Instead of the AI asking &quot;What do you need help with?&quot;, it understands the user context is viewing failed transactions from last week and can immediately offer to retry them, export the data, or investigate patterns.<p>Technical details:<p>- Works with React, Next.js, Vite (Vue &amp; Angular coming soon)<p>- LLM-agnostic (bring your own model)<p>- State injection via context providers<p>- Tool execution layer for safe function calling<p>- Full data ownership (everything runs in your infrastructure)<p>Docs + examples: <a href=\"https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;copilot-sdk.yourgpt.ai</a><p>Happy to answer technical questions about implementation, or specific use cases.","title":"Show HN: YourGPT Copilot SDK \u2013 Open-source toolkit for product-aware AI agents","updated_at":"2026-02-03T13:35:54Z","url":"https://copilot-sdk.yourgpt.ai/docs"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"yol"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function","calling"],"value":"How existing prompt management solutions work bothers me, it seems to go against programming best practices: the prompt templates are stored in completely separate system from its dependencies, and there\u2019s no interface definitions for using them. It\u2019s like <em>calling</em> a <em>function</em> (the prompt template) that takes ANY arguments and can silently return crap when the arguments don\u2019t align with its internal implementation.<p>So I made this project according to how I think prompt management should work - strongly typed interface, defined in the code; the prompt templates are co-located in the same codebase as their dependencies; and there\u2019s type-hint and validation for devEx. Doing this also brings additional benefit: because the variables are strong typed at compose time, it\u2019s save to support complex prompt templates with if/else/for control loops with full type safety.<p>I\u2019d love to know whether this resonate with others, or is it just my pet peeve."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: Pixie-prompts \u2013 manage <em>LLM</em> prompt templates like code"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://gopixie.ai"}},"_tags":["story","author_yol","story_46781675","show_hn"],"author":"yol","created_at":"2026-01-27T15:55:40Z","created_at_i":1769529340,"num_comments":0,"objectID":"46781675","points":1,"story_id":46781675,"story_text":"How existing prompt management solutions work bothers me, it seems to go against programming best practices: the prompt templates are stored in completely separate system from its dependencies, and there\u2019s no interface definitions for using them. It\u2019s like calling a function (the prompt template) that takes ANY arguments and can silently return crap when the arguments don\u2019t align with its internal implementation.<p>So I made this project according to how I think prompt management should work - strongly typed interface, defined in the code; the prompt templates are co-located in the same codebase as their dependencies; and there\u2019s type-hint and validation for devEx. Doing this also brings additional benefit: because the variables are strong typed at compose time, it\u2019s save to support complex prompt templates with if&#x2F;else&#x2F;for control loops with full type safety.<p>I\u2019d love to know whether this resonate with others, or is it just my pet peeve.","title":"Show HN: Pixie-prompts \u2013 manage LLM prompt templates like code","updated_at":"2026-01-27T15:57:44Z","url":"https://gopixie.ai"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"iCeGaming"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Hey everyone,\nI built <em>llm</em>-schema-guard because LLMs are amazing at spitting out JSON... until they suddenly aren't. Even with JSON mode or <em>function</em> <em>calling</em>, you still get missing fields, wrong types, or just plain broken syntax that kills your agents, RAG flows, or any tool-<em>calling</em> setup.\nThis is a lightweight Rust HTTP proxy that sits in front of any OpenAI-compatible API (think Ollama, vLLM, LocalAI, OpenAI itself, Groq, you name it). It grabs the generated output, checks it against a JSON Schema you provide, and only lets it through if it's valid.\nIf it's invalid, strict mode kicks back a clean 400 with details. Permissive mode tries auto-retrying a few times by tweaking the prompt with a fix instruction and exponential backoff.\nEverything else stays the same: full streaming support (it buffers the response to validate), Prometheus metrics so you can monitor validation fails, retries, latency, and more. Config is simple YAML for upstreams, schemas per model, rate limiting, caching, etc. There's even an offline CLI if you just want to test schemas locally.\nIt's built with Axum and Tokio for really low latency and high throughput, plus jsonschema-rs under the hood. Docker compose makes it dead simple to spin up with Ollama.<p>This grew out of my earlier schema-gateway project, and I'm happy to add stuff like Anthropic support, tool <em>calling</em> validation, or better streaming fixes if people find it useful.\nStars or contributions are very welcome!<p>Thanks for taking a look :)"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: <em>LLM</em>-schema-guard \u2013 Rust proxy enforcing JSON schemas on <em>LLM</em> outputs"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"https://github.com/AncientiCe/<em>llm</em>-schema-guard"}},"_tags":["story","author_iCeGaming","story_46778689","show_hn"],"author":"iCeGaming","created_at":"2026-01-27T11:39:24Z","created_at_i":1769513964,"num_comments":0,"objectID":"46778689","points":1,"story_id":46778689,"story_text":"Hey everyone,\nI built llm-schema-guard because LLMs are amazing at spitting out JSON... until they suddenly aren&#x27;t. Even with JSON mode or function calling, you still get missing fields, wrong types, or just plain broken syntax that kills your agents, RAG flows, or any tool-calling setup.\nThis is a lightweight Rust HTTP proxy that sits in front of any OpenAI-compatible API (think Ollama, vLLM, LocalAI, OpenAI itself, Groq, you name it). It grabs the generated output, checks it against a JSON Schema you provide, and only lets it through if it&#x27;s valid.\nIf it&#x27;s invalid, strict mode kicks back a clean 400 with details. Permissive mode tries auto-retrying a few times by tweaking the prompt with a fix instruction and exponential backoff.\nEverything else stays the same: full streaming support (it buffers the response to validate), Prometheus metrics so you can monitor validation fails, retries, latency, and more. Config is simple YAML for upstreams, schemas per model, rate limiting, caching, etc. There&#x27;s even an offline CLI if you just want to test schemas locally.\nIt&#x27;s built with Axum and Tokio for really low latency and high throughput, plus jsonschema-rs under the hood. Docker compose makes it dead simple to spin up with Ollama.<p>This grew out of my earlier schema-gateway project, and I&#x27;m happy to add stuff like Anthropic support, tool calling validation, or better streaming fixes if people find it useful.\nStars or contributions are very welcome!<p>Thanks for taking a look :)","title":"Show HN: LLM-schema-guard \u2013 Rust proxy enforcing JSON schemas on LLM outputs","updated_at":"2026-01-27T11:45:58Z","url":"https://github.com/AncientiCe/llm-schema-guard"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ashtadmir"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function","calling"],"value":"I've been playing around with the new official GitHub Copilot SDK and realized it's a goldmine for building programmatic bridges to their models.<p>I built this server in Go to act as a OpenAI-compatible proxy. It essentially lets you treat your GitHub Copilot subscription as a standard OpenAI backend for any tool that supports it. I have tested it against OpenWebUI and Langchain.<p>Key Highlights:<p>- Official SDK: Built using the new Github Copilot SDK. It\u2019s much more robust than the reverse-engineered solutions floating around and does not use unpublished APIs.<p>- Tool <em>Calling</em> Support: It maps OpenAI <em>function</em> definitions to Copilot's agentic tools. You can use your own tools/functions through the Copilot without copilot needing access to the said tools just the definitions is enough.<p>The goal was to create a reliable &quot;bridge&quot; so I can use my subscription models in my preferred interfaces."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: An OpenAI API compatible server that uses GitHub Copilot SDK for <em>LLMs</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/RajatGarga/copilot-openai-server"}},"_tags":["story","author_ashtadmir","story_46775889","show_hn"],"author":"ashtadmir","children":[46776731],"created_at":"2026-01-27T05:31:17Z","created_at_i":1769491877,"num_comments":1,"objectID":"46775889","points":2,"story_id":46775889,"story_text":"I&#x27;ve been playing around with the new official GitHub Copilot SDK and realized it&#x27;s a goldmine for building programmatic bridges to their models.<p>I built this server in Go to act as a OpenAI-compatible proxy. It essentially lets you treat your GitHub Copilot subscription as a standard OpenAI backend for any tool that supports it. I have tested it against OpenWebUI and Langchain.<p>Key Highlights:<p>- Official SDK: Built using the new Github Copilot SDK. It\u2019s much more robust than the reverse-engineered solutions floating around and does not use unpublished APIs.<p>- Tool Calling Support: It maps OpenAI function definitions to Copilot&#x27;s agentic tools. You can use your own tools&#x2F;functions through the Copilot without copilot needing access to the said tools just the definitions is enough.<p>The goal was to create a reliable &quot;bridge&quot; so I can use my subscription models in my preferred interfaces.","title":"Show HN: An OpenAI API compatible server that uses GitHub Copilot SDK for LLMs","updated_at":"2026-01-27T07:50:26Z","url":"https://github.com/RajatGarga/copilot-openai-server"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"charlielidbury"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["function"],"value":"If agent's tools are exposed as <em>functions</em>/objects in a Python REPL (as opposed to JSON schemas) they perform better, I linked the explainer article we wrote, but if you want to jump straight in check out the docs! <a href=\"https://docs.symbolica.ai/\" rel=\"nofollow\">https://docs.symbolica.ai/</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["calling","llm"],"value":"Show HN: <em>Calling</em> tools w/ Python improves <em>LLM</em> perf. vs MCP (77.1% on BrowseComp)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://www.symbolica.ai/blog/beyond-code-mode-agentica?theme=dark"}},"_tags":["story","author_charlielidbury","story_46287394","show_hn"],"author":"charlielidbury","created_at":"2025-12-16T11:41:34Z","created_at_i":1765885294,"num_comments":0,"objectID":"46287394","points":10,"story_id":46287394,"story_text":"If agent&#x27;s tools are exposed as functions&#x2F;objects in a Python REPL (as opposed to JSON schemas) they perform better, I linked the explainer article we wrote, but if you want to jump straight in check out the docs! <a href=\"https:&#x2F;&#x2F;docs.symbolica.ai&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.symbolica.ai&#x2F;</a>","title":"Show HN: Calling tools w/ Python improves LLM perf. vs MCP (77.1% on BrowseComp)","updated_at":"2025-12-22T21:48:03Z","url":"https://www.symbolica.ai/blog/beyond-code-mode-agentica?theme=dark"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"rhozeta"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"Hey HN! I built a platform that lets you upload an OpenAPI spec and get an AI-powered chat agent you can embed on your website.<p>- The motivation: \nI kept seeing companies with perfectly good APIs, but their users still struggled with complex UIs or had to read through documentation. Meanwhile, building a custom AI agent requires significant engineering resources most teams don't have.<p>- How it works:\nUpload your OpenAPI/Swagger spec (or paste your API docs)\nThe system maps out your endpoints and parameters\nYou get an embeddable widget that understands natural language queries\nWhen users ask questions, the agent translates them to API calls and returns results conversationally<p>Example: An e-commerce site with an inventory API. Instead of navigating filters, users ask &quot;Do you have running shoes in size 10 under $100?&quot; and get instant results.<p>- Technical details:\nUses <em>LLM</em> <em>function</em> <em>calling</em> to map queries to API endpoints\nHandles authentication (API keys, OAuth)\nRate limiting and caching to avoid hammering your API\nWorks with REST APIs (GraphQL support coming)<p>Current state: Functional MVP, testing with a few early users. Still figuring out edge cases around complex APIs and auth flows.<p>- What I'm looking for feedback on:\nHave you encountered this problem? How did you solve it?\nSecurity concerns I should be thinking about?\nIs this actually useful or just a cool demo?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Turn any API into an embeddable AI agent"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://gethelmagent.com/"}},"_tags":["story","author_rhozeta","story_46204985","show_hn"],"author":"rhozeta","created_at":"2025-12-09T13:59:23Z","created_at_i":1765288763,"num_comments":0,"objectID":"46204985","points":1,"story_id":46204985,"story_text":"Hey HN! I built a platform that lets you upload an OpenAPI spec and get an AI-powered chat agent you can embed on your website.<p>- The motivation: \nI kept seeing companies with perfectly good APIs, but their users still struggled with complex UIs or had to read through documentation. Meanwhile, building a custom AI agent requires significant engineering resources most teams don&#x27;t have.<p>- How it works:\nUpload your OpenAPI&#x2F;Swagger spec (or paste your API docs)\nThe system maps out your endpoints and parameters\nYou get an embeddable widget that understands natural language queries\nWhen users ask questions, the agent translates them to API calls and returns results conversationally<p>Example: An e-commerce site with an inventory API. Instead of navigating filters, users ask &quot;Do you have running shoes in size 10 under $100?&quot; and get instant results.<p>- Technical details:\nUses LLM function calling to map queries to API endpoints\nHandles authentication (API keys, OAuth)\nRate limiting and caching to avoid hammering your API\nWorks with REST APIs (GraphQL support coming)<p>Current state: Functional MVP, testing with a few early users. Still figuring out edge cases around complex APIs and auth flows.<p>- What I&#x27;m looking for feedback on:\nHave you encountered this problem? How did you solve it?\nSecurity concerns I should be thinking about?\nIs this actually useful or just a cool demo?","title":"Show HN: Turn any API into an embeddable AI agent","updated_at":"2025-12-09T14:00:39Z","url":"https://gethelmagent.com/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"maxtermed"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"I was using <em>LLM</em> frameworks everywhere but had no idea what was happening inside them. One day I needed to optimize something and realized I couldn't. Hard truth: I didn't understand the fundamentals, just which framework <em>function</em> to call.<p>So I stripped everything away. No abstractions. Just Python, HTTP requests, and the OpenAI/Anthropic APIs.<p>What I found was anticlimactic in the best way: there's almost nothing there.<p><pre><code>  - &quot;AI agents&quot; are just functions the model tells you to call\n  - &quot;Memory&quot; is literally just a list you append to and send back\n  - &quot;RAG&quot; is search, concatenate to prompt, send it off\n  - &quot;Multi-agent systems&quot; are just API calls in sequence\n</code></pre>\nIt all clicked after that. Not because the patterns are hard. They're not. In fact, they're trivial. They're just buried under layers of abstraction that make them seem hard.<p>I created 7 modules showing the basics: API calls, conversation state, tool <em>calling</em>, RAG, streaming, prompt chaining. Each one is heavily commented, nothing fancy. Side-by-side examples for Claude and GPT so you can see they're fundamentally the same thing.<p>Now when I use frameworks, I actually know if I need them or if I'm just adding bloat.<p>Repo: <a href=\"https://github.com/jmedia65/learn-ai-right\" rel=\"nofollow\">https://github.com/jmedia65/learn-ai-right</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["llm"],"value":"Show HN: Understanding <em>LLM</em> fundamentals without frameworks"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/jmedia65/learn-ai-right"}},"_tags":["story","author_maxtermed","story_45696980","show_hn"],"author":"maxtermed","created_at":"2025-10-24T17:29:28Z","created_at_i":1761326968,"num_comments":0,"objectID":"45696980","points":6,"story_id":45696980,"story_text":"I was using LLM frameworks everywhere but had no idea what was happening inside them. One day I needed to optimize something and realized I couldn&#x27;t. Hard truth: I didn&#x27;t understand the fundamentals, just which framework function to call.<p>So I stripped everything away. No abstractions. Just Python, HTTP requests, and the OpenAI&#x2F;Anthropic APIs.<p>What I found was anticlimactic in the best way: there&#x27;s almost nothing there.<p><pre><code>  - &quot;AI agents&quot; are just functions the model tells you to call\n  - &quot;Memory&quot; is literally just a list you append to and send back\n  - &quot;RAG&quot; is search, concatenate to prompt, send it off\n  - &quot;Multi-agent systems&quot; are just API calls in sequence\n</code></pre>\nIt all clicked after that. Not because the patterns are hard. They&#x27;re not. In fact, they&#x27;re trivial. They&#x27;re just buried under layers of abstraction that make them seem hard.<p>I created 7 modules showing the basics: API calls, conversation state, tool calling, RAG, streaming, prompt chaining. Each one is heavily commented, nothing fancy. Side-by-side examples for Claude and GPT so you can see they&#x27;re fundamentally the same thing.<p>Now when I use frameworks, I actually know if I need them or if I&#x27;m just adding bloat.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;jmedia65&#x2F;learn-ai-right\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;jmedia65&#x2F;learn-ai-right</a>","title":"Show HN: Understanding LLM fundamentals without frameworks","updated_at":"2025-10-26T01:28:39Z","url":"https://github.com/jmedia65/learn-ai-right"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"yincong0822"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["function","calling","llm"],"value":"I built MuseBot, a multi-platform chatbot that integrates with <em>LLM</em> APIs to provide AI-powered responses.\nIt supports platforms like Telegram, Discord, Slack, Lark (Feishu), DingTalk, WeCom (\u4f01\u4e1a\u5fae\u4fe1), QQ, and WeChat, so you can talk to your favorite model anywhere.<p>MuseBot connects seamlessly with OpenAI, DeepSeek, Gemini, and OpenRouter models to make conversations feel natural, dynamic, and responsive.<p>Key Features<p>AI Responses \u2013 Intelligent chatbot replies using <em>LLM</em> APIs.<p>Streaming Output \u2013 Real-time responses that feel conversational.<p>Easy Deployment \u2013 Run locally or on any cloud server in just a few steps.<p>Image Understanding \u2013 Send an image, and the bot can interpret and respond.<p>Voice Support \u2013 Communicate using voice messages.<p><em>Function</em> <em>Calling</em> \u2013 Supports MCP-style <em>function</em> calls for extending capabilities.<p>RAG Support \u2013 Enhances context understanding with retrieval-augmented generation.<p>Admin Platform \u2013 Web-based platform to manage bots and configurations.<p>Service Registration \u2013 Automatically register bot instances to a service registry.<p>Metrics and Monitoring \u2013 Built-in Prometheus metrics for observability.<p>MuseBot is built entirely in Golang, designed for performance, modularity, and easy extensibility.\nI\u2019d love to hear feedback from developers working with chatbots, <em>LLM</em> integrations, or Go-based infrastructure \u2014 especially ideas to improve scalability and real-time performance."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: MuseBot\u2013An AI Chatbot for Telegram, Discord, Slack, Lark, QQ, and More"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/yincongcyincong/MuseBot"}},"_tags":["story","author_yincong0822","story_45679248","show_hn"],"author":"yincong0822","children":[45679252],"created_at":"2025-10-23T07:38:08Z","created_at_i":1761205088,"num_comments":1,"objectID":"45679248","points":1,"story_id":45679248,"story_text":"I built MuseBot, a multi-platform chatbot that integrates with LLM APIs to provide AI-powered responses.\nIt supports platforms like Telegram, Discord, Slack, Lark (Feishu), DingTalk, WeCom (\u4f01\u4e1a\u5fae\u4fe1), QQ, and WeChat, so you can talk to your favorite model anywhere.<p>MuseBot connects seamlessly with OpenAI, DeepSeek, Gemini, and OpenRouter models to make conversations feel natural, dynamic, and responsive.<p>Key Features<p>AI Responses \u2013 Intelligent chatbot replies using LLM APIs.<p>Streaming Output \u2013 Real-time responses that feel conversational.<p>Easy Deployment \u2013 Run locally or on any cloud server in just a few steps.<p>Image Understanding \u2013 Send an image, and the bot can interpret and respond.<p>Voice Support \u2013 Communicate using voice messages.<p>Function Calling \u2013 Supports MCP-style function calls for extending capabilities.<p>RAG Support \u2013 Enhances context understanding with retrieval-augmented generation.<p>Admin Platform \u2013 Web-based platform to manage bots and configurations.<p>Service Registration \u2013 Automatically register bot instances to a service registry.<p>Metrics and Monitoring \u2013 Built-in Prometheus metrics for observability.<p>MuseBot is built entirely in Golang, designed for performance, modularity, and easy extensibility.\nI\u2019d love to hear feedback from developers working with chatbots, LLM integrations, or Go-based infrastructure \u2014 especially ideas to improve scalability and real-time performance.","title":"Show HN: MuseBot\u2013An AI Chatbot for Telegram, Discord, Slack, Lark, QQ, and More","updated_at":"2025-10-23T07:40:00Z","url":"https://github.com/yincongcyincong/MuseBot"}],"hitsPerPage":10,"nbHits":130,"nbPages":13,"page":0,"params":"query=function+calling+LLM&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":104,"processingTimingsMS":{"_request":{"roundTrip":14},"afterFetch":{"format":{"highlighting":1,"total":1},"merge":{"mergeLoop":{"total":10},"total":10},"total":10},"fetch":{"query":80,"scanning":11,"total":92},"total":104},"query":"function calling LLM","serverTimeMS":105}
