{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"javiercr"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"<em>Claude</em> Agent in VS <em>Code</em>: no extension required, Copilot subscription supported"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"https://<em>code</em>.visualstudio.com/docs/copilot/agents/third-party-agents#_<em>claude</em>-agent-preview"}},"_tags":["story","author_javiercr","story_47014017"],"author":"javiercr","created_at":"2026-02-14T12:21:16Z","created_at_i":1771071676,"num_comments":0,"objectID":"47014017","points":2,"story_id":47014017,"title":"Claude Agent in VS Code: no extension required, Copilot subscription supported","updated_at":"2026-02-14T12:52:37Z","url":"https://code.visualstudio.com/docs/copilot/agents/third-party-agents#_claude-agent-preview"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"anistark"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"If you use <em>claude</em> with other coding agents like pi or opencode, you might not get the `/usage` skill that <em>claude</em> <em>code</em> comes with.<p>So, I made this small CLI tool `claudemon`.<p>Install with `npm install -g claudemon`<p>Setup once with `claudemon setup`<p>And then run it: `claudemon`<p>I keep it running on one small terminal window. It keeps track of usage. Refreshes every few seconds. Completely local and private. Also, open source, if someone wants to help add some new features.<p>Open to feedback."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude"],"value":"<em>Claude</em> Usage Monitor"}},"_tags":["story","author_anistark","story_47013785","ask_hn"],"author":"anistark","created_at":"2026-02-14T11:48:23Z","created_at_i":1771069703,"num_comments":0,"objectID":"47013785","points":1,"story_id":47013785,"story_text":"If you use claude with other coding agents like pi or opencode, you might not get the `&#x2F;usage` skill that claude code comes with.<p>So, I made this small CLI tool `claudemon`.<p>Install with `npm install -g claudemon`<p>Setup once with `claudemon setup`<p>And then run it: `claudemon`<p>I keep it running on one small terminal window. It keeps track of usage. Refreshes every few seconds. Completely local and private. Also, open source, if someone wants to help add some new features.<p>Open to feedback.","title":"Claude Usage Monitor","updated_at":"2026-02-14T11:53:52Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mazilin"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Subject: My attempt at an &quot;OS-inspired&quot; AI architecture\nHi HN,\nI'm a Product Manager, not a systems engineer. I built AI Station Navigator as a proof-of-concept to solve a specific problem I faced: Context Pollution.\nWhen using AI agents for complex tasks, the context window gets cluttered quickly, causing the model to hallucinate or get confused.\nTo solve this, I designed this project using a Computer Architecture Analogy. I treated the agent system like a traditional OS to better manage resources and isolation.\nHere is the mapping I used to design the system:\n-- CPU approx. LLM (<em>Claude</em>)\nThe raw computing power driving the capabilities.\n-- Kernel approx. Orchestration Layer (<em>Claude</em> <em>Code</em> + <em>CLAUDE</em>.md)\nHandles intent recognition, task scheduling, and context management.\n-- Processes approx. Sub-Agents\nThe core feature: Execution runs in isolated sub-agents. When a task is done, the sub-agent &quot;dies,&quot; freeing up context. This prevents the main thread from getting polluted.\n-- Applications approx. Skills (GitHub Repos)\n&quot;App Store-style&quot; installation of tools via GitHub links.\n-- Drivers approx. MCP + Hooks\nStandardized interfaces for external tools and system automation.\n-- Runtime approx. Portable Environment\nSelf-contained Python/Node environment (no installation hell).\nThe <em>Code</em>: <a href=\"https://github.com/canishowtime/ai-station-navigator/\" rel=\"nofollow\">https://github.com/canishowtime/ai-station-navigator/</a>\nI'd love to hear your thoughts on this architectural approach."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: AI Station Navigator \u2013 LLM=CPU, Agents=Processes, Skills=Apps"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/canishowtime/ai-station-navigator"}},"_tags":["story","author_mazilin","story_47013274","show_hn"],"author":"mazilin","created_at":"2026-02-14T10:05:55Z","created_at_i":1771063555,"num_comments":0,"objectID":"47013274","points":2,"story_id":47013274,"story_text":"Subject: My attempt at an &quot;OS-inspired&quot; AI architecture\nHi HN,\nI&#x27;m a Product Manager, not a systems engineer. I built AI Station Navigator as a proof-of-concept to solve a specific problem I faced: Context Pollution.\nWhen using AI agents for complex tasks, the context window gets cluttered quickly, causing the model to hallucinate or get confused.\nTo solve this, I designed this project using a Computer Architecture Analogy. I treated the agent system like a traditional OS to better manage resources and isolation.\nHere is the mapping I used to design the system:\n-- CPU approx. LLM (Claude)\nThe raw computing power driving the capabilities.\n-- Kernel approx. Orchestration Layer (Claude Code + CLAUDE.md)\nHandles intent recognition, task scheduling, and context management.\n-- Processes approx. Sub-Agents\nThe core feature: Execution runs in isolated sub-agents. When a task is done, the sub-agent &quot;dies,&quot; freeing up context. This prevents the main thread from getting polluted.\n-- Applications approx. Skills (GitHub Repos)\n&quot;App Store-style&quot; installation of tools via GitHub links.\n-- Drivers approx. MCP + Hooks\nStandardized interfaces for external tools and system automation.\n-- Runtime approx. Portable Environment\nSelf-contained Python&#x2F;Node environment (no installation hell).\nThe Code: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;canishowtime&#x2F;ai-station-navigator&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;canishowtime&#x2F;ai-station-navigator&#x2F;</a>\nI&#x27;d love to hear your thoughts on this architectural approach.","title":"Show HN: AI Station Navigator \u2013 LLM=CPU, Agents=Processes, Skills=Apps","updated_at":"2026-02-14T10:09:52Z","url":"https://github.com/canishowtime/ai-station-navigator"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"meghendra"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Hi HN \u2014 I built cgrep, a local-first, <em>code</em>-aware search tool for AI coding agents (and humans).<p>The goal is to reduce noisy retrieval loops and token waste in real repositories.  \ncgrep combines BM25 + tree-sitter symbol awareness, with optional semantic/hybrid search, and returns deterministic JSON for agent workflows.<p>What it does:\n- <em>Code</em> navigation: definition, references, callers, dependents\n- Focused context tools: read, map\n- Agent flow: `agent locate` -&gt; `agent expand` (small payload first, expand only selected IDs)\n- MCP support: `cgrep mcp serve` + host install helpers\n- Agent install support: <em>claude</em>-<em>code</em>, codex, copilot, cursor, opencode<p>Benchmark snapshot (PyTorch, 6 implementation-tracing scenarios):\n- Baseline (`grep`) tokens-to-complete: 127,665\n- cgrep (`agent locate/expand`) tokens-to-complete: 6,153\n- 95.2% fewer tokens (20.75x smaller)\n- Avg retrieval latency to completion: 1321.3ms -&gt; 22.7ms (~58.2x faster after indexing)<p>Links:\n- Repo: <a href=\"https://github.com/meghendra6/cgrep\" rel=\"nofollow\">https://github.com/meghendra6/cgrep</a>\n- Docs: <a href=\"https://meghendra6.github.io/cgrep/\" rel=\"nofollow\">https://meghendra6.github.io/cgrep/</a>\n- Benchmark method/results: <a href=\"https://meghendra6.github.io/cgrep/benchmarks/pytorch-agent-token-efficiency/\" rel=\"nofollow\">https://meghendra6.github.io/cgrep/benchmarks/pytorch-agent-...</a><p>I\u2019d really appreciate feedback on:\n- Real-world agent workflows I should benchmark next\n- MCP/agent integrations I should add\n- Cases where cgrep retrieval quality still falls short"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["code"],"value":"Show HN: cgrep \u2013 local, <em>code</em>-aware search for AI coding agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/meghendra6/cgrep"}},"_tags":["story","author_meghendra","story_47013067","show_hn"],"author":"meghendra","created_at":"2026-02-14T09:30:37Z","created_at_i":1771061437,"num_comments":0,"objectID":"47013067","points":2,"story_id":47013067,"story_text":"Hi HN \u2014 I built cgrep, a local-first, code-aware search tool for AI coding agents (and humans).<p>The goal is to reduce noisy retrieval loops and token waste in real repositories.  \ncgrep combines BM25 + tree-sitter symbol awareness, with optional semantic&#x2F;hybrid search, and returns deterministic JSON for agent workflows.<p>What it does:\n- Code navigation: definition, references, callers, dependents\n- Focused context tools: read, map\n- Agent flow: `agent locate` -&gt; `agent expand` (small payload first, expand only selected IDs)\n- MCP support: `cgrep mcp serve` + host install helpers\n- Agent install support: claude-code, codex, copilot, cursor, opencode<p>Benchmark snapshot (PyTorch, 6 implementation-tracing scenarios):\n- Baseline (`grep`) tokens-to-complete: 127,665\n- cgrep (`agent locate&#x2F;expand`) tokens-to-complete: 6,153\n- 95.2% fewer tokens (20.75x smaller)\n- Avg retrieval latency to completion: 1321.3ms -&gt; 22.7ms (~58.2x faster after indexing)<p>Links:\n- Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;meghendra6&#x2F;cgrep\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;meghendra6&#x2F;cgrep</a>\n- Docs: <a href=\"https:&#x2F;&#x2F;meghendra6.github.io&#x2F;cgrep&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;meghendra6.github.io&#x2F;cgrep&#x2F;</a>\n- Benchmark method&#x2F;results: <a href=\"https:&#x2F;&#x2F;meghendra6.github.io&#x2F;cgrep&#x2F;benchmarks&#x2F;pytorch-agent-token-efficiency&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;meghendra6.github.io&#x2F;cgrep&#x2F;benchmarks&#x2F;pytorch-agent-...</a><p>I\u2019d really appreciate feedback on:\n- Real-world agent workflows I should benchmark next\n- MCP&#x2F;agent integrations I should add\n- Cases where cgrep retrieval quality still falls short","title":"Show HN: cgrep \u2013 local, code-aware search for AI coding agents","updated_at":"2026-02-14T09:33:52Z","url":"https://github.com/meghendra6/cgrep"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"lingxiao10"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Show HN: Long Mem <em>code</em> agent cut 95% costs for <em>Claude</em> with small model reading"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://marketplace.visualstudio.com/items?itemName=devokaicode.cosave"}},"_tags":["story","author_lingxiao10","story_47012614","show_hn"],"author":"lingxiao10","children":[47012719],"created_at":"2026-02-14T08:00:58Z","created_at_i":1771056058,"num_comments":1,"objectID":"47012614","points":26,"story_id":47012614,"title":"Show HN: Long Mem code agent cut 95% costs for Claude with small model reading","updated_at":"2026-02-14T12:10:22Z","url":"https://marketplace.visualstudio.com/items?itemName=devokaicode.cosave"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"hoangnnguyen"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"After using Cursor and <em>Claude</em> <em>Code</em> daily, I\u2019ve noticed that when an AI coding agent drifts or forgets constraints, we assume it\u2019s a model limitation.<p>In many cases, it\u2019s context management.<p>A few observations:\n- Tokens are not just limits. They\u2019re attention competition.\n- Even before hitting the hard window limit, attention dilution happens.\n- Coding tasks degrade faster than chat because of dependency density and multi-representation juggling (diffs, logs, tests).<p>I started managing context deliberately:\n- Always write a contract\n- Chunk sessions by intent\n- Snapshot state and restart\n- Prefer on-demand CLI instead of preloading large MCP responses<p>It dramatically improved the stability of the agent.<p>Curious how others are handling context optimization."},"title":{"matchLevel":"none","matchedWords":[],"value":"Context management is the real bottleneck in AI-assisted coding"}},"_tags":["story","author_hoangnnguyen","story_47012302","ask_hn"],"author":"hoangnnguyen","children":[47012353],"created_at":"2026-02-14T06:58:16Z","created_at_i":1771052296,"num_comments":1,"objectID":"47012302","points":1,"story_id":47012302,"story_text":"After using Cursor and Claude Code daily, I\u2019ve noticed that when an AI coding agent drifts or forgets constraints, we assume it\u2019s a model limitation.<p>In many cases, it\u2019s context management.<p>A few observations:\n- Tokens are not just limits. They\u2019re attention competition.\n- Even before hitting the hard window limit, attention dilution happens.\n- Coding tasks degrade faster than chat because of dependency density and multi-representation juggling (diffs, logs, tests).<p>I started managing context deliberately:\n- Always write a contract\n- Chunk sessions by intent\n- Snapshot state and restart\n- Prefer on-demand CLI instead of preloading large MCP responses<p>It dramatically improved the stability of the agent.<p>Curious how others are handling context optimization.","title":"Context management is the real bottleneck in AI-assisted coding","updated_at":"2026-02-14T07:08:51Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"jordanappsite"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Hi HN, I'm Ty. I built LUCID because I kept shipping bugs that my AI coding assistant hallucinated into existence.<p>Three independent papers have proven that LLM hallucination is mathematically inevitable (Xu et al. 2024, Banerjee et al. 2024, Karpowicz 2025). You can't train it away. You can't prompt it away. So I built a verification layer instead.<p>How it works: LUCID extracts implicit claims from AI-generated <em>code</em> (e.g., &quot;this function handles null input,&quot; &quot;this query is injection-safe,&quot; &quot;this handles concurrent access&quot;), then uses a second, adversarial AI pass to verify each claim against the actual implementation. You get a report showing exactly what would have shipped to production without verification.<p>&quot;But can't the verifier hallucinate too?&quot; Yes -- and that's the right question. The benchmarks below were validated by running real test suites, not by trusting LUCID's judgment. The value is that structured claim extraction + adversarial verification catches bugs that a single generation pass misses. The architecture also supports swapping LLM verification for formal methods (SMT solvers, property-based testing) per claim type as those integrations mature.<p>Benchmarks:<p>- HumanEval: 86.6% baseline -&gt; 100% pass@5 with LUCID (164/164 problems)\n- SWE-bench: 18.3% baseline -&gt; 30.3% with LUCID (+65.5%)\n- Both benchmarks were validated by running actual test suites, not by LLM judgment\n- LLM-as-judge actually performs worse at higher k values -- it hallucinates false positives<p>Three ways to use it:<p>1. MCP Server (<em>Claude</em> <em>Code</em>, Cursor, Windsurf) -- one config line, verification as a native tool\n2. GitHub Action -- automated verification on every PR with inline comments\n3. CLI -- npx lucid verify --repo /path/to/<em>code</em><p>Free tier: 100 verifications/month. Get a key at <a href=\"https://trylucid.dev\" rel=\"nofollow\">https://trylucid.dev</a><p><em>Code</em>: <a href=\"https://github.com/gtsbahamas/hallucination-reversing-system\" rel=\"nofollow\">https://github.com/gtsbahamas/hallucination-reversing-system</a>\nPaper: <a href=\"https://doi.org/10.5281/zenodo.18522644\" rel=\"nofollow\">https://doi.org/10.5281/zenodo.18522644</a>\nDashboard: <a href=\"https://trylucid.dev\" rel=\"nofollow\">https://trylucid.dev</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["code"],"value":"Show HN: Lucid \u2013 Catch hallucinations in AI-generated <em>code</em> before they ship"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/gtsbahamas/hallucination-reversing-system"}},"_tags":["story","author_jordanappsite","story_47011695","show_hn"],"author":"jordanappsite","created_at":"2026-02-14T04:55:10Z","created_at_i":1771044910,"num_comments":0,"objectID":"47011695","points":4,"story_id":47011695,"story_text":"Hi HN, I&#x27;m Ty. I built LUCID because I kept shipping bugs that my AI coding assistant hallucinated into existence.<p>Three independent papers have proven that LLM hallucination is mathematically inevitable (Xu et al. 2024, Banerjee et al. 2024, Karpowicz 2025). You can&#x27;t train it away. You can&#x27;t prompt it away. So I built a verification layer instead.<p>How it works: LUCID extracts implicit claims from AI-generated code (e.g., &quot;this function handles null input,&quot; &quot;this query is injection-safe,&quot; &quot;this handles concurrent access&quot;), then uses a second, adversarial AI pass to verify each claim against the actual implementation. You get a report showing exactly what would have shipped to production without verification.<p>&quot;But can&#x27;t the verifier hallucinate too?&quot; Yes -- and that&#x27;s the right question. The benchmarks below were validated by running real test suites, not by trusting LUCID&#x27;s judgment. The value is that structured claim extraction + adversarial verification catches bugs that a single generation pass misses. The architecture also supports swapping LLM verification for formal methods (SMT solvers, property-based testing) per claim type as those integrations mature.<p>Benchmarks:<p>- HumanEval: 86.6% baseline -&gt; 100% pass@5 with LUCID (164&#x2F;164 problems)\n- SWE-bench: 18.3% baseline -&gt; 30.3% with LUCID (+65.5%)\n- Both benchmarks were validated by running actual test suites, not by LLM judgment\n- LLM-as-judge actually performs worse at higher k values -- it hallucinates false positives<p>Three ways to use it:<p>1. MCP Server (Claude Code, Cursor, Windsurf) -- one config line, verification as a native tool\n2. GitHub Action -- automated verification on every PR with inline comments\n3. CLI -- npx lucid verify --repo &#x2F;path&#x2F;to&#x2F;code<p>Free tier: 100 verifications&#x2F;month. Get a key at <a href=\"https:&#x2F;&#x2F;trylucid.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;trylucid.dev</a><p>Code: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;gtsbahamas&#x2F;hallucination-reversing-system\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;gtsbahamas&#x2F;hallucination-reversing-system</a>\nPaper: <a href=\"https:&#x2F;&#x2F;doi.org&#x2F;10.5281&#x2F;zenodo.18522644\" rel=\"nofollow\">https:&#x2F;&#x2F;doi.org&#x2F;10.5281&#x2F;zenodo.18522644</a>\nDashboard: <a href=\"https:&#x2F;&#x2F;trylucid.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;trylucid.dev</a>","title":"Show HN: Lucid \u2013 Catch hallucinations in AI-generated code before they ship","updated_at":"2026-02-14T10:06:07Z","url":"https://github.com/gtsbahamas/hallucination-reversing-system"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Fendy1"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Problem: I installed OpenClaw multiple times on several Macs. It just didn't respond to me. Some of my friends met with the same problem.<p>I suspect that it might be the failure of calling <em>Claude</em> <em>Code</em> through setup-token because I use its subscription plan.<p>The official doc says it supports calling <em>Claude</em> <em>Code</em> through subscription, and I just need to generate a setup token. But it turns out it never worked. Openclaw just didn;t respond at all.<p>I changed to calling the OpenAI API key. It worked.<p>So has anyone met with the same problem and solved it? Is it really because Anthropic banned us from calling <em>Claude</em> <em>code</em> through the subscription plan?<p>Can somebody please share your experience? Thanks"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: My OpenClaw doesn't respond. Anybody met with the same problem?"}},"_tags":["story","author_Fendy1","story_47011472","ask_hn"],"author":"Fendy1","children":[47012863,47011559,47011763],"created_at":"2026-02-14T04:09:29Z","created_at_i":1771042169,"num_comments":5,"objectID":"47011472","points":4,"story_id":47011472,"story_text":"Problem: I installed OpenClaw multiple times on several Macs. It just didn&#x27;t respond to me. Some of my friends met with the same problem.<p>I suspect that it might be the failure of calling Claude Code through setup-token because I use its subscription plan.<p>The official doc says it supports calling Claude Code through subscription, and I just need to generate a setup token. But it turns out it never worked. Openclaw just didn;t respond at all.<p>I changed to calling the OpenAI API key. It worked.<p>So has anyone met with the same problem and solved it? Is it really because Anthropic banned us from calling Claude code through the subscription plan?<p>Can somebody please share your experience? Thanks","title":"Ask HN: My OpenClaw doesn't respond. Anybody met with the same problem?","updated_at":"2026-02-14T08:51:37Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"lordokami"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"My wife and I built and shipped a simple iOS app without writing a single line of <em>code</em> in the traditional sense.<p>She hates when I bring my laptop on trips. I love building things. This was our compromise.<p>I had been wanting to experiment with building an iOS app using <em>Claude</em> <em>Code</em>. I had never built for iOS before, and the idea of exploring it through AI-assisted development felt like a new frontier for me. But bringing a laptop to Japan again would not go unnoticed, and not in a good way.<p>So I made a plan.<p>Before leaving Spain, I configured my Mac so it would never sleep. I set up a VPN so I could SSH into it securely from my phone. I installed Zellij to maintain persistent terminal sessions in case the connection dropped. I also prepared a deployment pipeline to TestFlight, so I could trigger builds remotely and test them about 15 minutes later from the other side of the world, asynchronously.<p>This was our second time visiting Japan, and we have always wanted to learn more of the language. So we decided to build something we would actually use: a lightweight phrase app with useful tourist sentences and built-in text to speech. Things like ordering in restaurants, asking how much something costs, or navigating train stations.<p>The funny part is how it evolved.<p>While I was driving between cities, my wife would sit in the passenger seat dictating changes and features into Terminus on my iPhone, connected via SSH to my Mac back home. We used voice input to modify prompts, refine UI text, and generate new features. It became a shared game.<p>Development happened in short bursts, in parking lots, at rest stops, during train rides. We would ship a build, test it in real restaurants or shops, notice friction, and tweak it again that same evening from a ryokan or small hotel room.<p>The feedback loop was almost absurdly tight. We would use it in the real world, find awkward phrasing, improve it, redeploy, and test again the next day.<p>We never opened Xcode locally. We never touched the Mac physically during the trip. Everything happened remotely from a phone across continents.<p>What started as a workaround to avoid bringing a laptop turned into one of the most fun and lightweight building experiences I have ever had. It did not feel like working on vacation. It felt like co-creating something useful for the trip itself.<p>By the end of the journey, the app was not just a prototype. It was stable, usable, and something we genuinely relied on.<p>More than the app itself, the experiment was the interesting part: remote vibecoding, persistent sessions, AI-assisted iteration, and building in real-world feedback loops instead of simulated ones.<p>It made me rethink what a development environment even means.<p>Happy to answer questions about the setup, tooling, workflow, or what broke along the way."},"title":{"matchLevel":"none","matchedWords":[],"value":"Built and shipped an iOS app from my phone while traveling Japan"}},"_tags":["story","author_lordokami","story_47011100","ask_hn"],"author":"lordokami","children":[47012467,47011127,47011145],"created_at":"2026-02-14T03:00:57Z","created_at_i":1771038057,"num_comments":4,"objectID":"47011100","points":4,"story_id":47011100,"story_text":"My wife and I built and shipped a simple iOS app without writing a single line of code in the traditional sense.<p>She hates when I bring my laptop on trips. I love building things. This was our compromise.<p>I had been wanting to experiment with building an iOS app using Claude Code. I had never built for iOS before, and the idea of exploring it through AI-assisted development felt like a new frontier for me. But bringing a laptop to Japan again would not go unnoticed, and not in a good way.<p>So I made a plan.<p>Before leaving Spain, I configured my Mac so it would never sleep. I set up a VPN so I could SSH into it securely from my phone. I installed Zellij to maintain persistent terminal sessions in case the connection dropped. I also prepared a deployment pipeline to TestFlight, so I could trigger builds remotely and test them about 15 minutes later from the other side of the world, asynchronously.<p>This was our second time visiting Japan, and we have always wanted to learn more of the language. So we decided to build something we would actually use: a lightweight phrase app with useful tourist sentences and built-in text to speech. Things like ordering in restaurants, asking how much something costs, or navigating train stations.<p>The funny part is how it evolved.<p>While I was driving between cities, my wife would sit in the passenger seat dictating changes and features into Terminus on my iPhone, connected via SSH to my Mac back home. We used voice input to modify prompts, refine UI text, and generate new features. It became a shared game.<p>Development happened in short bursts, in parking lots, at rest stops, during train rides. We would ship a build, test it in real restaurants or shops, notice friction, and tweak it again that same evening from a ryokan or small hotel room.<p>The feedback loop was almost absurdly tight. We would use it in the real world, find awkward phrasing, improve it, redeploy, and test again the next day.<p>We never opened Xcode locally. We never touched the Mac physically during the trip. Everything happened remotely from a phone across continents.<p>What started as a workaround to avoid bringing a laptop turned into one of the most fun and lightweight building experiences I have ever had. It did not feel like working on vacation. It felt like co-creating something useful for the trip itself.<p>By the end of the journey, the app was not just a prototype. It was stable, usable, and something we genuinely relied on.<p>More than the app itself, the experiment was the interesting part: remote vibecoding, persistent sessions, AI-assisted iteration, and building in real-world feedback loops instead of simulated ones.<p>It made me rethink what a development environment even means.<p>Happy to answer questions about the setup, tooling, workflow, or what broke along the way.","title":"Built and shipped an iOS app from my phone while traveling Japan","updated_at":"2026-02-14T12:24:22Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"stubbi"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Hi there,<p>I found it very useful recently to have my own &quot;Steve Jobs in my pocket&quot; when developing with <em>Claude</em> <em>Code</em>. I can ask it what Steve Jobs would think and usually comes up with great user, simplicity, and story-first thinking.<p>I thought this might be useful for others, so sharing it here. In the article I also describe why I think skills are the new apps, just with different economics. If you haven't tried them yet, it's certainly worth it. Much more of a change to &quot;default&quot; <em>Claude</em> <em>Code</em> than without.<p>Happy to hear any feedback!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: A Steve Jobs in Your Pocket AI Skill"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://openclaw.rocks/blog/ai-skills-are-the-new-apps"}},"_tags":["story","author_stubbi","story_47010979","show_hn"],"author":"stubbi","created_at":"2026-02-14T02:38:57Z","created_at_i":1771036737,"num_comments":0,"objectID":"47010979","points":2,"story_id":47010979,"story_text":"Hi there,<p>I found it very useful recently to have my own &quot;Steve Jobs in my pocket&quot; when developing with Claude Code. I can ask it what Steve Jobs would think and usually comes up with great user, simplicity, and story-first thinking.<p>I thought this might be useful for others, so sharing it here. In the article I also describe why I think skills are the new apps, just with different economics. If you haven&#x27;t tried them yet, it&#x27;s certainly worth it. Much more of a change to &quot;default&quot; Claude Code than without.<p>Happy to hear any feedback!","title":"Show HN: A Steve Jobs in Your Pocket AI Skill","updated_at":"2026-02-14T03:16:51Z","url":"https://openclaw.rocks/blog/ai-skills-are-the-new-apps"}],"hitsPerPage":10,"nbHits":3809,"nbPages":100,"page":0,"params":"query=Claude+Code&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":15,"processingTimingsMS":{"_request":{"roundTrip":26},"afterFetch":{"format":{"total":1}},"fetch":{"query":4,"scanning":9,"total":14},"total":15},"query":"Claude Code","serverTimeMS":17}
