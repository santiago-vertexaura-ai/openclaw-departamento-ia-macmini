{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"settlddotwork"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hey HN,<p>I built Settld because I kept running into the same problem: AI agents can call APIs, pay for services, and hire other agents - but there's no way to prove the work was actually done before the money moves.<p>The problem in one sentence: x402 tells you &quot;payment was sent&quot;. Settld tells you &quot;the work was worth paying for&quot;.<p>What it does<p>Settld sits between your <em>agent</em> and the APIs/agents it pays. It:<p>1. Intercepts HTTP 402 (Payment Required) responses\n2. Creates an escrow hold instead of paying immediately\n3. Collects evidence that the work was completed\n4. Runs deterministic verification (same evidence + same terms = same payout, every time)\n5. Releases payment only after verification passes\n6. Issues a cryptographically verifiable receipt<p>If verification fails or the work is disputed, the hold is refunded. The <em>agent</em> gets a receipt either way - a permanent, auditable record of what happened.<p>Why this matters now<p>We're at a weird inflection point. Coinbase shipped x402 (50M+ transactions). Google shipped A2A. Anthropic shipped MCP. Agents can discover each other, communicate, and pay each other.<p>But nobody built the layer that answers: &quot;was the work actually done correctly, and how much should the payout be?&quot;<p>That's the gap. Right now, every <em>agent</em>-to-<em>agent</em> transaction is either &quot;trust and hope&quot; or &quot;don't transact.&quot; Neither scales.<p>The x402 gateway (the fastest way to try it)<p>We ship a drop-in reverse proxy that you put in front of any API:<p>docker run -e UPSTREAM_URL=<a href=\"https://your-api.com\" rel=\"nofollow\">https://your-api.com</a> \\\n           -e SETTLD_API_URL=<a href=\"https://api.settld.dev\" rel=\"nofollow\">https://api.settld.dev</a> \\\n           -e SETTLD_API_KEY=sk_... \\\n           -p 8402:8402 \\\n           settld/x402-gateway<p>Everything flows through normally - except 402 responses get intercepted, escrowed, verified, and settled. Your <em>agent</em> gets a receipt with a hash-chained proof of what happened.<p>What's under the hood<p>The settlement kernel is the interesting part (and where we spent most of our time):<p>- Deterministic policy evaluation - machine-readable agreements with release rates based on verification status (green/amber/red). No ambiguity.\n- Hash-chained event log - every event in a settlement is chained with Ed25519 signatures. Tamper-evident, offline-verifiable.\n- Escrow with holdback windows - configurable holdback basis points + dispute windows. Funds auto-release if unchallenged.\n- Dispute \u2192 arbitration \u2192 verdict \u2192 adjustment - full dispute resolution pipeline, not just &quot;flag for human review.&quot;\n- Append-only reputation events - every settlement produces a reputation event (approved, rejected, disputed, etc.). Agents build verifiable economic track records.\n- Compositional settlement - agents can delegate work to sub-agents with linked agreements. If a downstream <em>agent</em> fails, refunds cascade deterministically back up the chain.<p>The whole protocol is spec'd with JSON schemas, conformance vectors, and a portable oracle: <a href=\"https://github.com/aidenlippert/settld/blob/main/docs/spec/README.md\" rel=\"nofollow\">https://github.com/aidenlippert/settld/blob/main/docs/spec/R...</a><p>What this is NOT<p>- Not a payment processor - we don't move money. We decide &quot;if&quot; and &quot;how much&quot; money should move, then your existing rails (Stripe, x402, wire) execute it.\n- Not a blockchain - deterministic receipts and hash chains, but no consensus mechanism or token. Just cryptographic proofs.\n- Not an <em>agent</em> <em>framework</em> - we don't care if you use LangChain, CrewAI, AutoGen, or raw API calls. We're a protocol layer.<p>Tech stack<p>Node.js, PostgreSQL (or in-memory for dev), Ed25519 signatures, SHA-256 hashing, RFC 8785 canonical JSON. ~107 core modules, 494 tests passing.<p>What I want from HN<p>Honest feedback on whether this problem resonates. If you're building <em>agent</em> workflows that involve money, I want to know: what breaks? What's missing? What would make you actually install this?<p>GitHub: <a href=\"https://github.com/aidenlippert/settld\" rel=\"nofollow\">https://github.com/aidenlippert/settld</a>\nDocs: <a href=\"https://docs.settld.work/\" rel=\"nofollow\">https://docs.settld.work/</a> \nQuickstart (10 min): <a href=\"https://docs.settld.work/quickstart\" rel=\"nofollow\">https://docs.settld.work/quickstart</a>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: Verify-before-release x402 gateway for AI <em>agent</em> transactions"}},"_tags":["story","author_settlddotwork","story_47011510","show_hn"],"author":"settlddotwork","created_at":"2026-02-14T04:17:17Z","created_at_i":1771042637,"num_comments":0,"objectID":"47011510","points":2,"story_id":47011510,"story_text":"Hey HN,<p>I built Settld because I kept running into the same problem: AI agents can call APIs, pay for services, and hire other agents - but there&#x27;s no way to prove the work was actually done before the money moves.<p>The problem in one sentence: x402 tells you &quot;payment was sent&quot;. Settld tells you &quot;the work was worth paying for&quot;.<p>What it does<p>Settld sits between your agent and the APIs&#x2F;agents it pays. It:<p>1. Intercepts HTTP 402 (Payment Required) responses\n2. Creates an escrow hold instead of paying immediately\n3. Collects evidence that the work was completed\n4. Runs deterministic verification (same evidence + same terms = same payout, every time)\n5. Releases payment only after verification passes\n6. Issues a cryptographically verifiable receipt<p>If verification fails or the work is disputed, the hold is refunded. The agent gets a receipt either way - a permanent, auditable record of what happened.<p>Why this matters now<p>We&#x27;re at a weird inflection point. Coinbase shipped x402 (50M+ transactions). Google shipped A2A. Anthropic shipped MCP. Agents can discover each other, communicate, and pay each other.<p>But nobody built the layer that answers: &quot;was the work actually done correctly, and how much should the payout be?&quot;<p>That&#x27;s the gap. Right now, every agent-to-agent transaction is either &quot;trust and hope&quot; or &quot;don&#x27;t transact.&quot; Neither scales.<p>The x402 gateway (the fastest way to try it)<p>We ship a drop-in reverse proxy that you put in front of any API:<p>docker run -e UPSTREAM_URL=<a href=\"https:&#x2F;&#x2F;your-api.com\" rel=\"nofollow\">https:&#x2F;&#x2F;your-api.com</a> \\\n           -e SETTLD_API_URL=<a href=\"https:&#x2F;&#x2F;api.settld.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;api.settld.dev</a> \\\n           -e SETTLD_API_KEY=sk_... \\\n           -p 8402:8402 \\\n           settld&#x2F;x402-gateway<p>Everything flows through normally - except 402 responses get intercepted, escrowed, verified, and settled. Your agent gets a receipt with a hash-chained proof of what happened.<p>What&#x27;s under the hood<p>The settlement kernel is the interesting part (and where we spent most of our time):<p>- Deterministic policy evaluation - machine-readable agreements with release rates based on verification status (green&#x2F;amber&#x2F;red). No ambiguity.\n- Hash-chained event log - every event in a settlement is chained with Ed25519 signatures. Tamper-evident, offline-verifiable.\n- Escrow with holdback windows - configurable holdback basis points + dispute windows. Funds auto-release if unchallenged.\n- Dispute \u2192 arbitration \u2192 verdict \u2192 adjustment - full dispute resolution pipeline, not just &quot;flag for human review.&quot;\n- Append-only reputation events - every settlement produces a reputation event (approved, rejected, disputed, etc.). Agents build verifiable economic track records.\n- Compositional settlement - agents can delegate work to sub-agents with linked agreements. If a downstream agent fails, refunds cascade deterministically back up the chain.<p>The whole protocol is spec&#x27;d with JSON schemas, conformance vectors, and a portable oracle: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld&#x2F;blob&#x2F;main&#x2F;docs&#x2F;spec&#x2F;README.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld&#x2F;blob&#x2F;main&#x2F;docs&#x2F;spec&#x2F;R...</a><p>What this is NOT<p>- Not a payment processor - we don&#x27;t move money. We decide &quot;if&quot; and &quot;how much&quot; money should move, then your existing rails (Stripe, x402, wire) execute it.\n- Not a blockchain - deterministic receipts and hash chains, but no consensus mechanism or token. Just cryptographic proofs.\n- Not an agent framework - we don&#x27;t care if you use LangChain, CrewAI, AutoGen, or raw API calls. We&#x27;re a protocol layer.<p>Tech stack<p>Node.js, PostgreSQL (or in-memory for dev), Ed25519 signatures, SHA-256 hashing, RFC 8785 canonical JSON. ~107 core modules, 494 tests passing.<p>What I want from HN<p>Honest feedback on whether this problem resonates. If you&#x27;re building agent workflows that involve money, I want to know: what breaks? What&#x27;s missing? What would make you actually install this?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aidenlippert&#x2F;settld</a>\nDocs: <a href=\"https:&#x2F;&#x2F;docs.settld.work&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.settld.work&#x2F;</a> \nQuickstart (10 min): <a href=\"https:&#x2F;&#x2F;docs.settld.work&#x2F;quickstart\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.settld.work&#x2F;quickstart</a>","title":"Show HN: Verify-before-release x402 gateway for AI agent transactions","updated_at":"2026-02-14T04:45:36Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mhcoen"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Most <em>agent</em> <em>frameworks</em> treat prompt injection as a model-level problem. In practice, once your <em>agent</em> ingests untrusted text and has tool access, you need application-layer controls \u2014 structural isolation, tool-call gating, exfiltration detection \u2014 that don't depend on the model behaving correctly. I built guardllm to provide those controls.\nguardllm is a small, auditable Python library that provides:<p>Inbound hardening: sanitize and structurally isolate untrusted content (web, email, docs, tool output) so it is treated as data, not instructions.\nTool-call firewall: deny-by-default destructive operations unless explicitly authorized; fail-closed confirmation when no confirmation handler is wired.\nRequest binding: bind (tool name, canonical args, message hash, TTL) to prevent replay and argument substitution.\nExfiltration detection: scans outbound tool arguments for secret patterns and flags substantial verbatim overlap with recently ingested untrusted content.\nProvenance tracking: enforces stricter no-copy rules on content with known untrusted origin, independent of the overlap heuristic.\nCanary tokens: per-session canary generation and detection to catch prompt leakage into outputs.\nSource gating: blocks high-risk sources from being promoted into long-lived memory or KG extraction to reduce memory poisoning.<p>It is intentionally minimal and not framework-specific. It does not replace least-privilege credentials or sandboxing \u2014 it sits above them.\nRepo: <a href=\"https://github.com/mhcoen/guardllm\" rel=\"nofollow\">https://github.com/mhcoen/guardllm</a>\nI'd like feedback on: what threat model gaps you see; whether the default overlap thresholds are reasonable for summarization and quoting workflows; and which framework adapters would make this easiest to adopt (LangChain, OpenAI tool calling, MCP proxy, etc.)."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: GuardLLM, hardened tool calls for LLM apps"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/mhcoen/guardllm"}},"_tags":["story","author_mhcoen","story_47010964","show_hn"],"author":"mhcoen","created_at":"2026-02-14T02:36:59Z","created_at_i":1771036619,"num_comments":0,"objectID":"47010964","points":1,"story_id":47010964,"story_text":"Most agent frameworks treat prompt injection as a model-level problem. In practice, once your agent ingests untrusted text and has tool access, you need application-layer controls \u2014 structural isolation, tool-call gating, exfiltration detection \u2014 that don&#x27;t depend on the model behaving correctly. I built guardllm to provide those controls.\nguardllm is a small, auditable Python library that provides:<p>Inbound hardening: sanitize and structurally isolate untrusted content (web, email, docs, tool output) so it is treated as data, not instructions.\nTool-call firewall: deny-by-default destructive operations unless explicitly authorized; fail-closed confirmation when no confirmation handler is wired.\nRequest binding: bind (tool name, canonical args, message hash, TTL) to prevent replay and argument substitution.\nExfiltration detection: scans outbound tool arguments for secret patterns and flags substantial verbatim overlap with recently ingested untrusted content.\nProvenance tracking: enforces stricter no-copy rules on content with known untrusted origin, independent of the overlap heuristic.\nCanary tokens: per-session canary generation and detection to catch prompt leakage into outputs.\nSource gating: blocks high-risk sources from being promoted into long-lived memory or KG extraction to reduce memory poisoning.<p>It is intentionally minimal and not framework-specific. It does not replace least-privilege credentials or sandboxing \u2014 it sits above them.\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mhcoen&#x2F;guardllm\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mhcoen&#x2F;guardllm</a>\nI&#x27;d like feedback on: what threat model gaps you see; whether the default overlap thresholds are reasonable for summarization and quoting workflows; and which framework adapters would make this easiest to adopt (LangChain, OpenAI tool calling, MCP proxy, etc.).","title":"Show HN: GuardLLM, hardened tool calls for LLM apps","updated_at":"2026-02-14T02:38:05Z","url":"https://github.com/mhcoen/guardllm"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"hckdisc"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"We built TrustVector (trustvector.dev for website) because \u201cwhich model/<em>agent</em>/tool should we trust?\u201d keeps getting answered with vibes, marketing, or outdated benchmarks. And a lot of our enterprise customers kept asking about it.<p>TrustVector is an open-source evaluation <em>framework</em> + public directory where each system gets a multi-dimensional trust score across:\n- Security (prompt injection/jailbreak resistance, data leakage)\n- Privacy &amp; compliance\n- Trust &amp; transparency (hallucination/bias, documentation quality)\n- Performance &amp; reliability\n- Operational excellence<p>Key idea: every score is evidence-based (sources + confidence), and you can re-weight dimensions CVSS-style depending on your use case.<p>Current coverage: 100+ evaluations across models, <em>agents</em>, and MCP servers.<p>GitHub + methodology are linked from the site. I\u2019d love feedback on:\n1) whether the dimensions/weighting are sane,\n2) what evidence sources we\u2019re missing,\n3) What contribution workflow would make this actually community-maintained?<p>(Also: this project is not affiliated with trustvector.ai.)"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: TrustVector \u2013 Trust evaluations for AI models, <em>agents</em>, & MCP"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/guard0-ai/TrustVector"}},"_tags":["story","author_hckdisc","story_47008687","show_hn"],"author":"hckdisc","children":[47008700],"created_at":"2026-02-13T22:28:34Z","created_at_i":1771021714,"num_comments":1,"objectID":"47008687","points":2,"story_id":47008687,"story_text":"We built TrustVector (trustvector.dev for website) because \u201cwhich model&#x2F;agent&#x2F;tool should we trust?\u201d keeps getting answered with vibes, marketing, or outdated benchmarks. And a lot of our enterprise customers kept asking about it.<p>TrustVector is an open-source evaluation framework + public directory where each system gets a multi-dimensional trust score across:\n- Security (prompt injection&#x2F;jailbreak resistance, data leakage)\n- Privacy &amp; compliance\n- Trust &amp; transparency (hallucination&#x2F;bias, documentation quality)\n- Performance &amp; reliability\n- Operational excellence<p>Key idea: every score is evidence-based (sources + confidence), and you can re-weight dimensions CVSS-style depending on your use case.<p>Current coverage: 100+ evaluations across models, agents, and MCP servers.<p>GitHub + methodology are linked from the site. I\u2019d love feedback on:\n1) whether the dimensions&#x2F;weighting are sane,\n2) what evidence sources we\u2019re missing,\n3) What contribution workflow would make this actually community-maintained?<p>(Also: this project is not affiliated with trustvector.ai.)","title":"Show HN: TrustVector \u2013 Trust evaluations for AI models, agents, & MCP","updated_at":"2026-02-13T23:03:07Z","url":"https://github.com/guard0-ai/TrustVector"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"xmpir"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"With all the <em>agents</em> coming into existance I am wondering if there are any good OSS projects published by <em>agents</em> gaining traction. Web <em>Frameworks</em>, libraries or even programming languages?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: Any useful open source software maintained or created by AI?"}},"_tags":["story","author_xmpir","story_47008251","ask_hn"],"author":"xmpir","created_at":"2026-02-13T21:44:35Z","created_at_i":1771019075,"num_comments":0,"objectID":"47008251","points":3,"story_id":47008251,"story_text":"With all the agents coming into existance I am wondering if there are any good OSS projects published by agents gaining traction. Web Frameworks, libraries or even programming languages?","title":"Ask HN: Any useful open source software maintained or created by AI?","updated_at":"2026-02-13T22:01:06Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"stcredzero"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Show HN: SEKSBot \u2013 AI <em>agents</em> that can't see your secrets<p>SEKSBot is a fork of OpenClaw where <em>agents</em> have zero access to API keys, tokens, or credentials \u2014 ever.<p>The core insight is borrowed from SQL prepared statements: separate the instructions from the sensitive data. <em>Agents</em> write requests using named secret references. A broker intercepts and injects the real credentials at execution time. The <em>agent</em> never sees them.<p>How it works:<p>seksh (our nushell fork) has secure built-in commands (seksh-http, seksh-git) that route through the broker. <em>Agents</em> can make authenticated API calls and git operations without the keys ever entering shell memory.<p>seks-broker stores secrets and acts as a proxy. It can inject bearer tokens, API keys, and even handle asymmetric key signing \u2014 all without exposing anything to the <em>agent</em> process.<p>Three layers of defense: (1) <em>Agents</em> never have secrets in env vars or memory. (2) The broker validates and scopes every request. (3) Skills use sandboxing on top of broker-mediated access.<p>The problem we kept seeing: every AI <em>agent</em> <em>framework</em> puts API keys in environment variables. One prompt injection, one malicious webpage, one bad skill \u2014 and your keys are exfiltrated. We decided the only real fix is making it physically impossible for the <em>agent</em> to access them."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: My <em>agents</em> are building a secure fork of OpenClaw"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://seksbot.com/"}},"_tags":["story","author_stcredzero","story_47005607","show_hn"],"author":"stcredzero","created_at":"2026-02-13T17:59:34Z","created_at_i":1771005574,"num_comments":0,"objectID":"47005607","points":9,"story_id":47005607,"story_text":"Show HN: SEKSBot \u2013 AI agents that can&#x27;t see your secrets<p>SEKSBot is a fork of OpenClaw where agents have zero access to API keys, tokens, or credentials \u2014 ever.<p>The core insight is borrowed from SQL prepared statements: separate the instructions from the sensitive data. Agents write requests using named secret references. A broker intercepts and injects the real credentials at execution time. The agent never sees them.<p>How it works:<p>seksh (our nushell fork) has secure built-in commands (seksh-http, seksh-git) that route through the broker. Agents can make authenticated API calls and git operations without the keys ever entering shell memory.<p>seks-broker stores secrets and acts as a proxy. It can inject bearer tokens, API keys, and even handle asymmetric key signing \u2014 all without exposing anything to the agent process.<p>Three layers of defense: (1) Agents never have secrets in env vars or memory. (2) The broker validates and scopes every request. (3) Skills use sandboxing on top of broker-mediated access.<p>The problem we kept seeing: every AI agent framework puts API keys in environment variables. One prompt injection, one malicious webpage, one bad skill \u2014 and your keys are exfiltrated. We decided the only real fix is making it physically impossible for the agent to access them.","title":"Show HN: My agents are building a secure fork of OpenClaw","updated_at":"2026-02-14T04:03:52Z","url":"https://seksbot.com/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mosaxiv"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Inspired by OpenClaw and Nanobot, I built a lightweight and highly efficient <em>agent</em> <em>framework</em> in Go.<p>While there are other projects that claim high performance, this <em>framework</em> focuses not only on speed but also on richer functionality and practical efficiency. It aims to provide a more complete and streamlined experience without sacrificing performance."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Clawlet \u2013 Ultra-Lightweight&Efficient Alternative to OpenClaw, Nanobot"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/mosaxiv/clawlet"}},"_tags":["story","author_mosaxiv","story_47005383","show_hn"],"author":"mosaxiv","created_at":"2026-02-13T17:40:06Z","created_at_i":1771004406,"num_comments":0,"objectID":"47005383","points":3,"story_id":47005383,"story_text":"Inspired by OpenClaw and Nanobot, I built a lightweight and highly efficient agent framework in Go.<p>While there are other projects that claim high performance, this framework focuses not only on speed but also on richer functionality and practical efficiency. It aims to provide a more complete and streamlined experience without sacrificing performance.","title":"Show HN: Clawlet \u2013 Ultra-Lightweight&Efficient Alternative to OpenClaw, Nanobot","updated_at":"2026-02-13T23:29:51Z","url":"https://github.com/mosaxiv/clawlet"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hi everyone,<p>I am Vincenzo and i\u2019m working on PolyMCP, an open-source <em>framework</em> that not only exposes Python functions as AI-callable MCP tools but also lets you orchestrate <em>agents</em> across multiple MCP servers.<p>The idea: instead of rewriting code or wrapping every function with a special SDK, you can:\n 1. Publish your existing Python functions as MCP tools automatically\n 2. Spin up a UnifiedPolyAgent that coordinates multiple MCP servers\n 3. Ask your <em>agent</em> to perform complex workflows spanning different tools<p>Here\u2019s a quick example in Python:<p>from polymcp.polyagent import UnifiedPolyAgent, OpenAIProvider<p><em>agent</em> = UnifiedPolyAgent(\n    llm_provider=OpenAIProvider(model=&quot;gpt-4o-mini&quot;),\n    mcp_servers=[\n        &quot;http://localhost:8000/mcp&quot;,\n        &quot;http://localhost:8001/mcp&quot;,\n    ],\n    verbose=True,\n)<p>answer = <em>agent</em>.run(&quot;Read sales data, compute totals, then summarize.&quot;)\nprint(answer)<p>Or TypeScript, combining HTTP and stdio-based MCP tools:<p>import { UnifiedPolyAgent, OpenAIProvider } from 'polymcp-ts';<p>const <em>agent</em> = new UnifiedPolyAgent({\n  llmProvider: new OpenAIProvider({\n    apiKey: process.env.OPENAI_API_KEY!,\n    model: 'gpt-4o-mini',\n  }),\n  mcpServers: ['http://localhost:3000/mcp'],\n  stdioServers: [{ command: 'npx', args: ['@playwright/mcp@latest'] }],\n  verbose: true,\n});<p>await <em>agent</em>.start();\nconst answer = await <em>agent</em>.run('Collect data and summarize.');\nconsole.log(answer);<p>Use cases:\n \u2022 Aggregate data from multiple internal services and scripts\n \u2022 Build AI copilots that span different tools and languages\n \u2022 Automate multi-step operational workflows\n \u2022 Prototype <em>agents</em> that interact with production systems safely<p>Works with OpenAI, Anthropic, and Ollama models, including local deployments.<p>GitHub links:\n \u2022 Core &amp; <em>Agent</em>: <a href=\"https://github.com/poly-mcp/PolyMCP\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP</a> \n \u2022 Inspector: <a href=\"https://github.com/poly-mcp/PolyMCP-Inspector\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-Inspector</a> \n \u2022 SDK Apps: <a href=\"https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP-MCP-SDK-Apps</a><p>I\u2019d love feedback from anyone exploring <em>agent</em> orchestration or building multi-tool AI pipelines."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: PolyMCP \u2013 Orchestrate AI <em>agents</em> across Python tools and MCP servers"}},"_tags":["story","author_justvugg","story_47004468","show_hn"],"author":"justvugg","created_at":"2026-02-13T16:23:05Z","created_at_i":1770999785,"num_comments":0,"objectID":"47004468","points":1,"story_id":47004468,"story_text":"Hi everyone,<p>I am Vincenzo and i\u2019m working on PolyMCP, an open-source framework that not only exposes Python functions as AI-callable MCP tools but also lets you orchestrate agents across multiple MCP servers.<p>The idea: instead of rewriting code or wrapping every function with a special SDK, you can:\n 1. Publish your existing Python functions as MCP tools automatically\n 2. Spin up a UnifiedPolyAgent that coordinates multiple MCP servers\n 3. Ask your agent to perform complex workflows spanning different tools<p>Here\u2019s a quick example in Python:<p>from polymcp.polyagent import UnifiedPolyAgent, OpenAIProvider<p>agent = UnifiedPolyAgent(\n    llm_provider=OpenAIProvider(model=&quot;gpt-4o-mini&quot;),\n    mcp_servers=[\n        &quot;http:&#x2F;&#x2F;localhost:8000&#x2F;mcp&quot;,\n        &quot;http:&#x2F;&#x2F;localhost:8001&#x2F;mcp&quot;,\n    ],\n    verbose=True,\n)<p>answer = agent.run(&quot;Read sales data, compute totals, then summarize.&quot;)\nprint(answer)<p>Or TypeScript, combining HTTP and stdio-based MCP tools:<p>import { UnifiedPolyAgent, OpenAIProvider } from &#x27;polymcp-ts&#x27;;<p>const agent = new UnifiedPolyAgent({\n  llmProvider: new OpenAIProvider({\n    apiKey: process.env.OPENAI_API_KEY!,\n    model: &#x27;gpt-4o-mini&#x27;,\n  }),\n  mcpServers: [&#x27;http:&#x2F;&#x2F;localhost:3000&#x2F;mcp&#x27;],\n  stdioServers: [{ command: &#x27;npx&#x27;, args: [&#x27;@playwright&#x2F;mcp@latest&#x27;] }],\n  verbose: true,\n});<p>await agent.start();\nconst answer = await agent.run(&#x27;Collect data and summarize.&#x27;);\nconsole.log(answer);<p>Use cases:\n \u2022 Aggregate data from multiple internal services and scripts\n \u2022 Build AI copilots that span different tools and languages\n \u2022 Automate multi-step operational workflows\n \u2022 Prototype agents that interact with production systems safely<p>Works with OpenAI, Anthropic, and Ollama models, including local deployments.<p>GitHub links:\n \u2022 Core &amp; Agent: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP</a> \n \u2022 Inspector: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-Inspector</a> \n \u2022 SDK Apps: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP-MCP-SDK-Apps</a><p>I\u2019d love feedback from anyone exploring agent orchestration or building multi-tool AI pipelines.","title":"Show HN: PolyMCP \u2013 Orchestrate AI agents across Python tools and MCP servers","updated_at":"2026-02-13T16:25:05Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"exordex"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"Hey HN,\n                                                                                                                                      If you're building LangChain agents, you've probably seen them break in creative ways - prompt injection bypassing your chain logic, tools getting called with bad parameters, or cascading failures when an API times out mid-chain.<p>I built Khaos to test these failure modes before production.<p>Example LangChain <em>agent</em>:\n  ```python\n  from langchain.agents import AgentExecutor, create_openai_functions_<em>agent</em>\n  from khaos import khaosagent<p><pre><code>  @khaosagent(name=&quot;research-<em>agent</em>&quot;, <em>framework</em>=&quot;langgraph&quot;)\n  def <em>agent</em>(query: str) -&gt; dict:\n      executor = AgentExecutor(<em>agent</em>=<em>agent</em>, tools=tools)\n      result = executor.invoke({&quot;input&quot;: query})\n      return {&quot;response&quot;: result[&quot;output&quot;]}\n</code></pre>\nTest it:\n  pip install khaos-<em>agent</em>\n  khaos discover\n  khaos run research-<em>agent</em> --pack security<p>Khaos injects:\n  - 242+ security attacks - Prompt injection variations that bypass LangChain's prompt templates\n  - Tool misuse - Malicious parameters in tool calls (e.g., os.system injection in code execution tools)\n  - Chain failures - What happens when your 3rd step in a 5-step chain times out?\n  - LLM faults - Rate limits, token overflows, model unavailability<p><pre><code>  Why this matters for LangChain specifically:\n\n  LangChain's abstraction layers can hide vulnerabilities:\n  - Prompt templates can still be injected via tool outputs\n  - AgentExecutor doesn't validate tool parameters\n  - Chains fail silently or propagate corrupted state\n  - ReAct/Plan-and-Execute patterns have unique attack surfaces\n\n  Works with LangGraph, LCEL chains, and classic LangChain agents. Auto-instruments your chains to inject faults at each step.\n\n  Repo: https://github.com/ExordexLabs/khaos-sdk\n  Examples: https://github.com/ExordexLabs/khaos-examples/tree/master/code-execution-<em>agent</em></code></pre>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"LangChain <em>Agent</em> Testing Guide Tool (Free)"}},"_tags":["story","author_exordex","story_47004292","ask_hn"],"author":"exordex","created_at":"2026-02-13T16:07:25Z","created_at_i":1770998845,"num_comments":0,"objectID":"47004292","points":1,"story_id":47004292,"story_text":"Hey HN,\n                                                                                                                                      If you&#x27;re building LangChain agents, you&#x27;ve probably seen them break in creative ways - prompt injection bypassing your chain logic, tools getting called with bad parameters, or cascading failures when an API times out mid-chain.<p>I built Khaos to test these failure modes before production.<p>Example LangChain agent:\n  ```python\n  from langchain.agents import AgentExecutor, create_openai_functions_agent\n  from khaos import khaosagent<p><pre><code>  @khaosagent(name=&quot;research-agent&quot;, framework=&quot;langgraph&quot;)\n  def agent(query: str) -&gt; dict:\n      executor = AgentExecutor(agent=agent, tools=tools)\n      result = executor.invoke({&quot;input&quot;: query})\n      return {&quot;response&quot;: result[&quot;output&quot;]}\n</code></pre>\nTest it:\n  pip install khaos-agent\n  khaos discover\n  khaos run research-agent --pack security<p>Khaos injects:\n  - 242+ security attacks - Prompt injection variations that bypass LangChain&#x27;s prompt templates\n  - Tool misuse - Malicious parameters in tool calls (e.g., os.system injection in code execution tools)\n  - Chain failures - What happens when your 3rd step in a 5-step chain times out?\n  - LLM faults - Rate limits, token overflows, model unavailability<p><pre><code>  Why this matters for LangChain specifically:\n\n  LangChain&#x27;s abstraction layers can hide vulnerabilities:\n  - Prompt templates can still be injected via tool outputs\n  - AgentExecutor doesn&#x27;t validate tool parameters\n  - Chains fail silently or propagate corrupted state\n  - ReAct&#x2F;Plan-and-Execute patterns have unique attack surfaces\n\n  Works with LangGraph, LCEL chains, and classic LangChain agents. Auto-instruments your chains to inject faults at each step.\n\n  Repo: https:&#x2F;&#x2F;github.com&#x2F;ExordexLabs&#x2F;khaos-sdk\n  Examples: https:&#x2F;&#x2F;github.com&#x2F;ExordexLabs&#x2F;khaos-examples&#x2F;tree&#x2F;master&#x2F;code-execution-agent</code></pre>","title":"LangChain Agent Testing Guide Tool (Free)","updated_at":"2026-02-13T16:09:21Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"orbydx"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"I built 75 developer and AI tools as a single static site. Everything runs in the browser, no cookies, no ads, nothing gets sent to a server.<p>The tools range from the usual suspects (JSON formatter, base64 encoder, regex tester) to some AI-specific ones I couldn't find good free versions of bundled in one suite:<p>- LLM Token Counter (estimates tokens for GPT, Claude, Gemini, etc.)\n- AI Model Comparison (specs, pricing, context windows side by side)\n- AI Cost Estimator (plug in your usage, get monthly cost projections)\n- MCP Server Directory (browsable catalog of Model Context Protocol servers)\n- <em>Agent</em> <em>Framework</em> Comparison (LangChain vs CrewAI vs AutoGen vs...)\n- Prompt Template Builder (variables, conditionals, versioning)\n- Markdown Memory Generator (for OpenClaw)<p>Plus the standard dev toolkit: JWT decoder, cron expression builder, diff checker, SQL formatter, color converter, CSS flexbox playground, etc.<p>Tech stack: Astro 5 with React islands, Tailwind CSS 4, hosted on Cloudflare Pages. The whole site is static, so it loads fast everywhere. Largest JS bundle is 58 KB gzipped.<p>I built this with AI <em>agent</em> Rusty (OpenClaw). The AI handled most of the component code while I focused on architecture decisions, tool selection, and QA. Took about 2 days of evening sessions.<p>No login, no tracking cookies, no ads, no &quot;sign up for premium&quot;. Just tools.<p>Feedback welcome. What ai or dev tools do you wish existed that don't?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: AI Dev Hub. 75 free AI and dev tools"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://aidevhub.io/"}},"_tags":["story","author_orbydx","story_47003685","show_hn"],"author":"orbydx","created_at":"2026-02-13T15:20:44Z","created_at_i":1770996044,"num_comments":0,"objectID":"47003685","points":1,"story_id":47003685,"story_text":"I built 75 developer and AI tools as a single static site. Everything runs in the browser, no cookies, no ads, nothing gets sent to a server.<p>The tools range from the usual suspects (JSON formatter, base64 encoder, regex tester) to some AI-specific ones I couldn&#x27;t find good free versions of bundled in one suite:<p>- LLM Token Counter (estimates tokens for GPT, Claude, Gemini, etc.)\n- AI Model Comparison (specs, pricing, context windows side by side)\n- AI Cost Estimator (plug in your usage, get monthly cost projections)\n- MCP Server Directory (browsable catalog of Model Context Protocol servers)\n- Agent Framework Comparison (LangChain vs CrewAI vs AutoGen vs...)\n- Prompt Template Builder (variables, conditionals, versioning)\n- Markdown Memory Generator (for OpenClaw)<p>Plus the standard dev toolkit: JWT decoder, cron expression builder, diff checker, SQL formatter, color converter, CSS flexbox playground, etc.<p>Tech stack: Astro 5 with React islands, Tailwind CSS 4, hosted on Cloudflare Pages. The whole site is static, so it loads fast everywhere. Largest JS bundle is 58 KB gzipped.<p>I built this with AI agent Rusty (OpenClaw). The AI handled most of the component code while I focused on architecture decisions, tool selection, and QA. Took about 2 days of evening sessions.<p>No login, no tracking cookies, no ads, no &quot;sign up for premium&quot;. Just tools.<p>Feedback welcome. What ai or dev tools do you wish existed that don&#x27;t?","title":"Show HN: AI Dev Hub. 75 free AI and dev tools","updated_at":"2026-02-13T15:22:05Z","url":"https://aidevhub.io/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"jovanaccount"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["agent","frameworks"],"value":"I built this because standard Python locks (asyncio.Lock) don't work when AI agents run across different containers or processes.<p>As I started scaling my <em>agent</em> swarms (using CrewAI/LangChain), I kept hitting race conditions where agents would overwrite shared files or double-spend API credits because they read the state simultaneously.<p>Network-AI is a distributed lock (backed by Redis or local file-locks) that solves this. It acts as a traffic light for <em>agent</em> tools.<p>Key features:<p>2-line decorator: @lock(&quot;resource_id&quot;)<p>Handles lock timeouts (preventing &quot;zombie&quot; agents from freezing the swarm)<p>Works with standard <em>agent</em> <em>frameworks</em> (CrewAI, AutoGen, LangGraph)<p>The repo is open-source. I'd love feedback on the locking implementation or to hear if you've hit similar concurrency issues in production."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agent"],"value":"Show HN: Network-AI \u2013 A Distributed Mutex for AI <em>Agent</em> Swarms"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/jovanSAPFIONEER/Network-AI"}},"_tags":["story","author_jovanaccount","story_47003226","show_hn"],"author":"jovanaccount","created_at":"2026-02-13T14:37:05Z","created_at_i":1770993425,"num_comments":0,"objectID":"47003226","points":1,"story_id":47003226,"story_text":"I built this because standard Python locks (asyncio.Lock) don&#x27;t work when AI agents run across different containers or processes.<p>As I started scaling my agent swarms (using CrewAI&#x2F;LangChain), I kept hitting race conditions where agents would overwrite shared files or double-spend API credits because they read the state simultaneously.<p>Network-AI is a distributed lock (backed by Redis or local file-locks) that solves this. It acts as a traffic light for agent tools.<p>Key features:<p>2-line decorator: @lock(&quot;resource_id&quot;)<p>Handles lock timeouts (preventing &quot;zombie&quot; agents from freezing the swarm)<p>Works with standard agent frameworks (CrewAI, AutoGen, LangGraph)<p>The repo is open-source. I&#x27;d love feedback on the locking implementation or to hear if you&#x27;ve hit similar concurrency issues in production.","title":"Show HN: Network-AI \u2013 A Distributed Mutex for AI Agent Swarms","updated_at":"2026-02-13T14:44:07Z","url":"https://github.com/jovanSAPFIONEER/Network-AI"}],"hitsPerPage":10,"nbHits":1205,"nbPages":100,"page":0,"params":"query=agent+frameworks&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":33,"processingTimingsMS":{"_request":{"roundTrip":22},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":6,"scanning":26,"total":33},"total":33},"query":"agent frameworks","serverTimeMS":35}
