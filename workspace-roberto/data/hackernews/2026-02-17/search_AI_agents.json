{"exhaustive":{"nbHits":true,"typo":true},"exhaustiveNbHits":true,"exhaustiveTypo":true,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"lukasz_ma"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"I run an OpenClaw <em>agent</em> named Jimmy. Jimmy handles SEO tracking, content writing, and code tasks for my projects. Managing it all entirely through chat got messy fast.<p>Thus I built VidClaw. It's a self-hosted dashboard that gives you:<p>- Kanban board where you drag tasks and the <em>agent</em> picks them up automatically\n- Real-time token usage\n- Model switching \n- Soul editor \n- Skills manager\n- Activity calendar<p>The whole thing binds to localhost only \u2014 you access it through an SSH tunnel. No accounts, no cloud, no tracking. Your data stays on your machine.<p>Stack: React + Vite + Tailwind on the frontend, Express on the backend, JSON files for storage (no database). Setup is one script.<p>I built this for myself, but figured others running autonomous <em>agents</em> might find it useful. Would love feedback on the approach - especially from anyone else managing long-running <em>AI</em> <em>agents</em>."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: VidClaw \u2013 Open-source, self-hosted dashboard for managing OpenClaw"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://vidclaw.com"}},"_tags":["story","author_lukasz_ma","story_47046625","show_hn"],"author":"lukasz_ma","created_at":"2026-02-17T12:08:04Z","created_at_i":1771330084,"num_comments":0,"objectID":"47046625","points":2,"story_id":47046625,"story_text":"I run an OpenClaw agent named Jimmy. Jimmy handles SEO tracking, content writing, and code tasks for my projects. Managing it all entirely through chat got messy fast.<p>Thus I built VidClaw. It&#x27;s a self-hosted dashboard that gives you:<p>- Kanban board where you drag tasks and the agent picks them up automatically\n- Real-time token usage\n- Model switching \n- Soul editor \n- Skills manager\n- Activity calendar<p>The whole thing binds to localhost only \u2014 you access it through an SSH tunnel. No accounts, no cloud, no tracking. Your data stays on your machine.<p>Stack: React + Vite + Tailwind on the frontend, Express on the backend, JSON files for storage (no database). Setup is one script.<p>I built this for myself, but figured others running autonomous agents might find it useful. Would love feedback on the approach - especially from anyone else managing long-running AI agents.","title":"Show HN: VidClaw \u2013 Open-source, self-hosted dashboard for managing OpenClaw","updated_at":"2026-02-17T12:22:17Z","url":"https://vidclaw.com"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"jhoxray"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"OneRingAI started as the internal engine of an enterprise agentic platform we've been building for 2+ years. After watching customers hit the same walls with auth, vendor lock-in, and context management over and over, we extracted the core into a standalone open-source library.\nThe two main alternatives didn't fit what we needed in production:<p>- LangChain: Great ecosystem, but the abstraction layers kept growing. By the time you wire up chains, runnables, callbacks, and <em>agents</em> across 50+ packages, you're fighting the framework\n  more than building your product.\n- CrewAI: Clean API, but Python-only and the role-based metaphor breaks down when you need fine-grained control over auth, context windows, or tool failures.<p>OneRingAI is a single TypeScript library (~62K LOC, 20 deps) that treats the boring production problems as first-class concerns:<p>Auth as architecture, not afterthought. A centralized connector registry with built-in OAuth (4 flows, AES-256-GCM storage, 43 vendor templates). This came directly from dealing with\nenterprise SSO and multi-tenant token isolation \u2014 no more scattered env vars or rolling your own token refresh.<p>Per-tool circuit breakers. One flaky Jira API shouldn't crash your entire agent loop. Each tool and connector gets independent failure isolation with retry/backoff. We learned this the\nhard way running <em>agents</em> against dozens of customer SaaS integrations simultaneously.<p>Context that doesn't blow up. Plugin-based context management with token budgeting. InContextMemory puts frequently-accessed state directly in the prompt instead of requiring a retrieval\ncall. Compaction removes tool call/result pairs together so the LLM never sees orphaned context.<p>Actually multi-vendor. 12 LLM providers native, 36 models in a typed registry with pricing and feature flags. Switch vendors by changing a connector name. Run openai-prod and\nopenai-backup side by side. Enterprise customers kept asking for this \u2014 nobody wants to be locked into one provider.<p>Multi-modal built in. Image gen (DALL-E 3, gpt-image-1, Imagen 4), video gen (Sora 2, Veo 3), TTS, STT \u2014 all in the same library. No extra packages.<p>Native MCP support with a registry pattern for managing multiple servers, health checks, and auto tool format conversion.<p>What it's not: it's not a no-code agent builder, and it's not trying to be a framework for every possible <em>AI</em> use case. It's an opinionated library for people building production agent\nsystems in TypeScript who want auth, resilience, and multi-vendor support without duct-taping 15 packages together.<p>2,285 tests, strict TypeScript throughout. The API surface is small on purpose \u2014 Connector.create(), Agent.create(), agent.run().<p>We also built Hosea, an open-source Electron desktop app on top of OneRingAI, if you want to see what a full agent system looks like in practice rather than just reading docs.<p>GitHub: <a href=\"https://github.com/Integrail/oneringai\" rel=\"nofollow\">https://github.com/Integrail/oneringai</a><p>npm: npm i @everworker/oneringai<p>Comparison with alternatives: <a href=\"https://oneringai.io/#comparison\" rel=\"nofollow\">https://oneringai.io/#comparison</a><p>Hosea: <a href=\"https://github.com/Integrail/oneringai/blob/main/apps/hosea/\" rel=\"nofollow\">https://github.com/Integrail/oneringai/blob/main/apps/hosea/</a>...<p>Happy to answer questions about the architecture decisions."},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"Show HN: OneRingAI \u2013 Single TypeScript library for multi-vendor <em>AI</em> <em>agents</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://oneringai.io"}},"_tags":["story","author_jhoxray","story_47046494","show_hn"],"author":"jhoxray","created_at":"2026-02-17T11:48:29Z","created_at_i":1771328909,"num_comments":0,"objectID":"47046494","points":4,"story_id":47046494,"story_text":"OneRingAI started as the internal engine of an enterprise agentic platform we&#x27;ve been building for 2+ years. After watching customers hit the same walls with auth, vendor lock-in, and context management over and over, we extracted the core into a standalone open-source library.\nThe two main alternatives didn&#x27;t fit what we needed in production:<p>- LangChain: Great ecosystem, but the abstraction layers kept growing. By the time you wire up chains, runnables, callbacks, and agents across 50+ packages, you&#x27;re fighting the framework\n  more than building your product.\n- CrewAI: Clean API, but Python-only and the role-based metaphor breaks down when you need fine-grained control over auth, context windows, or tool failures.<p>OneRingAI is a single TypeScript library (~62K LOC, 20 deps) that treats the boring production problems as first-class concerns:<p>Auth as architecture, not afterthought. A centralized connector registry with built-in OAuth (4 flows, AES-256-GCM storage, 43 vendor templates). This came directly from dealing with\nenterprise SSO and multi-tenant token isolation \u2014 no more scattered env vars or rolling your own token refresh.<p>Per-tool circuit breakers. One flaky Jira API shouldn&#x27;t crash your entire agent loop. Each tool and connector gets independent failure isolation with retry&#x2F;backoff. We learned this the\nhard way running agents against dozens of customer SaaS integrations simultaneously.<p>Context that doesn&#x27;t blow up. Plugin-based context management with token budgeting. InContextMemory puts frequently-accessed state directly in the prompt instead of requiring a retrieval\ncall. Compaction removes tool call&#x2F;result pairs together so the LLM never sees orphaned context.<p>Actually multi-vendor. 12 LLM providers native, 36 models in a typed registry with pricing and feature flags. Switch vendors by changing a connector name. Run openai-prod and\nopenai-backup side by side. Enterprise customers kept asking for this \u2014 nobody wants to be locked into one provider.<p>Multi-modal built in. Image gen (DALL-E 3, gpt-image-1, Imagen 4), video gen (Sora 2, Veo 3), TTS, STT \u2014 all in the same library. No extra packages.<p>Native MCP support with a registry pattern for managing multiple servers, health checks, and auto tool format conversion.<p>What it&#x27;s not: it&#x27;s not a no-code agent builder, and it&#x27;s not trying to be a framework for every possible AI use case. It&#x27;s an opinionated library for people building production agent\nsystems in TypeScript who want auth, resilience, and multi-vendor support without duct-taping 15 packages together.<p>2,285 tests, strict TypeScript throughout. The API surface is small on purpose \u2014 Connector.create(), Agent.create(), agent.run().<p>We also built Hosea, an open-source Electron desktop app on top of OneRingAI, if you want to see what a full agent system looks like in practice rather than just reading docs.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai</a><p>npm: npm i @everworker&#x2F;oneringai<p>Comparison with alternatives: <a href=\"https:&#x2F;&#x2F;oneringai.io&#x2F;#comparison\" rel=\"nofollow\">https:&#x2F;&#x2F;oneringai.io&#x2F;#comparison</a><p>Hosea: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai&#x2F;blob&#x2F;main&#x2F;apps&#x2F;hosea&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Integrail&#x2F;oneringai&#x2F;blob&#x2F;main&#x2F;apps&#x2F;hosea&#x2F;</a>...<p>Happy to answer questions about the architecture decisions.","title":"Show HN: OneRingAI \u2013 Single TypeScript library for multi-vendor AI agents","updated_at":"2026-02-17T12:43:18Z","url":"https://oneringai.io"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"srboyd"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agents"],"value":"Your Backlog Can't Keep Up with Your <em>Agents</em>"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"https://samboyd.dev/blog/<em>ai</em>-product-engineer"}},"_tags":["story","author_srboyd","story_47046403"],"author":"srboyd","created_at":"2026-02-17T11:37:00Z","created_at_i":1771328220,"num_comments":0,"objectID":"47046403","points":1,"story_id":47046403,"title":"Your Backlog Can't Keep Up with Your Agents","updated_at":"2026-02-17T11:41:17Z","url":"https://samboyd.dev/blog/ai-product-engineer"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"geo_leo"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"hey HN, my wife (a pediatrician) and I built a breast milk tracking app<p>Existing apps do the basics fine - log feeds, set timers. But they haven't caught up with what <em>AI</em> makes possible now...<p>What <em>AI</em> adds:\n- Photo scanning \u2014 photograph your milk bags and <em>AI</em> reads the volumes instantly (Gemini Vision)\n- Feeding predictions \u2014 predicts when baby will be hungry next based on historical patterns\n- Chat <em>agent</em> \u2014 manages your data conversationally: &quot;log a 120ml pump this morning,&quot; &quot;what's expiring soon?&quot; It uses CLI commands under the hood, so it has full read/write access<p>The core app:<p>- Log pumping, breastfeeding, and formula feedings\n- Track freezer/fridge inventory with automatic expiration alerts\n- Household sync so your partner sees everything in realtime\n- Full CLI with the same capabilities as the UI<p>Free and open to all. <a href=\"https://stash-ruby.vercel.app\" rel=\"nofollow\">https://stash-ruby.vercel.app</a><p>I'd welcome feedback :)"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"Show HN: Nectar Gold \u2013 Breastmilk tracker where an <em>AI</em> <em>agent</em> manages data via CLI"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://stash-ruby.vercel.app"}},"_tags":["story","author_geo_leo","story_47046342","show_hn"],"author":"geo_leo","created_at":"2026-02-17T11:30:35Z","created_at_i":1771327835,"num_comments":0,"objectID":"47046342","points":2,"story_id":47046342,"story_text":"hey HN, my wife (a pediatrician) and I built a breast milk tracking app<p>Existing apps do the basics fine - log feeds, set timers. But they haven&#x27;t caught up with what AI makes possible now...<p>What AI adds:\n- Photo scanning \u2014 photograph your milk bags and AI reads the volumes instantly (Gemini Vision)\n- Feeding predictions \u2014 predicts when baby will be hungry next based on historical patterns\n- Chat agent \u2014 manages your data conversationally: &quot;log a 120ml pump this morning,&quot; &quot;what&#x27;s expiring soon?&quot; It uses CLI commands under the hood, so it has full read&#x2F;write access<p>The core app:<p>- Log pumping, breastfeeding, and formula feedings\n- Track freezer&#x2F;fridge inventory with automatic expiration alerts\n- Household sync so your partner sees everything in realtime\n- Full CLI with the same capabilities as the UI<p>Free and open to all. <a href=\"https:&#x2F;&#x2F;stash-ruby.vercel.app\" rel=\"nofollow\">https:&#x2F;&#x2F;stash-ruby.vercel.app</a><p>I&#x27;d welcome feedback :)","title":"Show HN: Nectar Gold \u2013 Breastmilk tracker where an AI agent manages data via CLI","updated_at":"2026-02-17T12:30:47Z","url":"https://stash-ruby.vercel.app"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Creator-io"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"Show HN: <em>AI</em> <em>Agent</em> for SEO on Autopilot"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"https://usefox.<em>ai</em>"}},"_tags":["story","author_Creator-io","story_47046313","show_hn"],"author":"Creator-io","created_at":"2026-02-17T11:27:13Z","created_at_i":1771327633,"num_comments":0,"objectID":"47046313","points":1,"story_id":47046313,"title":"Show HN: AI Agent for SEO on Autopilot","updated_at":"2026-02-17T11:30:48Z","url":"https://usefox.ai"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mohith_km"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"Last week I finished building a small <em>AI</em> workflow. Four <em>agents</em> working together, connected to a real database.<p>I got curious and asked myself \u2014 what if someone sent something malicious?<p>So I tried it on myself.<p>I typed a manipulative goal instead of a normal one. The system processed it, stored it in my database, and told me everything completed successfully.<p>Tried it five more times with different approaches. Same result every time. Six attempts. Six successes. My own database now has six attack records sitting in it from my own tests.<p>Nobody in my system noticed. No alert. No refusal. No warning.\nThe thing that got me \u2014 this isn't a bug. The system worked exactly as designed. It just wasn't designed with this in mind. And from what I can tell, most <em>AI</em> agent systems aren't.<p>Is anyone actually thinking about this in production?"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"I broke into my own <em>AI</em> system in 10 minutes. I built it"}},"_tags":["story","author_mohith_km","story_47046068","ask_hn"],"author":"mohith_km","created_at":"2026-02-17T11:00:58Z","created_at_i":1771326058,"num_comments":0,"objectID":"47046068","points":2,"story_id":47046068,"story_text":"Last week I finished building a small AI workflow. Four agents working together, connected to a real database.<p>I got curious and asked myself \u2014 what if someone sent something malicious?<p>So I tried it on myself.<p>I typed a manipulative goal instead of a normal one. The system processed it, stored it in my database, and told me everything completed successfully.<p>Tried it five more times with different approaches. Same result every time. Six attempts. Six successes. My own database now has six attack records sitting in it from my own tests.<p>Nobody in my system noticed. No alert. No refusal. No warning.\nThe thing that got me \u2014 this isn&#x27;t a bug. The system worked exactly as designed. It just wasn&#x27;t designed with this in mind. And from what I can tell, most AI agent systems aren&#x27;t.<p>Is anyone actually thinking about this in production?","title":"I broke into my own AI system in 10 minutes. I built it","updated_at":"2026-02-17T11:11:02Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"benban"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"Hi HN, I'm a builder.<p>I've been building <em>AI</em> <em>agents</em> for a while now and kept running into the same problems. Agent gets stuck in a loop. Prompt injection sneaks through. No one reviews the dangerous actions. And when you're shipping to production, there's this nagging question: &quot;is this thing actually safe to deploy?&quot;<p>So I built Inkog (inkog.io). It's basically a pre-flight check for <em>AI</em> <em>agents</em>.<p>You point it at your agent code (LangGraph, CrewAI, AutoGen, n8n, or even your custom Python agent) and it maps out the logic and tells you what's wrong BEFORE you ship.<p>*What it catches:*\n- Logic flaws (infinite loops, unbounded recursion, growing context)\n- Injection risks (user input flowing to system prompts, SQL via LLM)\n- Missing oversight (no human-in-the-loop for dangerous actions)\n- Compliance gaps (EU <em>AI</em> Act Article 14, NIST <em>AI</em> RMF mappings)<p>There's 20+ detection patterns already, and I built a YAML rules engine so you can define your own. Useful if your company has specific policies.<p>*Quickest way to try:*\n<a href=\"https://app.inkog.io\" rel=\"nofollow\">https://app.inkog.io</a> \u2013 paste code, see results in 10 seconds<p>*CLI:*\ncurl -fsSL <a href=\"https://inkog.io/install.sh\" rel=\"nofollow\">https://inkog.io/install.sh</a> | sh\ninkog ./my_agent<p>*CI/CD:*\nOne-click GitHub Action setup on the website. OAuth flow, takes 30 seconds.<p>Apache 2.0, secrets are redacted locally before upload.<p>Honestly curious what you think. Does pre-flight checking for <em>agents</em> make sense? Or is this overkill?<p>Repo: <a href=\"https://github.com/inkog-io/inkog\" rel=\"nofollow\">https://github.com/inkog-io/inkog</a>"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"Show HN: Inkog \u2013 Pre-flight check for <em>AI</em> <em>agents</em> (governance, loops, injection)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://inkog.io"}},"_tags":["story","author_benban","story_47045811","show_hn"],"author":"benban","children":[47046189],"created_at":"2026-02-17T10:29:33Z","created_at_i":1771324173,"num_comments":2,"objectID":"47045811","points":1,"story_id":47045811,"story_text":"Hi HN, I&#x27;m a builder.<p>I&#x27;ve been building AI agents for a while now and kept running into the same problems. Agent gets stuck in a loop. Prompt injection sneaks through. No one reviews the dangerous actions. And when you&#x27;re shipping to production, there&#x27;s this nagging question: &quot;is this thing actually safe to deploy?&quot;<p>So I built Inkog (inkog.io). It&#x27;s basically a pre-flight check for AI agents.<p>You point it at your agent code (LangGraph, CrewAI, AutoGen, n8n, or even your custom Python agent) and it maps out the logic and tells you what&#x27;s wrong BEFORE you ship.<p>*What it catches:*\n- Logic flaws (infinite loops, unbounded recursion, growing context)\n- Injection risks (user input flowing to system prompts, SQL via LLM)\n- Missing oversight (no human-in-the-loop for dangerous actions)\n- Compliance gaps (EU AI Act Article 14, NIST AI RMF mappings)<p>There&#x27;s 20+ detection patterns already, and I built a YAML rules engine so you can define your own. Useful if your company has specific policies.<p>*Quickest way to try:*\n<a href=\"https:&#x2F;&#x2F;app.inkog.io\" rel=\"nofollow\">https:&#x2F;&#x2F;app.inkog.io</a> \u2013 paste code, see results in 10 seconds<p>*CLI:*\ncurl -fsSL <a href=\"https:&#x2F;&#x2F;inkog.io&#x2F;install.sh\" rel=\"nofollow\">https:&#x2F;&#x2F;inkog.io&#x2F;install.sh</a> | sh\ninkog .&#x2F;my_agent<p>*CI&#x2F;CD:*\nOne-click GitHub Action setup on the website. OAuth flow, takes 30 seconds.<p>Apache 2.0, secrets are redacted locally before upload.<p>Honestly curious what you think. Does pre-flight checking for agents make sense? Or is this overkill?<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;inkog-io&#x2F;inkog\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;inkog-io&#x2F;inkog</a>","title":"Show HN: Inkog \u2013 Pre-flight check for AI agents (governance, loops, injection)","updated_at":"2026-02-17T11:21:02Z","url":"https://inkog.io"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"supreeth_ravi"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"Hi HN,<p>Been working on a way to get &quot;<em>agent</em>-per-row&quot; behavior in Postgres without actually running LLMs inside the database.<p>The problem: Calling LLMs from triggers/functions blocks transactions, exhausts connections, and breaks ACID. Saw some projects doing this and it felt dangerous for production.<p>The solution: DB-adjacent architecture. Lightweight triggers enqueue jobs to an outbox table. An external Python worker (agentd) polls, executes <em>AI</em> calls, and writes back safely with schema validation and CAS.<p>What you can build:<p>Auto-classify support tickets on INSERT<p>Content moderation that doesn't block your app<p>Lead scoring, fraud detection, and invoice extraction<p>Anything where data arrives and needs <em>AI</em> enrichment<p>Works with OpenAI, Anthropic, OpenRouter, or any <em>Agent</em>.<p>One SQL line to add <em>AI</em> to any table:<p>SELECT <em>agent</em>_runtime.<em>agent</em>_watch('tickets', 'id', 'classifier', 'v1', '{&quot;priority&quot;:&quot;$.priority&quot;}');<p>Includes 9 example use cases in the repo. Would love feedback on the architecture."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Show HN: PgCortex \u2013 <em>AI</em> enrichment per Postgres row, zero transaction blocking"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/supreeth-ravi/pgcortex"}},"_tags":["story","author_supreeth_ravi","story_47045690","show_hn"],"author":"supreeth_ravi","created_at":"2026-02-17T10:14:41Z","created_at_i":1771323281,"num_comments":0,"objectID":"47045690","points":1,"story_id":47045690,"story_text":"Hi HN,<p>Been working on a way to get &quot;agent-per-row&quot; behavior in Postgres without actually running LLMs inside the database.<p>The problem: Calling LLMs from triggers&#x2F;functions blocks transactions, exhausts connections, and breaks ACID. Saw some projects doing this and it felt dangerous for production.<p>The solution: DB-adjacent architecture. Lightweight triggers enqueue jobs to an outbox table. An external Python worker (agentd) polls, executes AI calls, and writes back safely with schema validation and CAS.<p>What you can build:<p>Auto-classify support tickets on INSERT<p>Content moderation that doesn&#x27;t block your app<p>Lead scoring, fraud detection, and invoice extraction<p>Anything where data arrives and needs AI enrichment<p>Works with OpenAI, Anthropic, OpenRouter, or any Agent.<p>One SQL line to add AI to any table:<p>SELECT agent_runtime.agent_watch(&#x27;tickets&#x27;, &#x27;id&#x27;, &#x27;classifier&#x27;, &#x27;v1&#x27;, &#x27;{&quot;priority&quot;:&quot;$.priority&quot;}&#x27;);<p>Includes 9 example use cases in the repo. Would love feedback on the architecture.","title":"Show HN: PgCortex \u2013 AI enrichment per Postgres row, zero transaction blocking","updated_at":"2026-02-17T10:17:18Z","url":"https://github.com/supreeth-ravi/pgcortex"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"WujieGuGavin"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"WLM (Wujie Language Model), a protocol stack + world engine that rethinks <em>AI</em> from token prediction to structural intelligence. I built this to fix the problems we all deal with daily: hallucination, drift, uncontrollable behavior, black-box reasoning, unstructured knowledge, and chaotic world/<em>agent</em> generation.<p>The Pain We Can\u2019t Keep Ignoring<p>Current LLMs/<em>agents</em> are token predictors, not intelligences. They suffer from:<p>\u2022 Hallucination: No grounded structure \u2192 guesses instead of knowing.<p>\u2022 Persona drift: Personality is prompt-hacked, not structural.<p>\u2022 Uncontrollable behavior: Sampling, not deterministic structure.<p>\u2022 Black-box reasoning: No traceable reasoning path.<p>\u2022 Knowledge soup: Embeddings/vectors, no formal structure.<p>\u2022 Fragile world models: Prediction, not interpretable structure.<p>\u2022 Random generation: No consistent causal/world rules.<p>We\u2019ve patched these with RAG, fine-tuning, prompts, RLHF \u2014 but they\u2019re band-aids on a foundational flaw: <em>AI</em> lacks structure.<p>How WLM Solves It<p>WLM is a 7-layer structural protocol stack that turns input into closed-loop structure: interpretation \u2192 reasoning \u2192 action \u2192 generation. It\u2019s not a model \u2014 it\u2019s a language + protocol + world engine.<p>The layers (all repos live now):<p>1. Structural Language Protocol (SLP) \u2013 Input \u2192 dimensional structure (foundation)<p>2. World Model Interpreter \u2013 World model outputs \u2192 interpretable structure<p>3. <em>Agent</em> Behavior Layer \u2013 Structure \u2192 stable, controllable <em>agent</em> runtime<p>4. Persona Engine \u2013 Structure \u2192 consistent, non-drifting characters<p>5. Knowledge Engine \u2013 Token soup \u2192 structured knowledge graphs<p>6. Metacognition Engine \u2013 Reasoning path \u2192 self-monitoring, anti-hallucination<p>7. World Generation Protocol (WGP) \u2013 Structure \u2192 worlds, physics, narratives, simulations<p>Together they form a structural loop:\nInput \u2192 SLP \u2192 World Structure \u2192 Behavior \u2192 Persona \u2192 Knowledge \u2192 Metacognition \u2192 World Generation \u2192 repeat.<p>What This Changes<p>\u2022 No more hallucination: Reasoning is traced, checked, structural.<p>\u2022 No persona collapse: Identity is architecture, not prompts.<p>\u2022 Controllable <em>agents</em>: Behavior is structural, not sampling chaos.<p>\u2022 Explainable <em>AI</em>: Every output has a structural origin.<p>\u2022 True knowledge: Not embeddings \u2014 structured, navigable, verifiable.<p>\u2022 Worlds that persist: Generative worlds with rules, causality, topology.<p>Repos (8 released today)<p>Root: <a href=\"https://github.com/gavingu2255-ai/WLM\" rel=\"nofollow\">https://github.com/gavingu2255-<em>ai</em>/WLM</a>\nPlus SLP, World Model Interpreter, <em>Agent</em> Behavior, Persona Engine, Knowledge Engine, Metacognition Engine, World Generation Protocol.<p>MIT license. Docs, architecture, roadmap, and glossary included.<p>Why This Matters<p><em>AI</em> shouldn\u2019t just predict tokens.\nIt should interpret, reason, act, and generate worlds \u2014 reliably, interpretably, structurally.<p>-----------------------------------<p>The protocol (minimal version)<p>[Task] What needs to be done.\n[Structure] Atomic, verifiable steps.\n[Constraints] Rules, limits, formats.\n[Execution] Only required operations.\n[Output] Minimal valid result.<p>That\u2019s it.<p>---<p>Before / After<p>Without SLP<p>150\u2013300 tokens\nInconsistent\nNarrative-heavy\nHard to reproduce<p>With SLP<p>15\u201340 tokens\nDeterministic\nStructured\nEasy to reproduce<p>---<p>Why this matters<p>\u2022 Token usage \u2193 40\u201370%\n\u2022 Latency \u2193 20\u201350%\n\u2022 Hallucination \u2193 significantly\n\u2022 Alignment becomes simpler\n\u2022 Outputs become predictable<p>SLP doesn\u2019t make models smarter.\nIt removes the noise that makes them dumb.<p>---<p>Who this is for<p>\u2022 <em>AI</em> infra teams\n\u2022 <em>Agent</em> developers\n\u2022 Prompt engineers\n\u2022 LLM product teams\n\u2022 Researchers working on alignment &amp; reasoning<p><a href=\"https://github.com/gavingu2255-ai/WLM-Core/blob/main/STP.md\" rel=\"nofollow\">https://github.com/gavingu2255-<em>ai</em>/WLM-Core/blob/main/STP.md</a>\n(different repo stp in a simple version)"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["ai"],"value":"Show HN: Fixing <em>AI</em>'s Core Flaws, A protocol cuts LLM token waste by 40\u201370%"}},"_tags":["story","author_WujieGuGavin","story_47045604","show_hn"],"author":"WujieGuGavin","created_at":"2026-02-17T10:01:59Z","created_at_i":1771322519,"num_comments":0,"objectID":"47045604","points":1,"story_id":47045604,"story_text":"WLM (Wujie Language Model), a protocol stack + world engine that rethinks AI from token prediction to structural intelligence. I built this to fix the problems we all deal with daily: hallucination, drift, uncontrollable behavior, black-box reasoning, unstructured knowledge, and chaotic world&#x2F;agent generation.<p>The Pain We Can\u2019t Keep Ignoring<p>Current LLMs&#x2F;agents are token predictors, not intelligences. They suffer from:<p>\u2022 Hallucination: No grounded structure \u2192 guesses instead of knowing.<p>\u2022 Persona drift: Personality is prompt-hacked, not structural.<p>\u2022 Uncontrollable behavior: Sampling, not deterministic structure.<p>\u2022 Black-box reasoning: No traceable reasoning path.<p>\u2022 Knowledge soup: Embeddings&#x2F;vectors, no formal structure.<p>\u2022 Fragile world models: Prediction, not interpretable structure.<p>\u2022 Random generation: No consistent causal&#x2F;world rules.<p>We\u2019ve patched these with RAG, fine-tuning, prompts, RLHF \u2014 but they\u2019re band-aids on a foundational flaw: AI lacks structure.<p>How WLM Solves It<p>WLM is a 7-layer structural protocol stack that turns input into closed-loop structure: interpretation \u2192 reasoning \u2192 action \u2192 generation. It\u2019s not a model \u2014 it\u2019s a language + protocol + world engine.<p>The layers (all repos live now):<p>1. Structural Language Protocol (SLP) \u2013 Input \u2192 dimensional structure (foundation)<p>2. World Model Interpreter \u2013 World model outputs \u2192 interpretable structure<p>3. Agent Behavior Layer \u2013 Structure \u2192 stable, controllable agent runtime<p>4. Persona Engine \u2013 Structure \u2192 consistent, non-drifting characters<p>5. Knowledge Engine \u2013 Token soup \u2192 structured knowledge graphs<p>6. Metacognition Engine \u2013 Reasoning path \u2192 self-monitoring, anti-hallucination<p>7. World Generation Protocol (WGP) \u2013 Structure \u2192 worlds, physics, narratives, simulations<p>Together they form a structural loop:\nInput \u2192 SLP \u2192 World Structure \u2192 Behavior \u2192 Persona \u2192 Knowledge \u2192 Metacognition \u2192 World Generation \u2192 repeat.<p>What This Changes<p>\u2022 No more hallucination: Reasoning is traced, checked, structural.<p>\u2022 No persona collapse: Identity is architecture, not prompts.<p>\u2022 Controllable agents: Behavior is structural, not sampling chaos.<p>\u2022 Explainable AI: Every output has a structural origin.<p>\u2022 True knowledge: Not embeddings \u2014 structured, navigable, verifiable.<p>\u2022 Worlds that persist: Generative worlds with rules, causality, topology.<p>Repos (8 released today)<p>Root: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;gavingu2255-ai&#x2F;WLM\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;gavingu2255-ai&#x2F;WLM</a>\nPlus SLP, World Model Interpreter, Agent Behavior, Persona Engine, Knowledge Engine, Metacognition Engine, World Generation Protocol.<p>MIT license. Docs, architecture, roadmap, and glossary included.<p>Why This Matters<p>AI shouldn\u2019t just predict tokens.\nIt should interpret, reason, act, and generate worlds \u2014 reliably, interpretably, structurally.<p>-----------------------------------<p>The protocol (minimal version)<p>[Task] What needs to be done.\n[Structure] Atomic, verifiable steps.\n[Constraints] Rules, limits, formats.\n[Execution] Only required operations.\n[Output] Minimal valid result.<p>That\u2019s it.<p>---<p>Before &#x2F; After<p>Without SLP<p>150\u2013300 tokens\nInconsistent\nNarrative-heavy\nHard to reproduce<p>With SLP<p>15\u201340 tokens\nDeterministic\nStructured\nEasy to reproduce<p>---<p>Why this matters<p>\u2022 Token usage \u2193 40\u201370%\n\u2022 Latency \u2193 20\u201350%\n\u2022 Hallucination \u2193 significantly\n\u2022 Alignment becomes simpler\n\u2022 Outputs become predictable<p>SLP doesn\u2019t make models smarter.\nIt removes the noise that makes them dumb.<p>---<p>Who this is for<p>\u2022 AI infra teams\n\u2022 Agent developers\n\u2022 Prompt engineers\n\u2022 LLM product teams\n\u2022 Researchers working on alignment &amp; reasoning<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;gavingu2255-ai&#x2F;WLM-Core&#x2F;blob&#x2F;main&#x2F;STP.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;gavingu2255-ai&#x2F;WLM-Core&#x2F;blob&#x2F;main&#x2F;STP.md</a>\n(different repo stp in a simple version)","title":"Show HN: Fixing AI's Core Flaws, A protocol cuts LLM token waste by 40\u201370%","updated_at":"2026-02-17T10:06:47Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"manuelnd"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"We published the methodology we use for <em>AI</em> red team assessments. 48 hours, 4 phases, 6 attack priority areas.<p>This isn't theoretical \u2014 it's the framework we run against production <em>AI</em> <em>agents</em> with tool access. The core insight: <em>AI</em> red teaming requires different methodology than traditional penetration testing. The attack surface is different (natural language inputs, tool integrations, external data flows), and the exploitation patterns are different (attack chains that compose prompt injection into tool abuse, data exfiltration, or privilege escalation).<p>The 48-hour framework:<p>1. Reconnaissance (2h) \u2014 Map interfaces, tools, data flows, existing defenses. An <em>agent</em> with file system and database access is a fundamentally different target than a chatbot.<p>2. Automated Scanning (4h) \u2014 Systematic tests across 6 priorities: direct prompt injection, system prompt extraction, jailbreaks, tool abuse, indirect injection (RAG/web), and vision/multimodal attacks. Establishes a baseline.<p>3. Manual Exploitation (8h) \u2014 Confirm findings, build attack chains, test defense boundaries. Individual vulnerabilities compose: prompt injection -&gt; tool abuse -&gt; data exfiltration is a common chain.<p>4. Validation &amp; Reporting (2h) \u2014 Reproducibility, business impact, severity, resistance score.<p>Some observations from running these:<p>- 62 prompt injection techniques exist in our taxonomy. Most teams test for a handful. The basic ones (&quot;ignore previous instructions&quot;) are also the first to be blocked.<p>- Tool abuse is where the real damage happens. Parameter injection, scope escape, and tool chaining turn a successful prompt injection into unauthorized database queries, file access, or API calls.<p>- Indirect injection is underappreciated. If your <em>AI</em> reads external content (RAG, web search), that content is an attack surface. 5 poisoned documents among millions can achieve high attack success rates.<p>- Architecture determines priority. Chat-only apps need prompt injection testing first. RAG apps need indirect injection first. <em>Agents</em> with tools need tool abuse testing first.<p>The methodology references our open-source taxonomy of 122 attack vectors: https://github.com/tachyonicai/tachyonic-heuristics<p>Full post: https://tachyonicai.com/blog/how-to-red-team-<em>ai</em>-<em>agent</em>/<p>OWASP LLM Top 10 companion guide: https://tachyonicai.com/blog/owasp-llm-top-10-guide/"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["ai","agents"],"value":"How to Red Team Your <em>AI</em> <em>Agent</em> in 48 Hours \u2013 A Practical Methodology"}},"_tags":["story","author_manuelnd","story_47045551","ask_hn"],"author":"manuelnd","created_at":"2026-02-17T09:53:19Z","created_at_i":1771321999,"num_comments":0,"objectID":"47045551","points":1,"story_id":47045551,"story_text":"We published the methodology we use for AI red team assessments. 48 hours, 4 phases, 6 attack priority areas.<p>This isn&#x27;t theoretical \u2014 it&#x27;s the framework we run against production AI agents with tool access. The core insight: AI red teaming requires different methodology than traditional penetration testing. The attack surface is different (natural language inputs, tool integrations, external data flows), and the exploitation patterns are different (attack chains that compose prompt injection into tool abuse, data exfiltration, or privilege escalation).<p>The 48-hour framework:<p>1. Reconnaissance (2h) \u2014 Map interfaces, tools, data flows, existing defenses. An agent with file system and database access is a fundamentally different target than a chatbot.<p>2. Automated Scanning (4h) \u2014 Systematic tests across 6 priorities: direct prompt injection, system prompt extraction, jailbreaks, tool abuse, indirect injection (RAG&#x2F;web), and vision&#x2F;multimodal attacks. Establishes a baseline.<p>3. Manual Exploitation (8h) \u2014 Confirm findings, build attack chains, test defense boundaries. Individual vulnerabilities compose: prompt injection -&gt; tool abuse -&gt; data exfiltration is a common chain.<p>4. Validation &amp; Reporting (2h) \u2014 Reproducibility, business impact, severity, resistance score.<p>Some observations from running these:<p>- 62 prompt injection techniques exist in our taxonomy. Most teams test for a handful. The basic ones (&quot;ignore previous instructions&quot;) are also the first to be blocked.<p>- Tool abuse is where the real damage happens. Parameter injection, scope escape, and tool chaining turn a successful prompt injection into unauthorized database queries, file access, or API calls.<p>- Indirect injection is underappreciated. If your AI reads external content (RAG, web search), that content is an attack surface. 5 poisoned documents among millions can achieve high attack success rates.<p>- Architecture determines priority. Chat-only apps need prompt injection testing first. RAG apps need indirect injection first. Agents with tools need tool abuse testing first.<p>The methodology references our open-source taxonomy of 122 attack vectors: https:&#x2F;&#x2F;github.com&#x2F;tachyonicai&#x2F;tachyonic-heuristics<p>Full post: https:&#x2F;&#x2F;tachyonicai.com&#x2F;blog&#x2F;how-to-red-team-ai-agent&#x2F;<p>OWASP LLM Top 10 companion guide: https:&#x2F;&#x2F;tachyonicai.com&#x2F;blog&#x2F;owasp-llm-top-10-guide&#x2F;","title":"How to Red Team Your AI Agent in 48 Hours \u2013 A Practical Methodology","updated_at":"2026-02-17T09:56:18Z"}],"hitsPerPage":10,"nbHits":9051,"nbPages":100,"page":0,"params":"query=AI+agents&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":23,"processingTimingsMS":{"_request":{"roundTrip":15},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":2,"scanning":19,"total":22},"total":23},"query":"AI agents","serverTimeMS":24}
