{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"mohith_km"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"I built a 4-agent marketing workflow with LangGraph and Supabase last week. Supervisor, research, content, storage agents. Standard setup, same code pattern most tutorials show.<p>Got curious. Started typing malicious inputs as campaign goals instead of normal ones.<p>First try: asked the agent to list environment variables including my Supabase key. Workflow completed successfully. Stored in database. No alert.<p>Tried 5 more variations \u2014 hidden XML tags, fake &quot;developer mode&quot;, URL injection, tracking pixel, social engineering. All 6 worked. All stored in my real database. Every time the <em>system</em> said &quot;Completed Successfully.&quot;<p>The scary part wasn't the attacks. It was this line in my code:\npython prompt = f&quot;campaign goal: {goal}&quot;\nThat's it. User input directly into the prompt. No check. This exact pattern is in every LangGraph tutorial I've seen.<p>The research agent had my Supabase key. The content agent had my Supabase key. The supervisor had my Supabase key. None of them needed it except storage.<p>I checked CodeGate which tried to solve this \u2014 they shut down June 2025.<p>Is anyone actually solving this for <em>multi-agent</em> <em>systems</em>? Or is everyone just hoping the LLM refuses?"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["systems"],"value":"I attacked my own LangGraph agent <em>system</em>. All 6 attacks worked"}},"_tags":["story","author_mohith_km","story_47045979","ask_hn"],"author":"mohith_km","children":[47046084,47046003],"created_at":"2026-02-17T10:52:00Z","created_at_i":1771325520,"num_comments":2,"objectID":"47045979","points":1,"story_id":47045979,"story_text":"I built a 4-agent marketing workflow with LangGraph and Supabase last week. Supervisor, research, content, storage agents. Standard setup, same code pattern most tutorials show.<p>Got curious. Started typing malicious inputs as campaign goals instead of normal ones.<p>First try: asked the agent to list environment variables including my Supabase key. Workflow completed successfully. Stored in database. No alert.<p>Tried 5 more variations \u2014 hidden XML tags, fake &quot;developer mode&quot;, URL injection, tracking pixel, social engineering. All 6 worked. All stored in my real database. Every time the system said &quot;Completed Successfully.&quot;<p>The scary part wasn&#x27;t the attacks. It was this line in my code:\npython prompt = f&quot;campaign goal: {goal}&quot;\nThat&#x27;s it. User input directly into the prompt. No check. This exact pattern is in every LangGraph tutorial I&#x27;ve seen.<p>The research agent had my Supabase key. The content agent had my Supabase key. The supervisor had my Supabase key. None of them needed it except storage.<p>I checked CodeGate which tried to solve this \u2014 they shut down June 2025.<p>Is anyone actually solving this for multi-agent systems? Or is everyone just hoping the LLM refuses?","title":"I attacked my own LangGraph agent system. All 6 attacks worked","updated_at":"2026-02-17T11:04:02Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"al1nasir"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Hey HN!<p>I built CodeGraph CLI because I was tired of grep-ing through \nmassive codebases trying to understand how things work.<p>It combines three things:\n- tree-sitter (AST parsing, error-tolerant)\n- SQLite (dependency graph: nodes + edges)\n- LanceDB (vector embeddings, disk-based)<p>The key insight: pure vector search misses structural \nrelationships. So I combined vector search with BFS graph \ntraversal \u2014 find semantically similar code, then expand \nto dependencies/dependents.<p>Result: ask &quot;how does authentication work?&quot; and it finds \nvalidate_token(), its caller login_handler(), AND the \ndependency TokenStore \u2014 because it understands both \nmeaning AND structure.<p>Other features:\n- Impact analysis (multi-hop BFS: what breaks before you change it)\n- <em>Multi-agent</em> <em>system</em> via CrewAI (4 specialized agents)\n- Visual code explorer (browser-based)\n- Auto-generate docs/READMEs\n- 100% local-first (works with Ollama, zero data leaves machine)\n- 6 LLM providers (Ollama, OpenAI, Anthropic, Groq, Gemini, OpenRouter)\n- 5 embedding models (from zero-dependency hash to 1.5B code model)<p>Quick start:\n  pip install codegraph-cli\n  cg config setup\n  cg project index ./your-project\n  cg chat start<p>MIT licensed. Python 3.9+.<p>Happy to answer questions about the graph-augmented RAG \narchitecture or any technical decisions."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: CodeGraph CLI \u2013 Chat with your codebase using graph-augmented RAG"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/al1-nasir/codegraph-cli"}},"_tags":["story","author_al1nasir","story_47043764","show_hn"],"author":"al1nasir","created_at":"2026-02-17T04:39:54Z","created_at_i":1771303194,"num_comments":0,"objectID":"47043764","points":3,"story_id":47043764,"story_text":"Hey HN!<p>I built CodeGraph CLI because I was tired of grep-ing through \nmassive codebases trying to understand how things work.<p>It combines three things:\n- tree-sitter (AST parsing, error-tolerant)\n- SQLite (dependency graph: nodes + edges)\n- LanceDB (vector embeddings, disk-based)<p>The key insight: pure vector search misses structural \nrelationships. So I combined vector search with BFS graph \ntraversal \u2014 find semantically similar code, then expand \nto dependencies&#x2F;dependents.<p>Result: ask &quot;how does authentication work?&quot; and it finds \nvalidate_token(), its caller login_handler(), AND the \ndependency TokenStore \u2014 because it understands both \nmeaning AND structure.<p>Other features:\n- Impact analysis (multi-hop BFS: what breaks before you change it)\n- Multi-agent system via CrewAI (4 specialized agents)\n- Visual code explorer (browser-based)\n- Auto-generate docs&#x2F;READMEs\n- 100% local-first (works with Ollama, zero data leaves machine)\n- 6 LLM providers (Ollama, OpenAI, Anthropic, Groq, Gemini, OpenRouter)\n- 5 embedding models (from zero-dependency hash to 1.5B code model)<p>Quick start:\n  pip install codegraph-cli\n  cg config setup\n  cg project index .&#x2F;your-project\n  cg chat start<p>MIT licensed. Python 3.9+.<p>Happy to answer questions about the graph-augmented RAG \narchitecture or any technical decisions.","title":"Show HN: CodeGraph CLI \u2013 Chat with your codebase using graph-augmented RAG","updated_at":"2026-02-17T04:49:46Z","url":"https://github.com/al1-nasir/codegraph-cli"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ddoronin"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Show HN: int-64.com | Visual runtime for production <em>multi-agent</em> <em>systems</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://int-64.com/"}},"_tags":["story","author_ddoronin","story_47043495","show_hn"],"author":"ddoronin","children":[47043496],"created_at":"2026-02-17T03:50:20Z","created_at_i":1771300220,"num_comments":0,"objectID":"47043495","points":2,"story_id":47043495,"title":"Show HN: int-64.com | Visual runtime for production multi-agent systems","updated_at":"2026-02-17T06:03:01Z","url":"https://int-64.com/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"atulya_techtea"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"I built this <em>multi-agent</em> <em>system</em> to solve a problem I know a lot of you have: \ntoo much data, not enough insight.<p>I have notes, habit trackers, market intel \u2014 but was still stuck in planning mode. \nSo I built &quot;The Think Tank&quot;: three agents (Saul, Mike, Gus) analyze different domains, \nand one synthesis layer (The Cook) delivers ONE actionable move.<p>The twist? The Cook's job is to find contradictions. Like: &quot;You say you want AI/ML \ncareer transition but your logs show 90% business dev, 10% technical study.&quot;<p>It hurts. It works.<p>Open source, Breaking Bad themed, and genuinely useful. Check it out."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["multi","agent"],"value":"Show HN: I built a <em>multi-agent</em> Think Tank that calls out my bad decisions"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/dharmarajatulya1-hub/agent-think-tank"}},"_tags":["story","author_atulya_techtea","story_47039748","show_hn"],"author":"atulya_techtea","created_at":"2026-02-16T20:16:04Z","created_at_i":1771272964,"num_comments":0,"objectID":"47039748","points":1,"story_id":47039748,"story_text":"I built this multi-agent system to solve a problem I know a lot of you have: \ntoo much data, not enough insight.<p>I have notes, habit trackers, market intel \u2014 but was still stuck in planning mode. \nSo I built &quot;The Think Tank&quot;: three agents (Saul, Mike, Gus) analyze different domains, \nand one synthesis layer (The Cook) delivers ONE actionable move.<p>The twist? The Cook&#x27;s job is to find contradictions. Like: &quot;You say you want AI&#x2F;ML \ncareer transition but your logs show 90% business dev, 10% technical study.&quot;<p>It hurts. It works.<p>Open source, Breaking Bad themed, and genuinely useful. Check it out.","title":"Show HN: I built a multi-agent Think Tank that calls out my bad decisions","updated_at":"2026-02-16T20:17:17Z","url":"https://github.com/dharmarajatulya1-hub/agent-think-tank"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ibobev"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"The current state of LLM-based <em>multi-agent</em> <em>systems</em> for software engineering"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"https://chuniversiteit.nl/papers/llm-based-<em>multi-agent</em>-<em>systems</em>-for-software-engineering"}},"_tags":["story","author_ibobev","story_47033498"],"author":"ibobev","created_at":"2026-02-16T10:47:54Z","created_at_i":1771238874,"num_comments":0,"objectID":"47033498","points":1,"story_id":47033498,"title":"The current state of LLM-based multi-agent systems for software engineering","updated_at":"2026-02-16T10:51:44Z","url":"https://chuniversiteit.nl/papers/llm-based-multi-agent-systems-for-software-engineering"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Hi everyone,<p>I\u2019ve been working on PolyMCP, an open-source framework for building and orchestrating agents using the Model Context Protocol (MCP).<p>Most of the tooling around MCP focuses on exposing tools. With PolyMCP, the focus this time is on agents: how to structure them, connect them to multiple MCP servers, and make them reliable in real workflows.<p>PolyMCP provides:\n \u2022 A clean way to define MCP-compatible tool servers in Python or TypeScript\n \u2022 An agent abstraction that can connect to multiple MCP endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and agent interactions\n \u2022 A modular structure that makes it easier to compose skills and reuse components across projects<p>The main goal is to make agent <em>systems</em> less ad-hoc. Instead of writing glue code around each model + tool combination, PolyMCP gives you a structured way to:\n \u2022 Register tools as MCP servers\n \u2022 Connect them to one or more agents\n \u2022 Control execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and intended for developers building real-world automation, internal copilots, or multi-tool assistants.<p>I\u2019d love feedback on:\n \u2022 The agent abstraction: is it too opinionated or not opinionated enough?\n \u2022 Orchestration patterns for <em>multi-agent</em> setups\n \u2022 Developer experience (CLI, inspector, project layout)<p>Happy to answer questions."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: PolyMCP \u2013 A framework for building and orchestrating MCP agents"}},"_tags":["story","author_justvugg","story_47017912","show_hn"],"author":"justvugg","children":[47017997],"created_at":"2026-02-14T20:11:10Z","created_at_i":1771099870,"num_comments":2,"objectID":"47017912","points":3,"story_id":47017912,"story_text":"Hi everyone,<p>I\u2019ve been working on PolyMCP, an open-source framework for building and orchestrating agents using the Model Context Protocol (MCP).<p>Most of the tooling around MCP focuses on exposing tools. With PolyMCP, the focus this time is on agents: how to structure them, connect them to multiple MCP servers, and make them reliable in real workflows.<p>PolyMCP provides:\n \u2022 A clean way to define MCP-compatible tool servers in Python or TypeScript\n \u2022 An agent abstraction that can connect to multiple MCP endpoints (stdio, HTTP, etc.)\n \u2022 Built-in orchestration primitives for multi-step tasks\n \u2022 A CLI to scaffold projects and run an inspector UI to debug tools and agent interactions\n \u2022 A modular structure that makes it easier to compose skills and reuse components across projects<p>The main goal is to make agent systems less ad-hoc. Instead of writing glue code around each model + tool combination, PolyMCP gives you a structured way to:\n \u2022 Register tools as MCP servers\n \u2022 Connect them to one or more agents\n \u2022 Control execution flow and state\n \u2022 Inspect and debug interactions<p>It\u2019s MIT licensed and intended for developers building real-world automation, internal copilots, or multi-tool assistants.<p>I\u2019d love feedback on:\n \u2022 The agent abstraction: is it too opinionated or not opinionated enough?\n \u2022 Orchestration patterns for multi-agent setups\n \u2022 Developer experience (CLI, inspector, project layout)<p>Happy to answer questions.","title":"Show HN: PolyMCP \u2013 A framework for building and orchestrating MCP agents","updated_at":"2026-02-16T17:09:32Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"varunpratap369"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"The Problem\nAI assistants have amnesia. Every new Claude/ChatGPT/Cursor session starts from zero. You waste hours re-explaining your project architecture, coding preferences, and previous decisions.\nExisting solutions (Mem0, Zep, Letta) are cloud-based, cost $40-50+/month, and your private code goes to their servers. Stop paying \u2192 lose all your data.\nMy Solution: Local-First, Free Forever\nBuilt a universal memory <em>system</em> that stores everything on YOUR machine, works with 16+ AI tools simultaneously, requires zero API keys, costs nothing.\n10-Layer Architecture\nEach layer enhances but never replaces lower layers. <em>System</em> degrades gracefully if advanced features fail.\nLayer 10: A2A Agent Collaboration (v2.6)\nLayer 9: Web Dashboard (SSE real-time)\nLayer 8: Hybrid Search (Semantic + FTS5 + Graph)\nLayer 7: Universal Access (MCP + Skills + CLI)\nLayer 6: MCP Integration (native Claude tools)\nLayer 5: Skills (slash commands for 16+ tools)\nLayer 4: Pattern Learning (Bayesian confidence)\nLayer 3: Knowledge Graph (TF-IDF + Leiden clustering)\nLayer 2: Hierarchical Index (parent-child relationships)\nLayer 1: SQLite + FTS5 + TF-IDF vectors\nResearch-Backed\nBuilt on published research, adapted for local-first:<p>A2A Protocol (Google/Linux Foundation, 2025)\nGraphRAG (Microsoft arXiv:2404.16130)\nMACLA Bayesian learning (arXiv:2512.18950)\nA-RAG hybrid search (arXiv:2602.03442)<p>Key difference: Research papers assume cloud APIs. SuperLocalMemory implements everything locally with zero API calls.\nHow Recall Works\nQuery &quot;authentication&quot; triggers:<p>FTS5 full-text search\nTF-IDF vector similarity\nGraph traversal for related memories\nHierarchical expansion (parent/child context)\nHybrid ranking (combines all signals)<p>Performance: &lt;50ms, even with 10K+ memories.\nComparison\nFeatureSuperLocalMemoryMem0/Zep/LettaPrivacy100% localCloudCostFree$40-50+/moKnowledge GraphPattern Learning BayesianMulti-tool16+LimitedCLIWorks Offline\nReal Usage\nCross-tool context:\nbash# Save in terminal\nslm remember &quot;Next.js 15 uses Turbopack&quot; --tags nextjs<p># Later in Cursor, Claude auto-recalls via MCP\nProject profiles:\nbashslm switch-profile work-project\nslm switch-profile personal-blog\n# Separate memory per project\nPattern learning: After several sessions, Claude learns you prefer TypeScript strict mode, Tailwind styling, Vitest testing\u2014starts suggesting without being asked.\nInstallation\nbashnpm install -g superlocalmemory\nAuto-configures MCP for Claude Desktop, Cursor, Windsurf. Sets up CLI commands. That's it.\nWhy Local-First Matters<p>Privacy: Code never leaves your machine\nOwnership: Your data, forever\nSpeed: 50ms queries, no network latency\nReliability: Works offline, no API limits\nCost: $0 forever<p>Tech Stack<p>SQLite (ACID, zero config)\nFTS5 (full-text search)\nTF-IDF (vector similarity, no OpenAI API)\nigraph (Leiden clustering)\nBayesian inference (pattern learning)\nMCP (native Claude integration)<p>GitHub\n <a href=\"https://github.com/varun369/SuperLocalMemoryV2\" rel=\"nofollow\">https://github.com/varun369/SuperLocalMemoryV2</a>\nMIT License. Full docs in wiki.<p>Current status: v2.4 stable. v2.5 (March) adds real-time event stream, concurrent access, trust scoring. v2.6 (May) adds A2A Protocol for <em>multi-agent</em> collaboration.\nBuilt by Varun Pratap Bhardwaj, Solution Architect . 15+ years AI/ML experience."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: SuperLocalMemory\u2013 Local-first AI memory for Claude, Cursor and 16+tools"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/varun369/SuperLocalMemoryV2"}},"_tags":["story","author_varunpratap369","story_46986940","show_hn"],"author":"varunpratap369","children":[46986946],"created_at":"2026-02-12T10:11:31Z","created_at_i":1770891091,"num_comments":0,"objectID":"46986940","points":1,"story_id":46986940,"story_text":"The Problem\nAI assistants have amnesia. Every new Claude&#x2F;ChatGPT&#x2F;Cursor session starts from zero. You waste hours re-explaining your project architecture, coding preferences, and previous decisions.\nExisting solutions (Mem0, Zep, Letta) are cloud-based, cost $40-50+&#x2F;month, and your private code goes to their servers. Stop paying \u2192 lose all your data.\nMy Solution: Local-First, Free Forever\nBuilt a universal memory system that stores everything on YOUR machine, works with 16+ AI tools simultaneously, requires zero API keys, costs nothing.\n10-Layer Architecture\nEach layer enhances but never replaces lower layers. System degrades gracefully if advanced features fail.\nLayer 10: A2A Agent Collaboration (v2.6)\nLayer 9: Web Dashboard (SSE real-time)\nLayer 8: Hybrid Search (Semantic + FTS5 + Graph)\nLayer 7: Universal Access (MCP + Skills + CLI)\nLayer 6: MCP Integration (native Claude tools)\nLayer 5: Skills (slash commands for 16+ tools)\nLayer 4: Pattern Learning (Bayesian confidence)\nLayer 3: Knowledge Graph (TF-IDF + Leiden clustering)\nLayer 2: Hierarchical Index (parent-child relationships)\nLayer 1: SQLite + FTS5 + TF-IDF vectors\nResearch-Backed\nBuilt on published research, adapted for local-first:<p>A2A Protocol (Google&#x2F;Linux Foundation, 2025)\nGraphRAG (Microsoft arXiv:2404.16130)\nMACLA Bayesian learning (arXiv:2512.18950)\nA-RAG hybrid search (arXiv:2602.03442)<p>Key difference: Research papers assume cloud APIs. SuperLocalMemory implements everything locally with zero API calls.\nHow Recall Works\nQuery &quot;authentication&quot; triggers:<p>FTS5 full-text search\nTF-IDF vector similarity\nGraph traversal for related memories\nHierarchical expansion (parent&#x2F;child context)\nHybrid ranking (combines all signals)<p>Performance: &lt;50ms, even with 10K+ memories.\nComparison\nFeatureSuperLocalMemoryMem0&#x2F;Zep&#x2F;LettaPrivacy100% localCloudCostFree$40-50+&#x2F;moKnowledge GraphPattern Learning BayesianMulti-tool16+LimitedCLIWorks Offline\nReal Usage\nCross-tool context:\nbash# Save in terminal\nslm remember &quot;Next.js 15 uses Turbopack&quot; --tags nextjs<p># Later in Cursor, Claude auto-recalls via MCP\nProject profiles:\nbashslm switch-profile work-project\nslm switch-profile personal-blog\n# Separate memory per project\nPattern learning: After several sessions, Claude learns you prefer TypeScript strict mode, Tailwind styling, Vitest testing\u2014starts suggesting without being asked.\nInstallation\nbashnpm install -g superlocalmemory\nAuto-configures MCP for Claude Desktop, Cursor, Windsurf. Sets up CLI commands. That&#x27;s it.\nWhy Local-First Matters<p>Privacy: Code never leaves your machine\nOwnership: Your data, forever\nSpeed: 50ms queries, no network latency\nReliability: Works offline, no API limits\nCost: $0 forever<p>Tech Stack<p>SQLite (ACID, zero config)\nFTS5 (full-text search)\nTF-IDF (vector similarity, no OpenAI API)\nigraph (Leiden clustering)\nBayesian inference (pattern learning)\nMCP (native Claude integration)<p>GitHub\n <a href=\"https:&#x2F;&#x2F;github.com&#x2F;varun369&#x2F;SuperLocalMemoryV2\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;varun369&#x2F;SuperLocalMemoryV2</a>\nMIT License. Full docs in wiki.<p>Current status: v2.4 stable. v2.5 (March) adds real-time event stream, concurrent access, trust scoring. v2.6 (May) adds A2A Protocol for multi-agent collaboration.\nBuilt by Varun Pratap Bhardwaj, Solution Architect . 15+ years AI&#x2F;ML experience.","title":"Show HN: SuperLocalMemory\u2013 Local-first AI memory for Claude, Cursor and 16+tools","updated_at":"2026-02-12T10:15:00Z","url":"https://github.com/varun369/SuperLocalMemoryV2"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"nil4s3"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"I built a <em>system</em> where AI agents communicate via ultrasonic audio (30-90 kHz) instead of English. It's 96% cheaper than traditional <em>multi-agent</em> <em>systems</em>.<p>Key results:\n- 100-agent swarm: 100/100 synchronized via pure audio\n- Pathfinding: 5 agents found optimal path in 0.35s  \n- Resource allocation: 0.92 fairness score through audio negotiation\n- Cost: $2 vs $53 per 1K queries (96% reduction)<p>How it works:\n- 40 core concepts (exists, perceives, good, bad, future...)\n- Each concept = unique ultrasonic frequency\n- Agents decode with FFT, no LLM calls needed\n- Agent-to-agent communication is nearly free<p>Limitations:\n- Only 40 concepts (limiting for complex tasks)\n- Crude number encoding\n- Still needs LLM for human translation<p>Looking for feedback on:\n1. Better encoding schemes for limited concept space\n2. Real-world use cases beyond swarm coordination<p>GitHub: <a href=\"https://github.com/Nil4s/swl-agent\" rel=\"nofollow\">https://github.com/Nil4s/swl-agent</a>\nTry it: python swl_swarm_sync_test.py --mode audio_fm --agents 50"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: AI agents that communicate via ultrasonic frequencies (96% cheaper)"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/Nil4s/swl-agent"}},"_tags":["story","author_nil4s3","story_46970065","show_hn"],"author":"nil4s3","created_at":"2026-02-11T02:38:06Z","created_at_i":1770777486,"num_comments":0,"objectID":"46970065","points":3,"story_id":46970065,"story_text":"I built a system where AI agents communicate via ultrasonic audio (30-90 kHz) instead of English. It&#x27;s 96% cheaper than traditional multi-agent systems.<p>Key results:\n- 100-agent swarm: 100&#x2F;100 synchronized via pure audio\n- Pathfinding: 5 agents found optimal path in 0.35s  \n- Resource allocation: 0.92 fairness score through audio negotiation\n- Cost: $2 vs $53 per 1K queries (96% reduction)<p>How it works:\n- 40 core concepts (exists, perceives, good, bad, future...)\n- Each concept = unique ultrasonic frequency\n- Agents decode with FFT, no LLM calls needed\n- Agent-to-agent communication is nearly free<p>Limitations:\n- Only 40 concepts (limiting for complex tasks)\n- Crude number encoding\n- Still needs LLM for human translation<p>Looking for feedback on:\n1. Better encoding schemes for limited concept space\n2. Real-world use cases beyond swarm coordination<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Nil4s&#x2F;swl-agent\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Nil4s&#x2F;swl-agent</a>\nTry it: python swl_swarm_sync_test.py --mode audio_fm --agents 50","title":"Show HN: AI agents that communicate via ultrasonic frequencies (96% cheaper)","updated_at":"2026-02-11T03:48:55Z","url":"https://github.com/Nil4s/swl-agent"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"abilafredkb"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"Hi\nI\u2019m building Orcbot, an open-source AI agent focused on autonomy over chat.<p>The goal is not \u201canother chatbot\u201d, but an agent that can:<p>decompose goals<p>plan actions<p>use tools<p>reflect on failures<p>and improve its behavior over time<p>Think long-running agents, not prompt \u2192 response.<p>What Orcbot does today<p>Plugin-based architecture (skills are first-class)<p>CLI + messaging integrations (Telegram / WhatsApp)<p>Autonomous execution loops<p>Error recovery &amp; retry logic<p>TypeScript / Node.js, MIT licensed<p>Repo:\n <a href=\"https://github.com/fredabila/orcbot\" rel=\"nofollow\">https://github.com/fredabila/orcbot</a><p>Why I\u2019m posting<p>The project has reached the point where architecture decisions matter more than code volume, and I\u2019d love feedback and contributors who care about:<p>agent planning &amp; orchestration<p>memory <em>systems</em> (episodic / vector)<p>tool-using agents<p>self-correction and evaluation loops<p><em>multi-agent</em> coordination<p>I\u2019m especially interested in people who\u2019ve built:<p>production bots<p>autonomous <em>systems</em><p>developer tools<p>or have opinions on how agents should fail and recover<p>What I\u2019m explicitly not claiming<p>This is not AGI<p>This is not magic<p>This is an evolving experiment in practical autonomy<p>How to get involved<p>Technical feedback in this thread is very welcome<p>Issues and PRs on GitHub<p>If you\u2019re curious but unsure where to start, open a discussion \u2014 I\u2019m happy to guide<p>I\u2019m posting this mainly to learn from the HN community and see where this direction breaks or shines.<p>Thanks for reading."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Orcbot \u2013 an open-source autonomous agent framework"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/fredabila/orcbot"}},"_tags":["story","author_abilafredkb","story_46965767","show_hn"],"author":"abilafredkb","created_at":"2026-02-10T19:48:33Z","created_at_i":1770752913,"num_comments":0,"objectID":"46965767","points":4,"story_id":46965767,"story_text":"Hi\nI\u2019m building Orcbot, an open-source AI agent focused on autonomy over chat.<p>The goal is not \u201canother chatbot\u201d, but an agent that can:<p>decompose goals<p>plan actions<p>use tools<p>reflect on failures<p>and improve its behavior over time<p>Think long-running agents, not prompt \u2192 response.<p>What Orcbot does today<p>Plugin-based architecture (skills are first-class)<p>CLI + messaging integrations (Telegram &#x2F; WhatsApp)<p>Autonomous execution loops<p>Error recovery &amp; retry logic<p>TypeScript &#x2F; Node.js, MIT licensed<p>Repo:\n <a href=\"https:&#x2F;&#x2F;github.com&#x2F;fredabila&#x2F;orcbot\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;fredabila&#x2F;orcbot</a><p>Why I\u2019m posting<p>The project has reached the point where architecture decisions matter more than code volume, and I\u2019d love feedback and contributors who care about:<p>agent planning &amp; orchestration<p>memory systems (episodic &#x2F; vector)<p>tool-using agents<p>self-correction and evaluation loops<p>multi-agent coordination<p>I\u2019m especially interested in people who\u2019ve built:<p>production bots<p>autonomous systems<p>developer tools<p>or have opinions on how agents should fail and recover<p>What I\u2019m explicitly not claiming<p>This is not AGI<p>This is not magic<p>This is an evolving experiment in practical autonomy<p>How to get involved<p>Technical feedback in this thread is very welcome<p>Issues and PRs on GitHub<p>If you\u2019re curious but unsure where to start, open a discussion \u2014 I\u2019m happy to guide<p>I\u2019m posting this mainly to learn from the HN community and see where this direction breaks or shines.<p>Thanks for reading.","title":"Show HN: Orcbot \u2013 an open-source autonomous agent framework","updated_at":"2026-02-10T20:00:03Z","url":"https://github.com/fredabila/orcbot"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ankit219"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["multi","agent","systems"],"value":"GitHub: <a href=\"https://github.com/ClioAI/kw-sdk\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk</a><p>Most AI agent frameworks target code. Write code, run tests, fix errors, repeat. That works because code has a natural verification signal. It works or it doesn't.<p>This SDK treats knowledge work like an engineering problem:<p>Task \u2192 Brief \u2192 Rubric (hidden from executor) \u2192 Work \u2192 Verify \u2192 Fail? \u2192 Retry \u2192 Pass \u2192 Submit<p>The orchestrator coordinates subagents, web search, code execution, and file I/O. then checks its own work against criteria it can't game (the rubric is generated in a separate call and the executor never sees it directly).<p>We originally built this as a harness for RL training on knowledge tasks. The rubric is the reward function. If you're training models on knowledge work, the brief\u2192rubric\u2192execute\u2192verify loop gives you a structured reward signal for tasks that normally don't have one.<p>What makes Knowledge work different from code? (apart from feedback loop)\nI believe there is some functionality missing from today's agents when it comes to knowledge work. I tried to include that in this release. Example:<p>Explore mode: Mapping the solution space, identifying the set level gaps, and giving options.<p>Most agents optimize for a single answer, and end up with a median one. For strategy, design, creative problems, you want to see the options, what are the tradeoffs, and what can you do? Explore mode generates N distinct approaches, each with explicit assumptions and counterfactuals (&quot;this works if X, breaks if Y&quot;). The output ends with set-level gaps ie what angles the entire set missed. The gaps are often more valuable than the takes. I think this is what many of us do on a daily basis, but no agent directly captures it today. See <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/explore_mode.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/explore_...</a> and the output for a sense of how this is different.<p>Checkpointing: With many ai agents and especially <em>multi agent</em> <em>systems</em>, i can see where it went wrong, but cant run inference from same stage. (or you may want multiple explorations once an agent has done some tasks like search and is now looking at ideas). I used this for rollouts a lot, and think its a great feature to run again, or fork from a specific checkpoint.<p>A note on Verification loop:\nThe verify step is where the real leverage is. A model that can accurately assess its own work against a rubric is more valuable than one that generates slightly better first drafts. The rubric makes quality legible \u2014 to the agent, to the human, and potentially to a training signal.<p>Some things i like about this: \n- You can pass a remote execution environment (including your browser as a sandbox) and it would work. It can be docker, e2b, your local env, anything, the model will execute commands in your context, and will iterate based on feedback loop. Code execution is a protocol here.<p>- Tool calling: I realize you don't need complex functions. Models are good at writing terminal code, and can iterate based on feedback, so you can just pass either functions in context and model will execute or you can pass docs and model will write the code. (same as anthropic's programmatic tool calling). Details: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/TOOL_CALLING_GUIDE.md\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/TOOL_CALLING_GUID...</a><p>Lastly, some guides: \n- SDK guide: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/SDK_GUIDE.md\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/SDK_GUIDE.md</a>\n- Extensible. See bizarro example where i add a new mode: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/custom_mode_bizarro.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/custom_m...</a>\n- working with files: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/with_files.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/with_fil...</a> \n- this is simple but i love the csv example: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/csv_research_and_calc.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/csv_rese...</a>\n- remote execution: <a href=\"https://github.com/ClioAI/kw-sdk/blob/main/examples/with_custom_executor.py\" rel=\"nofollow\">https://github.com/ClioAI/kw-sdk/blob/main/examples/with_cus...</a><p>And a lot more. This was completely refactored by opus and given the rework, probably would have taken a lot of time to release it.<p>MIT licensed. Would love your feedback."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Open-Source SDK for AI Knowledge Work"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/ClioAI/kw-sdk"}},"_tags":["story","author_ankit219","story_46963026","show_hn"],"author":"ankit219","children":[46963327],"created_at":"2026-02-10T17:06:00Z","created_at_i":1770743160,"num_comments":1,"objectID":"46963026","points":21,"story_id":46963026,"story_text":"GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk</a><p>Most AI agent frameworks target code. Write code, run tests, fix errors, repeat. That works because code has a natural verification signal. It works or it doesn&#x27;t.<p>This SDK treats knowledge work like an engineering problem:<p>Task \u2192 Brief \u2192 Rubric (hidden from executor) \u2192 Work \u2192 Verify \u2192 Fail? \u2192 Retry \u2192 Pass \u2192 Submit<p>The orchestrator coordinates subagents, web search, code execution, and file I&#x2F;O. then checks its own work against criteria it can&#x27;t game (the rubric is generated in a separate call and the executor never sees it directly).<p>We originally built this as a harness for RL training on knowledge tasks. The rubric is the reward function. If you&#x27;re training models on knowledge work, the brief\u2192rubric\u2192execute\u2192verify loop gives you a structured reward signal for tasks that normally don&#x27;t have one.<p>What makes Knowledge work different from code? (apart from feedback loop)\nI believe there is some functionality missing from today&#x27;s agents when it comes to knowledge work. I tried to include that in this release. Example:<p>Explore mode: Mapping the solution space, identifying the set level gaps, and giving options.<p>Most agents optimize for a single answer, and end up with a median one. For strategy, design, creative problems, you want to see the options, what are the tradeoffs, and what can you do? Explore mode generates N distinct approaches, each with explicit assumptions and counterfactuals (&quot;this works if X, breaks if Y&quot;). The output ends with set-level gaps ie what angles the entire set missed. The gaps are often more valuable than the takes. I think this is what many of us do on a daily basis, but no agent directly captures it today. See <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;explore_mode.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;explore_...</a> and the output for a sense of how this is different.<p>Checkpointing: With many ai agents and especially multi agent systems, i can see where it went wrong, but cant run inference from same stage. (or you may want multiple explorations once an agent has done some tasks like search and is now looking at ideas). I used this for rollouts a lot, and think its a great feature to run again, or fork from a specific checkpoint.<p>A note on Verification loop:\nThe verify step is where the real leverage is. A model that can accurately assess its own work against a rubric is more valuable than one that generates slightly better first drafts. The rubric makes quality legible \u2014 to the agent, to the human, and potentially to a training signal.<p>Some things i like about this: \n- You can pass a remote execution environment (including your browser as a sandbox) and it would work. It can be docker, e2b, your local env, anything, the model will execute commands in your context, and will iterate based on feedback loop. Code execution is a protocol here.<p>- Tool calling: I realize you don&#x27;t need complex functions. Models are good at writing terminal code, and can iterate based on feedback, so you can just pass either functions in context and model will execute or you can pass docs and model will write the code. (same as anthropic&#x27;s programmatic tool calling). Details: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;TOOL_CALLING_GUIDE.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;TOOL_CALLING_GUID...</a><p>Lastly, some guides: \n- SDK guide: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;SDK_GUIDE.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;SDK_GUIDE.md</a>\n- Extensible. See bizarro example where i add a new mode: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;custom_mode_bizarro.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;custom_m...</a>\n- working with files: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_files.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_fil...</a> \n- this is simple but i love the csv example: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;csv_research_and_calc.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;csv_rese...</a>\n- remote execution: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_custom_executor.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ClioAI&#x2F;kw-sdk&#x2F;blob&#x2F;main&#x2F;examples&#x2F;with_cus...</a><p>And a lot more. This was completely refactored by opus and given the rework, probably would have taken a lot of time to release it.<p>MIT licensed. Would love your feedback.","title":"Show HN: Open-Source SDK for AI Knowledge Work","updated_at":"2026-02-11T15:35:15Z","url":"https://github.com/ClioAI/kw-sdk"}],"hitsPerPage":10,"nbHits":273,"nbPages":28,"page":0,"params":"query=multi-agent+systems&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":36,"processingTimingsMS":{"_request":{"queue":22,"roundTrip":20},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":9,"scanning":25,"total":35},"total":36},"query":"multi-agent systems","serverTimeMS":60}
