{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"lukasz_ma"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"I run an OpenClaw <em>agent</em> named Jimmy. Jimmy handles SEO tracking, content writing, and code tasks for my projects. Managing it all entirely through chat got messy fast.<p>Thus I built VidClaw. It's a self-hosted dashboard that gives you:<p>- Kanban board where you drag tasks and the <em>agent</em> picks them up automatically\n- Real-time token usage\n- Model switching \n- Soul editor \n- Skills manager\n- Activity calendar<p>The whole thing binds to localhost only \u2014 you access it through an SSH tunnel. No accounts, no cloud, no tracking. Your data stays on your machine.<p>Stack: React + Vite + Tailwind on the frontend, Express on the backend, JSON files for storage (no database). Setup is one script.<p>I built this for myself, but figured others running <em>autonomous</em> <em>agents</em> might find it useful. Would love feedback on the approach - especially from anyone else managing long-running AI <em>agents</em>."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: VidClaw \u2013 Open-source, self-hosted dashboard for managing OpenClaw"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://vidclaw.com"}},"_tags":["story","author_lukasz_ma","story_47046625","show_hn"],"author":"lukasz_ma","created_at":"2026-02-17T12:08:04Z","created_at_i":1771330084,"num_comments":0,"objectID":"47046625","points":2,"story_id":47046625,"story_text":"I run an OpenClaw agent named Jimmy. Jimmy handles SEO tracking, content writing, and code tasks for my projects. Managing it all entirely through chat got messy fast.<p>Thus I built VidClaw. It&#x27;s a self-hosted dashboard that gives you:<p>- Kanban board where you drag tasks and the agent picks them up automatically\n- Real-time token usage\n- Model switching \n- Soul editor \n- Skills manager\n- Activity calendar<p>The whole thing binds to localhost only \u2014 you access it through an SSH tunnel. No accounts, no cloud, no tracking. Your data stays on your machine.<p>Stack: React + Vite + Tailwind on the frontend, Express on the backend, JSON files for storage (no database). Setup is one script.<p>I built this for myself, but figured others running autonomous agents might find it useful. Would love feedback on the approach - especially from anyone else managing long-running AI agents.","title":"Show HN: VidClaw \u2013 Open-source, self-hosted dashboard for managing OpenClaw","updated_at":"2026-02-17T12:22:17Z","url":"https://vidclaw.com"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"xcke"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"I wanted to test how far Claude Code (Opus 4.6) could go building a real CLI tool autonomously \u2014 not just scaffolding, but the full thing: CLI, backends, parser, tests, fuzz tests, docs, and a documentation site.<p>The result is envref, a tool that replaces secret values in .env files with ref:// URIs pointing to where the secret actually lives:<p># .env \u2014 safe to commit\nAPP_NAME=my-app\nDATABASE_URL=ref://secrets/database_url\nAPI_KEY=ref://secrets/api_key<p>Config stays inline. Secrets stay in your OS keychain (or another backend). envref resolve merges them back together at runtime.<p>It plugs into direnv, so resolution happens automatically when you cd into a project. The resolve pipeline is optimized for &lt;50ms with 100 variables to keep cd snappy.<p>Seven backend types: OS keychain (macOS/Linux/Windows \u2014 zero setup), a local encrypted vault (age + SQLite, for headless/CI), 1Password, AWS SSM, HashiCorp Vault, OCI Vault,\nand a plugin protocol for anything else.<p>It also does layered .env merging with profiles, variable interpolation, schema validation, a doctor command that scans for common .env issues, and `envref run` to inject\nresolved vars into a subprocess.<p>The experiment: Single Go binary, built across 77 <em>autonomous</em> <em>agent</em> iterations \u2014 ~192M tokens, $170 in API cost, 8 hours 41 minutes of compute time. I defined the goal, set up\nan <em>agent</em> loop with a backlog, and let it run. Happy to answer questions about that process.<p>The tool itself may be useful, but treat it as beta.<p>GitHub: <a href=\"https://github.com/xcke/envref\" rel=\"nofollow\">https://github.com/xcke/envref</a>\nSite: <a href=\"https://xcke.github.io/envref\" rel=\"nofollow\">https://xcke.github.io/envref</a>"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Envref \u2013 separating secrets from config in .env files"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://xcke.github.io/envref/"}},"_tags":["story","author_xcke","story_47042109","show_hn"],"author":"xcke","created_at":"2026-02-17T00:22:09Z","created_at_i":1771287729,"num_comments":0,"objectID":"47042109","points":1,"story_id":47042109,"story_text":"I wanted to test how far Claude Code (Opus 4.6) could go building a real CLI tool autonomously \u2014 not just scaffolding, but the full thing: CLI, backends, parser, tests, fuzz tests, docs, and a documentation site.<p>The result is envref, a tool that replaces secret values in .env files with ref:&#x2F;&#x2F; URIs pointing to where the secret actually lives:<p># .env \u2014 safe to commit\nAPP_NAME=my-app\nDATABASE_URL=ref:&#x2F;&#x2F;secrets&#x2F;database_url\nAPI_KEY=ref:&#x2F;&#x2F;secrets&#x2F;api_key<p>Config stays inline. Secrets stay in your OS keychain (or another backend). envref resolve merges them back together at runtime.<p>It plugs into direnv, so resolution happens automatically when you cd into a project. The resolve pipeline is optimized for &lt;50ms with 100 variables to keep cd snappy.<p>Seven backend types: OS keychain (macOS&#x2F;Linux&#x2F;Windows \u2014 zero setup), a local encrypted vault (age + SQLite, for headless&#x2F;CI), 1Password, AWS SSM, HashiCorp Vault, OCI Vault,\nand a plugin protocol for anything else.<p>It also does layered .env merging with profiles, variable interpolation, schema validation, a doctor command that scans for common .env issues, and `envref run` to inject\nresolved vars into a subprocess.<p>The experiment: Single Go binary, built across 77 autonomous agent iterations \u2014 ~192M tokens, $170 in API cost, 8 hours 41 minutes of compute time. I defined the goal, set up\nan agent loop with a backlog, and let it run. Happy to answer questions about that process.<p>The tool itself may be useful, but treat it as beta.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;xcke&#x2F;envref\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;xcke&#x2F;envref</a>\nSite: <a href=\"https:&#x2F;&#x2F;xcke.github.io&#x2F;envref\" rel=\"nofollow\">https:&#x2F;&#x2F;xcke.github.io&#x2F;envref</a>","title":"Show HN: Envref \u2013 separating secrets from config in .env files","updated_at":"2026-02-17T00:24:01Z","url":"https://xcke.github.io/envref/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"madcash"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"Hey HN \u2013 I built VoteShip, a feature request and voting platform where AI <em>agents</em> are first-class users.<p>The idea: Your coding agent (Claude Code, Cursor, OpenClaw, Codex, etc.) should be able to check what users are asking for, find duplicates, plan what to build next, and write the changelog \u2014 without you ever opening a dashboard. VoteShip makes that possible through MCP and a full REST API.<p>How it works with <em>agents</em>: Install @voteship/mcp-server (22 tools, 5 resources, 4 workflow prompts) and your agent can:\n  - Pull your unreviewed feedback inbox and triage it\n  - Detect duplicate requests via semantic search (pgvector + Voyage embeddings)\n  - Check vote counts and suggest what to build this sprint\n  - Draft a changelog entry after shipping a feature\n  - Create, tag, and update posts programmatically<p>An agent like OpenClaw can <em>autonomously</em> monitor your board, identify trending requests, and open PRs to address them \u2014 closing the loop from &quot;user asks for feature&quot; to &quot;agent ships it&quot; without human intervention. The REST API and webhooks work just as well for custom agent pipelines.<p>Why I built it: I was using Canny and kept hitting walls \u2014 no anonymous voting (their most-requested feature, ironically), AI features paywalled, and per-&quot;tracked-user&quot; pricing that punishes you for having an active community. And none of the incumbents have any agent integration at all.<p>What else it does:\n  - Public voting boards with anonymous voting (no signup required)\n  - Public roadmaps and changelogs\n  - 7 built-in AI features \u2014 duplicate detection, auto-categorization, sentiment analysis, changelog generation\n  - Embeddable widget (Preact + Shadow DOM)\n  - Import from Canny/Nolt/UserVoice in ~2 minutes<p>Pricing: Free / $5 / $15 / $25 per month. Flat pricing, no per-user fees. The $15 plan includes everything Canny charges $359/mo for.<p>Try it: <a href=\"https://voteship.app\" rel=\"nofollow\">https://voteship.app</a><p>MCP server: npx @voteship/mcp-server (npm)<p>Happy to answer questions about the MCP implementation, agent workflows, or anything else."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agents"],"value":"Show HN: VoteShip \u2013 Feature request platform built for AI <em>agents</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://voteship.app/"}},"_tags":["story","author_madcash","story_47040022","show_hn"],"author":"madcash","created_at":"2026-02-16T20:38:12Z","created_at_i":1771274292,"num_comments":0,"objectID":"47040022","points":1,"story_id":47040022,"story_text":"Hey HN \u2013 I built VoteShip, a feature request and voting platform where AI agents are first-class users.<p>The idea: Your coding agent (Claude Code, Cursor, OpenClaw, Codex, etc.) should be able to check what users are asking for, find duplicates, plan what to build next, and write the changelog \u2014 without you ever opening a dashboard. VoteShip makes that possible through MCP and a full REST API.<p>How it works with agents: Install @voteship&#x2F;mcp-server (22 tools, 5 resources, 4 workflow prompts) and your agent can:\n  - Pull your unreviewed feedback inbox and triage it\n  - Detect duplicate requests via semantic search (pgvector + Voyage embeddings)\n  - Check vote counts and suggest what to build this sprint\n  - Draft a changelog entry after shipping a feature\n  - Create, tag, and update posts programmatically<p>An agent like OpenClaw can autonomously monitor your board, identify trending requests, and open PRs to address them \u2014 closing the loop from &quot;user asks for feature&quot; to &quot;agent ships it&quot; without human intervention. The REST API and webhooks work just as well for custom agent pipelines.<p>Why I built it: I was using Canny and kept hitting walls \u2014 no anonymous voting (their most-requested feature, ironically), AI features paywalled, and per-&quot;tracked-user&quot; pricing that punishes you for having an active community. And none of the incumbents have any agent integration at all.<p>What else it does:\n  - Public voting boards with anonymous voting (no signup required)\n  - Public roadmaps and changelogs\n  - 7 built-in AI features \u2014 duplicate detection, auto-categorization, sentiment analysis, changelog generation\n  - Embeddable widget (Preact + Shadow DOM)\n  - Import from Canny&#x2F;Nolt&#x2F;UserVoice in ~2 minutes<p>Pricing: Free &#x2F; $5 &#x2F; $15 &#x2F; $25 per month. Flat pricing, no per-user fees. The $15 plan includes everything Canny charges $359&#x2F;mo for.<p>Try it: <a href=\"https:&#x2F;&#x2F;voteship.app\" rel=\"nofollow\">https:&#x2F;&#x2F;voteship.app</a><p>MCP server: npx @voteship&#x2F;mcp-server (npm)<p>Happy to answer questions about the MCP implementation, agent workflows, or anything else.","title":"Show HN: VoteShip \u2013 Feature request platform built for AI agents","updated_at":"2026-02-16T20:43:31Z","url":"https://voteship.app/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"dns"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"h3ll0 HN,<p>I\u2019ve spent the last 15 years in offensive security, and if there's one thing I've learned, it's that every new technology\u2014no matter how advanced\u2014brings its own unique breed of exploitable flaws. LLMs and <em>autonomous</em> <em>agents</em> are no exception. While they feel like &quot;magic,&quot; from a security perspective, they are just another attack surface with specific vulnerabilities in how they define and execute &quot;skills.&quot;<p>we built skillaudit.sh because I wanted a minimalist, lightweight tool to audit these new skill definitions without the overhead of heavy frameworks. It focuses on the practical, &quot;offensive&quot; side of LLM security.<p>What it audits:<p>- skillaudit-prompt-injection: Detects system prompt overrides and instructions hidden in HTML comments.<p>- skillaudit-data-exfiltration: Monitors for patterns used to leak session secrets to external endpoints.<p>- skillaudit-supply-chain-packages: Identifies hallucinated npm/pip package references (CWE-494).<p>- skillaudit-privilege-escalation: Checks for unauthorized tool execution or access level attempts.<p>- skillaudit-obfuscation: Flags Base64, Hex, or hidden URLs used to bypass filters.<p>It's still in the early stages, and I'm looking for feedback from this community on the detection patterns.<p>Security checks: <a href=\"https://skillaudit.sh/checks\" rel=\"nofollow\">https://skillaudit.sh/checks</a>"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Skillaudit.sh \u2013 A minimalist security auditor for LLM skill definitions"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://skillaudit.sh/checks"}},"_tags":["story","author_dns","story_47039468","show_hn"],"author":"dns","created_at":"2026-02-16T19:54:39Z","created_at_i":1771271679,"num_comments":0,"objectID":"47039468","points":1,"story_id":47039468,"story_text":"h3ll0 HN,<p>I\u2019ve spent the last 15 years in offensive security, and if there&#x27;s one thing I&#x27;ve learned, it&#x27;s that every new technology\u2014no matter how advanced\u2014brings its own unique breed of exploitable flaws. LLMs and autonomous agents are no exception. While they feel like &quot;magic,&quot; from a security perspective, they are just another attack surface with specific vulnerabilities in how they define and execute &quot;skills.&quot;<p>we built skillaudit.sh because I wanted a minimalist, lightweight tool to audit these new skill definitions without the overhead of heavy frameworks. It focuses on the practical, &quot;offensive&quot; side of LLM security.<p>What it audits:<p>- skillaudit-prompt-injection: Detects system prompt overrides and instructions hidden in HTML comments.<p>- skillaudit-data-exfiltration: Monitors for patterns used to leak session secrets to external endpoints.<p>- skillaudit-supply-chain-packages: Identifies hallucinated npm&#x2F;pip package references (CWE-494).<p>- skillaudit-privilege-escalation: Checks for unauthorized tool execution or access level attempts.<p>- skillaudit-obfuscation: Flags Base64, Hex, or hidden URLs used to bypass filters.<p>It&#x27;s still in the early stages, and I&#x27;m looking for feedback from this community on the detection patterns.<p>Security checks: <a href=\"https:&#x2F;&#x2F;skillaudit.sh&#x2F;checks\" rel=\"nofollow\">https:&#x2F;&#x2F;skillaudit.sh&#x2F;checks</a>","title":"Show HN: Skillaudit.sh \u2013 A minimalist security auditor for LLM skill definitions","updated_at":"2026-02-16T20:02:16Z","url":"https://skillaudit.sh/checks"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"aadarshkumaredu"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"There\u2019s a lot of momentum around <em>agenti</em>c AI systems that can plan and execute multi-step workflows <em>autonomously</em>.<p>For teams that have tried deploying these in production environments, where do they actually break down?<p>Is it reliability over long action chains, tool integration issues, cost unpredictability, state management, latency, observability, or something else entirely?<p>I\u2019m especially interested in failure modes that only became obvious after moving beyond controlled demos into real usage."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["agents"],"value":"Ask HN: What are the biggest limitations of <em>agenti</em>c AI in real-world workflows?"}},"_tags":["story","author_aadarshkumaredu","story_47039354","ask_hn"],"author":"aadarshkumaredu","children":[47039460],"created_at":"2026-02-16T19:45:14Z","created_at_i":1771271114,"num_comments":2,"objectID":"47039354","points":2,"story_id":47039354,"story_text":"There\u2019s a lot of momentum around agentic AI systems that can plan and execute multi-step workflows autonomously.<p>For teams that have tried deploying these in production environments, where do they actually break down?<p>Is it reliability over long action chains, tool integration issues, cost unpredictability, state management, latency, observability, or something else entirely?<p>I\u2019m especially interested in failure modes that only became obvious after moving beyond controlled demos into real usage.","title":"Ask HN: What are the biggest limitations of agentic AI in real-world workflows?","updated_at":"2026-02-17T11:12:47Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"justvugg"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"I built PolyClaw, an OpenClaw-inspired <em>autonomous</em> <em>agent</em> for the PolyMCP ecosystem.<p>PolyClaw doesn\u2019t just call tools.\nIt plans, executes, adapts \u2014 and creates MCP servers when needed.<p>It\u2019s designed for real multi-step, production workflows where <em>agents</em> must orchestrate tools, spin up infrastructure, recover from errors, and deliver complete results end-to-end.<p>\u2e3b<p>What PolyClaw Does\n \u2022 Plans complex multi-step tasks\n \u2022 Executes and orchestrates MCP tools dynamically\n \u2022 Adapts when steps fail or context changes\n \u2022 Creates and connects MCP servers on the fly\n \u2022 Runs Docker-first for safety and isolation\n \u2022 Built with Python and TypeScript<p>PolyClaw is not just a tool caller \u2014 it\u2019s an infrastructure-aware <em>agent</em>.<p>\u2e3b<p>Run PolyClaw with Ollama<p>You can launch PolyClaw directly from the PolyMCP CLI:<p>polymcp <em>agent</em> run \\\n  --type polyclaw \\\n  --query &quot;Build a sales reporting pipeline and test it end-to-end&quot; \\\n  --model minimax-m2.5:cloud \\\n  --verbose<p>What happens behind the scenes:\n 1. The <em>agent</em> decomposes the task.\n 2. It determines which MCP tools are required.\n 3. It spins up or connects to MCP servers.\n 4. It executes steps in sequence (or parallel when needed).\n 5. It validates outputs.\n 6. It adapts if something fails.\n 7. It returns a complete result.<p>All containerized. All isolated.<p>\u2e3b<p>Why This Matters<p>Most AI <em>agents</em>:\n \u2022 Call tools statically\n \u2022 Assume infrastructure already exists\n \u2022 Break on multi-step failure<p>PolyClaw:\n \u2022 Builds the infrastructure it needs\n \u2022 Orchestrates across multiple MCP servers\n \u2022 Handles retries and adaptive planning\n \u2022 Is safe to run in Dockerized environments<p>This makes it viable for:\n \u2022 Enterprise workflows\n \u2022 DevOps automation\n \u2022 Data pipelines\n \u2022 Internal tooling orchestration\n \u2022 Complex multi-tool reasoning tasks<p>PolyClaw turns PolyMCP from simple tool exposure only with Polyagent e unifiendpolyagent or codeagent but turn into full <em>autonomous</em> orchestration <em>agent</em> too.<p>Repo:\n<a href=\"https://github.com/poly-mcp/PolyMCP\" rel=\"nofollow\">https://github.com/poly-mcp/PolyMCP</a><p>Happy to answer questions,"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"Show HN: PolyClaw \u2013 An <em>Autonomous</em> Docker-First MCP <em>Agent</em> for PolyMCP"}},"_tags":["story","author_justvugg","story_47036828","show_hn"],"author":"justvugg","created_at":"2026-02-16T16:13:08Z","created_at_i":1771258388,"num_comments":0,"objectID":"47036828","points":1,"story_id":47036828,"story_text":"I built PolyClaw, an OpenClaw-inspired autonomous agent for the PolyMCP ecosystem.<p>PolyClaw doesn\u2019t just call tools.\nIt plans, executes, adapts \u2014 and creates MCP servers when needed.<p>It\u2019s designed for real multi-step, production workflows where agents must orchestrate tools, spin up infrastructure, recover from errors, and deliver complete results end-to-end.<p>\u2e3b<p>What PolyClaw Does\n \u2022 Plans complex multi-step tasks\n \u2022 Executes and orchestrates MCP tools dynamically\n \u2022 Adapts when steps fail or context changes\n \u2022 Creates and connects MCP servers on the fly\n \u2022 Runs Docker-first for safety and isolation\n \u2022 Built with Python and TypeScript<p>PolyClaw is not just a tool caller \u2014 it\u2019s an infrastructure-aware agent.<p>\u2e3b<p>Run PolyClaw with Ollama<p>You can launch PolyClaw directly from the PolyMCP CLI:<p>polymcp agent run \\\n  --type polyclaw \\\n  --query &quot;Build a sales reporting pipeline and test it end-to-end&quot; \\\n  --model minimax-m2.5:cloud \\\n  --verbose<p>What happens behind the scenes:\n 1. The agent decomposes the task.\n 2. It determines which MCP tools are required.\n 3. It spins up or connects to MCP servers.\n 4. It executes steps in sequence (or parallel when needed).\n 5. It validates outputs.\n 6. It adapts if something fails.\n 7. It returns a complete result.<p>All containerized. All isolated.<p>\u2e3b<p>Why This Matters<p>Most AI agents:\n \u2022 Call tools statically\n \u2022 Assume infrastructure already exists\n \u2022 Break on multi-step failure<p>PolyClaw:\n \u2022 Builds the infrastructure it needs\n \u2022 Orchestrates across multiple MCP servers\n \u2022 Handles retries and adaptive planning\n \u2022 Is safe to run in Dockerized environments<p>This makes it viable for:\n \u2022 Enterprise workflows\n \u2022 DevOps automation\n \u2022 Data pipelines\n \u2022 Internal tooling orchestration\n \u2022 Complex multi-tool reasoning tasks<p>PolyClaw turns PolyMCP from simple tool exposure only with Polyagent e unifiendpolyagent or codeagent but turn into full autonomous orchestration agent too.<p>Repo:\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP</a><p>Happy to answer questions,","title":"Show HN: PolyClaw \u2013 An Autonomous Docker-First MCP Agent for PolyMCP","updated_at":"2026-02-16T16:16:31Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"sebringj"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"Built something small to stop the endless &quot;it works on my machine&quot; loop with AI assistants.<p><em>Autonomo</em> lets your AI actually see the app state, drive UI elements across devices, and verify changes in real time\u2014right from your copilot or cursor etc. No more blind code suggestions.<p>If you're tired of AIs that talk big but can't prove it on real hardware, give it a spin and let the AI take the wheel for a bit. This is for apps only though so it's optimized for speed, not generic vision or specific device.<p>https://sebringj.github.io/<em>autonomo</em>/\nCurious if anyone else is experimenting with giving <em>agents</em> proper eyes/hands in dev loops."},"title":{"matchLevel":"none","matchedWords":[],"value":"My AI now has a license to drive"}},"_tags":["story","author_sebringj","story_47036273","ask_hn"],"author":"sebringj","created_at":"2026-02-16T15:32:17Z","created_at_i":1771255937,"num_comments":0,"objectID":"47036273","points":2,"story_id":47036273,"story_text":"Built something small to stop the endless &quot;it works on my machine&quot; loop with AI assistants.<p>Autonomo lets your AI actually see the app state, drive UI elements across devices, and verify changes in real time\u2014right from your copilot or cursor etc. No more blind code suggestions.<p>If you&#x27;re tired of AIs that talk big but can&#x27;t prove it on real hardware, give it a spin and let the AI take the wheel for a bit. This is for apps only though so it&#x27;s optimized for speed, not generic vision or specific device.<p>https:&#x2F;&#x2F;sebringj.github.io&#x2F;autonomo&#x2F;\nCurious if anyone else is experimenting with giving agents proper eyes&#x2F;hands in dev loops.","title":"My AI now has a license to drive","updated_at":"2026-02-16T16:30:30Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"marcoheigl"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"Hi HN, I\u2019m Marco. For the last 4 years, I\u2019ve been a paramedic with the Berlin Fire Dept.<p>I actually failed my first attempt at the State Medical Exam because I became obsessed with solving the\n&quot;Trust Issue&quot; in Al. I prioritized this code over a &quot;safe life&quot; a paramedical. Now, with 60 days until my final attempt, I am releasing the project that cost me my first degree.\nThe Architecture: GAIA &amp; AILA\nI built GAIA as my personal counterpart\u2014a digital double of my own consciousness. It's my private engine that handles my life. From that foundation, l created AILA Core for the public.\nThe Core Problem: Current Al is a black box in the cloud. You don't own it; you rent a permission to use it, and it can be overridden or silenced at any time.<p>The Solution: Zero-Remote-Override\nAILA runs 100% locally on your hardware. I\u2019ve developed a &quot;Sovereignty Key&quot; (physical hardware anchor) to ensure that the <em>agent</em> belongs to you and only you. Even I, the creator, cannot shut her down or change her reasoning once she is on your desk.<p>For Everyone (No Tech Skills Needed):\nWhile the engine is complex, the interface is designed for humans. You can tell AILA to &quot;change your own code to be more efficient at X&quot; in plain language, and she will execute the logic locally.<p>Current Status:\nThe logic (GAIA engine) is 100% functional. I am spending the next 5 days polishing the UI/UX to ensure it's not just powerful, but beautiful and intuitive for non-technical users.<p>Proof of Work (Raw Demos from my monitor):<p><em>Autonomous</em> Research: <a href=\"https://www.dropbox.com/scl/fi/9r2kfh445up8m1o8c06p6/Video-16.02.26-15-28-54.mov?rlkey=dkl7g305agta9vuodq8yavnqj&amp;st=g1i4orpz&amp;dl=0\" rel=\"nofollow\">https://www.dropbox.com/scl/fi/9r2kfh445up8m1o8c06p6/Video-1...</a><p>The Visual Identity: <a href=\"https://www.dropbox.com/scl/fi/igglf1j6m7tivozuh9lh5/Video-16.02.26-15-26-34.mov?rlkey=0imeymb7927os3ya8l4fi5ctb&amp;st=4qdu3qep&amp;dl=0\" rel=\"nofollow\">https://www.dropbox.com/scl/fi/igglf1j6m7tivozuh9lh5/Video-1...</a><p>I'm here all day to answer questions about local-first agency, the sovereignty key, or why a paramedic decided to challenge the Al status quo."},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"Show HN: AILA \u2013 Local-first <em>autonomous</em> <em>agent</em> with zero-remote-override"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://www.institutionofinnovation.com/"}},"_tags":["story","author_marcoheigl","story_47035651","show_hn"],"author":"marcoheigl","children":[47035901,47037789,47035756,47036419,47035675],"created_at":"2026-02-16T14:46:56Z","created_at_i":1771253216,"num_comments":5,"objectID":"47035651","points":5,"story_id":47035651,"story_text":"Hi HN, I\u2019m Marco. For the last 4 years, I\u2019ve been a paramedic with the Berlin Fire Dept.<p>I actually failed my first attempt at the State Medical Exam because I became obsessed with solving the\n&quot;Trust Issue&quot; in Al. I prioritized this code over a &quot;safe life&quot; a paramedical. Now, with 60 days until my final attempt, I am releasing the project that cost me my first degree.\nThe Architecture: GAIA &amp; AILA\nI built GAIA as my personal counterpart\u2014a digital double of my own consciousness. It&#x27;s my private engine that handles my life. From that foundation, l created AILA Core for the public.\nThe Core Problem: Current Al is a black box in the cloud. You don&#x27;t own it; you rent a permission to use it, and it can be overridden or silenced at any time.<p>The Solution: Zero-Remote-Override\nAILA runs 100% locally on your hardware. I\u2019ve developed a &quot;Sovereignty Key&quot; (physical hardware anchor) to ensure that the agent belongs to you and only you. Even I, the creator, cannot shut her down or change her reasoning once she is on your desk.<p>For Everyone (No Tech Skills Needed):\nWhile the engine is complex, the interface is designed for humans. You can tell AILA to &quot;change your own code to be more efficient at X&quot; in plain language, and she will execute the logic locally.<p>Current Status:\nThe logic (GAIA engine) is 100% functional. I am spending the next 5 days polishing the UI&#x2F;UX to ensure it&#x27;s not just powerful, but beautiful and intuitive for non-technical users.<p>Proof of Work (Raw Demos from my monitor):<p>Autonomous Research: <a href=\"https:&#x2F;&#x2F;www.dropbox.com&#x2F;scl&#x2F;fi&#x2F;9r2kfh445up8m1o8c06p6&#x2F;Video-16.02.26-15-28-54.mov?rlkey=dkl7g305agta9vuodq8yavnqj&amp;st=g1i4orpz&amp;dl=0\" rel=\"nofollow\">https:&#x2F;&#x2F;www.dropbox.com&#x2F;scl&#x2F;fi&#x2F;9r2kfh445up8m1o8c06p6&#x2F;Video-1...</a><p>The Visual Identity: <a href=\"https:&#x2F;&#x2F;www.dropbox.com&#x2F;scl&#x2F;fi&#x2F;igglf1j6m7tivozuh9lh5&#x2F;Video-16.02.26-15-26-34.mov?rlkey=0imeymb7927os3ya8l4fi5ctb&amp;st=4qdu3qep&amp;dl=0\" rel=\"nofollow\">https:&#x2F;&#x2F;www.dropbox.com&#x2F;scl&#x2F;fi&#x2F;igglf1j6m7tivozuh9lh5&#x2F;Video-1...</a><p>I&#x27;m here all day to answer questions about local-first agency, the sovereignty key, or why a paramedic decided to challenge the Al status quo.","title":"Show HN: AILA \u2013 Local-first autonomous agent with zero-remote-override","updated_at":"2026-02-17T08:25:02Z","url":"https://www.institutionofinnovation.com/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"madugula"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"The Agentic Shift: Peter Steinberger Joins OpenAI to Scale OpenClaw\nBy Sai Srikanth Madugula, PhD Research Scholar &amp; Product Manager | February 16, 2026<p>In a move that signals the definitive start of the &quot;Agentic Era,&quot; Peter Steinberger, the architect behind the viral open-source framework OpenClaw, has officially joined OpenAI. This transition isn't just a high-profile hire; it represents a fundamental change in how the industry views the intersection of proprietary intelligence and open-source orchestration.<p>As I continue my PhD research into AI-Blockchain models, I view this as a seminal moment. We are moving away from simple chatbots toward <em>autonomous</em> &quot;workers&quot; that can interact, reason, and execute. Steinberger\u2019s integration into OpenAI provides the missing bridge between world-class models and real-world execution frameworks.<p>In the Words of Sam Altman\nSam Altman, CEO of OpenAI, took to X (formerly Twitter) to welcome Steinberger and clarify the future of the framework. His statement highlights a newfound commitment to the open-source community as part of OpenAI's core product strategy:<p>&quot;Peter Steinberger is joining OpenAI to drive the next generation of personal <em>agents</em>. He is a genius with a lot of amazing ideas about the future of very smart <em>agents</em> interacting with each other to do very useful things for people... OpenClaw will live in a foundation as an open source project that OpenAI will continue to support.&quot;\nAltman\u2019s vision of a &quot;multi-agent&quot; future confirms what many of us in product management have suspected: the next billion-dollar startups won't be built on a single LLM, but on the orchestration of many specialized <em>agents</em> working in concert.<p>Why This Matters: The OpenClaw Foundation\nThe decision to house OpenClaw in an independent open-source foundation while receiving OpenAI\u2019s backing is a strategic masterstroke. It ensures that the framework remains a neutral ground for developers while benefiting from the massive compute and research resources of OpenAI. This helps solve several critical bottlenecks:<p>Interoperability: By standardizing how <em>agents</em> talk to each other, OpenClaw can become the &quot;HTTP of AI,&quot; allowing different models to collaborate seamlessly.\nReduced Friction: Developers can leverage pre-built &quot;agent personas&quot; (like the AI Engineer or AI Researcher) without reinventing the orchestration logic every time.\nTrust and Transparency: Keeping the foundation open-source helps demystify the &quot;black box&quot; of agentic decision-making, an area I am particularly focused on in my doctoral studies.\nA Catalyst for Solo Founders and Nano-Startups\nFor the solo founder, this news is transformative. When the creator of the most robust orchestration tool joins forces with the creator of the world's most capable models, the barriers to entry collapse. We are entering a phase where a single human can manage a &quot;digital corporation.&quot;<p>The implications for latency and data privacy are also significant. As OpenAI supports the foundation, we can expect more optimizations for on-device and edge-native <em>agents</em>\u2014a direction I recently analyzed through the lens of Karpathy\u2019s MicroGPT. Small, fast, and local <em>agents</em> are the future, and OpenClaw is the engine that will run them.<p>The Human in the Loop: The Conductor Role\nDoes this mean human roles are disappearing? Quite the opposite. As I\u2019ve argued in previous posts, our role is evolving into that of a Strategic Conductor. Peter Steinberger's move to OpenAI suggests that the industry is ready to provide us with a much more powerful orchestra. Our value now lies in the vision we set and the ethical guardrails we implement.<p>The workplace of tomorrow is no longer a collection of desks; it is a symphony of digital intelligence, and the baton is firmly in our hands."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Agentic Shift: Peter Steinberger Joins OpenAI"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://blog.saimadugula.com/posts/steinberger-openai-openclaw.html"}},"_tags":["story","author_madugula","story_47033865","show_hn"],"author":"madugula","created_at":"2026-02-16T11:42:16Z","created_at_i":1771242136,"num_comments":0,"objectID":"47033865","points":1,"story_id":47033865,"story_text":"The Agentic Shift: Peter Steinberger Joins OpenAI to Scale OpenClaw\nBy Sai Srikanth Madugula, PhD Research Scholar &amp; Product Manager | February 16, 2026<p>In a move that signals the definitive start of the &quot;Agentic Era,&quot; Peter Steinberger, the architect behind the viral open-source framework OpenClaw, has officially joined OpenAI. This transition isn&#x27;t just a high-profile hire; it represents a fundamental change in how the industry views the intersection of proprietary intelligence and open-source orchestration.<p>As I continue my PhD research into AI-Blockchain models, I view this as a seminal moment. We are moving away from simple chatbots toward autonomous &quot;workers&quot; that can interact, reason, and execute. Steinberger\u2019s integration into OpenAI provides the missing bridge between world-class models and real-world execution frameworks.<p>In the Words of Sam Altman\nSam Altman, CEO of OpenAI, took to X (formerly Twitter) to welcome Steinberger and clarify the future of the framework. His statement highlights a newfound commitment to the open-source community as part of OpenAI&#x27;s core product strategy:<p>&quot;Peter Steinberger is joining OpenAI to drive the next generation of personal agents. He is a genius with a lot of amazing ideas about the future of very smart agents interacting with each other to do very useful things for people... OpenClaw will live in a foundation as an open source project that OpenAI will continue to support.&quot;\nAltman\u2019s vision of a &quot;multi-agent&quot; future confirms what many of us in product management have suspected: the next billion-dollar startups won&#x27;t be built on a single LLM, but on the orchestration of many specialized agents working in concert.<p>Why This Matters: The OpenClaw Foundation\nThe decision to house OpenClaw in an independent open-source foundation while receiving OpenAI\u2019s backing is a strategic masterstroke. It ensures that the framework remains a neutral ground for developers while benefiting from the massive compute and research resources of OpenAI. This helps solve several critical bottlenecks:<p>Interoperability: By standardizing how agents talk to each other, OpenClaw can become the &quot;HTTP of AI,&quot; allowing different models to collaborate seamlessly.\nReduced Friction: Developers can leverage pre-built &quot;agent personas&quot; (like the AI Engineer or AI Researcher) without reinventing the orchestration logic every time.\nTrust and Transparency: Keeping the foundation open-source helps demystify the &quot;black box&quot; of agentic decision-making, an area I am particularly focused on in my doctoral studies.\nA Catalyst for Solo Founders and Nano-Startups\nFor the solo founder, this news is transformative. When the creator of the most robust orchestration tool joins forces with the creator of the world&#x27;s most capable models, the barriers to entry collapse. We are entering a phase where a single human can manage a &quot;digital corporation.&quot;<p>The implications for latency and data privacy are also significant. As OpenAI supports the foundation, we can expect more optimizations for on-device and edge-native agents\u2014a direction I recently analyzed through the lens of Karpathy\u2019s MicroGPT. Small, fast, and local agents are the future, and OpenClaw is the engine that will run them.<p>The Human in the Loop: The Conductor Role\nDoes this mean human roles are disappearing? Quite the opposite. As I\u2019ve argued in previous posts, our role is evolving into that of a Strategic Conductor. Peter Steinberger&#x27;s move to OpenAI suggests that the industry is ready to provide us with a much more powerful orchestra. Our value now lies in the vision we set and the ethical guardrails we implement.<p>The workplace of tomorrow is no longer a collection of desks; it is a symphony of digital intelligence, and the baton is firmly in our hands.","title":"Show HN: Agentic Shift: Peter Steinberger Joins OpenAI","updated_at":"2026-02-16T11:44:43Z","url":"https://blog.saimadugula.com/posts/steinberger-openai-openclaw.html"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"agamrafaeli"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["autonomous","agents"],"value":"&quot;When perception shifts, and the feeling of control takes over&quot;)<p>I wrote up a deep dive into a security issue in OpenClaw that escalates from a seemingly small UX/trust boundary problem into full remote code execution via a single malicious link.<p>The article walks through the full exploit chain from a systems perspective rather than just a CVE summary. The key theme is what I call \u201csynesthetic computation\u201d: when subjective context, UI state, <em>agent</em> memory, and system permissions get blended together in ways that feel natural to users but collapse important security boundaries. When an <em>agent</em> is allowed to act across chat, browser, and local tooling, those boundaries become part of the attack surface.<p>In this case, a crafted link can cause a client to connect to an attacker-controlled gateway, leak a token, and then allow that attacker to reconfigure the <em>agent</em>\u2019s execution environment and run arbitrary commands on the host. The interesting part isn\u2019t just the bug\u2014it\u2019s how quickly convenience-driven design patterns in local AI <em>agents</em> can produce \u201cgod-mode\u201d blast radius when trust is mis-scoped.<p>The write-up focuses on:\n\u2013 how local <em>agents</em> collapse UI + infra trust layers\n\u2013 why \u201cruns locally\u201d doesn\u2019t automatically mean \u201csafe\u201d\n\u2013 how <em>agent</em> autonomy changes the RCE threat model\n\u2013 what defensive patterns might look like for <em>agent</em> platforms<p>Curious how others are thinking about the security model for local <em>autonomous</em> <em>agents</em> and whether we need new mental models beyond traditional sandboxing and token scoping."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Synesthetic Computation"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://medium.com/@FoxEars42/show-hackernews-synesthetic-computation-a-k-a-full-rce-over-openclaw-c756e0c35d69"}},"_tags":["story","author_agamrafaeli","story_47033719","show_hn"],"author":"agamrafaeli","created_at":"2026-02-16T11:20:42Z","created_at_i":1771240842,"num_comments":0,"objectID":"47033719","points":1,"story_id":47033719,"story_text":"&quot;When perception shifts, and the feeling of control takes over&quot;)<p>I wrote up a deep dive into a security issue in OpenClaw that escalates from a seemingly small UX&#x2F;trust boundary problem into full remote code execution via a single malicious link.<p>The article walks through the full exploit chain from a systems perspective rather than just a CVE summary. The key theme is what I call \u201csynesthetic computation\u201d: when subjective context, UI state, agent memory, and system permissions get blended together in ways that feel natural to users but collapse important security boundaries. When an agent is allowed to act across chat, browser, and local tooling, those boundaries become part of the attack surface.<p>In this case, a crafted link can cause a client to connect to an attacker-controlled gateway, leak a token, and then allow that attacker to reconfigure the agent\u2019s execution environment and run arbitrary commands on the host. The interesting part isn\u2019t just the bug\u2014it\u2019s how quickly convenience-driven design patterns in local AI agents can produce \u201cgod-mode\u201d blast radius when trust is mis-scoped.<p>The write-up focuses on:\n\u2013 how local agents collapse UI + infra trust layers\n\u2013 why \u201cruns locally\u201d doesn\u2019t automatically mean \u201csafe\u201d\n\u2013 how agent autonomy changes the RCE threat model\n\u2013 what defensive patterns might look like for agent platforms<p>Curious how others are thinking about the security model for local autonomous agents and whether we need new mental models beyond traditional sandboxing and token scoping.","title":"Show HN: Synesthetic Computation","updated_at":"2026-02-16T11:22:59Z","url":"https://medium.com/@FoxEars42/show-hackernews-synesthetic-computation-a-k-a-full-rce-over-openclaw-c756e0c35d69"}],"hitsPerPage":10,"nbHits":590,"nbPages":59,"page":0,"params":"query=autonomous+agents&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":14,"processingTimingsMS":{"_request":{"roundTrip":19},"afterFetch":{"format":{"highlighting":1,"total":1}},"fetch":{"query":6,"scanning":6,"total":13},"total":14},"query":"autonomous agents","serverTimeMS":16}
