{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"madcash"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"Hey HN \u2013 I built VoteShip, a feature request and voting platform where AI agents are first-class users.<p>The idea: Your coding agent (<em>Claude</em> <em>Code</em>, Cursor, OpenClaw, Codex, etc.) should be able to check what users are asking for, find duplicates, plan what to build next, and write the changelog \u2014 without you ever opening a dashboard. VoteShip makes that possible through MCP and a full REST API.<p>How it works with agents: Install @voteship/mcp-server (22 tools, 5 resources, 4 <em>workflow</em> prompts) and your agent can:\n  - Pull your unreviewed feedback inbox and triage it\n  - Detect duplicate requests via semantic search (pgvector + Voyage embeddings)\n  - Check vote counts and suggest what to build this sprint\n  - Draft a changelog entry after shipping a feature\n  - Create, tag, and update posts programmatically<p>An agent like OpenClaw can autonomously monitor your board, identify trending requests, and open PRs to address them \u2014 closing the loop from &quot;user asks for feature&quot; to &quot;agent ships it&quot; without human intervention. The REST API and webhooks work just as well for custom agent pipelines.<p>Why I built it: I was using Canny and kept hitting walls \u2014 no anonymous voting (their most-requested feature, ironically), AI features paywalled, and per-&quot;tracked-user&quot; pricing that punishes you for having an active community. And none of the incumbents have any agent integration at all.<p>What else it does:\n  - Public voting boards with anonymous voting (no signup required)\n  - Public roadmaps and changelogs\n  - 7 built-in AI features \u2014 duplicate detection, auto-categorization, sentiment analysis, changelog generation\n  - Embeddable widget (Preact + Shadow DOM)\n  - Import from Canny/Nolt/UserVoice in ~2 minutes<p>Pricing: Free / $5 / $15 / $25 per month. Flat pricing, no per-user fees. The $15 plan includes everything Canny charges $359/mo for.<p>Try it: <a href=\"https://voteship.app\" rel=\"nofollow\">https://voteship.app</a><p>MCP server: npx @voteship/mcp-server (npm)<p>Happy to answer questions about the MCP implementation, agent <em>workflows</em>, or anything else."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: VoteShip \u2013 Feature request platform built for AI agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://voteship.app/"}},"_tags":["story","author_madcash","story_47040022","show_hn"],"author":"madcash","created_at":"2026-02-16T20:38:12Z","created_at_i":1771274292,"num_comments":0,"objectID":"47040022","points":1,"story_id":47040022,"story_text":"Hey HN \u2013 I built VoteShip, a feature request and voting platform where AI agents are first-class users.<p>The idea: Your coding agent (Claude Code, Cursor, OpenClaw, Codex, etc.) should be able to check what users are asking for, find duplicates, plan what to build next, and write the changelog \u2014 without you ever opening a dashboard. VoteShip makes that possible through MCP and a full REST API.<p>How it works with agents: Install @voteship&#x2F;mcp-server (22 tools, 5 resources, 4 workflow prompts) and your agent can:\n  - Pull your unreviewed feedback inbox and triage it\n  - Detect duplicate requests via semantic search (pgvector + Voyage embeddings)\n  - Check vote counts and suggest what to build this sprint\n  - Draft a changelog entry after shipping a feature\n  - Create, tag, and update posts programmatically<p>An agent like OpenClaw can autonomously monitor your board, identify trending requests, and open PRs to address them \u2014 closing the loop from &quot;user asks for feature&quot; to &quot;agent ships it&quot; without human intervention. The REST API and webhooks work just as well for custom agent pipelines.<p>Why I built it: I was using Canny and kept hitting walls \u2014 no anonymous voting (their most-requested feature, ironically), AI features paywalled, and per-&quot;tracked-user&quot; pricing that punishes you for having an active community. And none of the incumbents have any agent integration at all.<p>What else it does:\n  - Public voting boards with anonymous voting (no signup required)\n  - Public roadmaps and changelogs\n  - 7 built-in AI features \u2014 duplicate detection, auto-categorization, sentiment analysis, changelog generation\n  - Embeddable widget (Preact + Shadow DOM)\n  - Import from Canny&#x2F;Nolt&#x2F;UserVoice in ~2 minutes<p>Pricing: Free &#x2F; $5 &#x2F; $15 &#x2F; $25 per month. Flat pricing, no per-user fees. The $15 plan includes everything Canny charges $359&#x2F;mo for.<p>Try it: <a href=\"https:&#x2F;&#x2F;voteship.app\" rel=\"nofollow\">https:&#x2F;&#x2F;voteship.app</a><p>MCP server: npx @voteship&#x2F;mcp-server (npm)<p>Happy to answer questions about the MCP implementation, agent workflows, or anything else.","title":"Show HN: VoteShip \u2013 Feature request platform built for AI agents","updated_at":"2026-02-16T20:43:31Z","url":"https://voteship.app/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"hcwilk"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"I've been using <em>Claude</em> <em>Code</em> ever since it came out, and I've seen good success with it. However, Matt Shumer's recent article [1] included a piece I'm curious about:<p>When describing his <em>workflow</em> now, he says &quot;Then, and this is the part that would have been unthinkable a year ago, it opens the app itself. It clicks through the buttons. It tests the features. It uses the app the way a person would.&quot;<p>Unless I missed a major development, do these coding agents have the ability to use feedback loops like this? If so, how have you all built this functionality into your <em>workflow</em>?<p>Any guidance / advice / documentation I can read would be super helpful!<p>[1]: https://x.com/mattshumer_/status/2021256989876109403"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: Feedback Loop with Coding Agents?"}},"_tags":["story","author_hcwilk","story_47038200","ask_hn"],"author":"hcwilk","created_at":"2026-02-16T18:11:54Z","created_at_i":1771265514,"num_comments":0,"objectID":"47038200","points":1,"story_id":47038200,"story_text":"I&#x27;ve been using Claude Code ever since it came out, and I&#x27;ve seen good success with it. However, Matt Shumer&#x27;s recent article [1] included a piece I&#x27;m curious about:<p>When describing his workflow now, he says &quot;Then, and this is the part that would have been unthinkable a year ago, it opens the app itself. It clicks through the buttons. It tests the features. It uses the app the way a person would.&quot;<p>Unless I missed a major development, do these coding agents have the ability to use feedback loops like this? If so, how have you all built this functionality into your workflow?<p>Any guidance &#x2F; advice &#x2F; documentation I can read would be super helpful!<p>[1]: https:&#x2F;&#x2F;x.com&#x2F;mattshumer_&#x2F;status&#x2F;2021256989876109403","title":"Ask HN: Feedback Loop with Coding Agents?","updated_at":"2026-02-16T18:17:00Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"laurex"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"I Tried New <em>Claude</em> <em>Code</em> Ollama <em>Workflow</em> (It's Wild and Free)"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"https://medium.com/@joe.njenga/i-tried-new-<em>claude</em>-<em>code</em>-ollama-<em>workflow</em>-its-wild-free-cb7a12b733b5"}},"_tags":["story","author_laurex","story_47037956"],"author":"laurex","created_at":"2026-02-16T17:50:41Z","created_at_i":1771264241,"num_comments":0,"objectID":"47037956","points":1,"story_id":47037956,"title":"I Tried New Claude Code Ollama Workflow (It's Wild and Free)","updated_at":"2026-02-16T17:56:16Z","url":"https://medium.com/@joe.njenga/i-tried-new-claude-code-ollama-workflow-its-wild-free-cb7a12b733b5"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"0xConstantine"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude","code"],"value":"MCP server + embedded terminal that gives <em>Claude</em> <em>Code</em> direct access to ComfyUI's workflow graph. It can search available nodes, create/connect/delete them, set values, run the queue, and see image outputs. The workflow is basically a JSON DAG, so each edit maps cleanly to a tool call."},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"Show HN: Comfy Pilot \u2013 MCP server that lets <em>Claude</em> <em>Code</em> edit ComfyUI <em>workflows</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/ConstantineB6/comfy-pilot"}},"_tags":["story","author_0xConstantine","story_47037267","show_hn"],"author":"0xConstantine","created_at":"2026-02-16T16:51:51Z","created_at_i":1771260711,"num_comments":0,"objectID":"47037267","points":2,"story_id":47037267,"story_text":"MCP server + embedded terminal that gives Claude Code direct access to ComfyUI&#x27;s workflow graph. It can search available nodes, create&#x2F;connect&#x2F;delete them, set values, run the queue, and see image outputs. The workflow is basically a JSON DAG, so each edit maps cleanly to a tool call.","title":"Show HN: Comfy Pilot \u2013 MCP server that lets Claude Code edit ComfyUI workflows","updated_at":"2026-02-16T16:59:00Z","url":"https://github.com/ConstantineB6/comfy-pilot"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Reebz"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"Hi HN, this is nothing fancy, but a tool I built for myself as a minimalist way to track usage. Also (and probably more importantly), colleagues who are marketers, writers, designers, and other non-engineering backgrounds who are/becoming power users of <em>Claude</em> Cowork or <em>Claude</em> <em>Code</em> and needed to keep better watch of usage.<p>Once Opus 4.6 landed, I was quickly aware I needed to keep an eye on usage as I capped out session limits faster than ever. I got frustrated checking usage manually and then explored the other apps and widgets out there. They're really good, and some are great, but they're just so feature-heavy and complex with that ICP in mind I mentioned earlier.<p>Tokens just don't feel like anything to me.... I'd watch them tick over in the thousands and then millions. So I just ignored them as I was planning and then inverted the usage metrics that <em>Claude</em> provides (i.e. start at 100% and not 0%), which helped me land on a battery concept. This felt good and definitely made sense to the people I asked. Then I was focused on adding only the bare minimum features... or &quot;Simplify, and add lightness&quot; in the words of Colin Chapman.<p>Anyway, that's the story and yes, the app is largely vibe coded before folks start to go digging through commits. I quite enjoyed the process and I wouldn't have hand-coded something like this myself (the issue-pain wouldn't have met the effort-required threshold). If anyone is curious, my <em>workflow</em> and tools used were <em>Claude</em> <em>Code</em>, with ui-ux-pro-max for design, heavy usage of compound-engineering (plan &gt; work &gt; review &gt; compound). Strongly recommend this plugin. I also used Condcutor on and off, but 80% of the work ended up just being done with CC in iTerm2. It handled the agent teams much better.<p>Let me know if you have any bugs or feedback."},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude"],"value":"Show HN: <em>Claude</em> Battery \u2013 usage at a glance. A minimalist macOS menu bar widget"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude"],"value":"https://github.com/Reebz/<em>claude</em>-battery"}},"_tags":["story","author_Reebz","story_47035304","show_hn"],"author":"Reebz","children":[47035356,47035387],"created_at":"2026-02-16T14:21:45Z","created_at_i":1771251705,"num_comments":4,"objectID":"47035304","points":1,"story_id":47035304,"story_text":"Hi HN, this is nothing fancy, but a tool I built for myself as a minimalist way to track usage. Also (and probably more importantly), colleagues who are marketers, writers, designers, and other non-engineering backgrounds who are&#x2F;becoming power users of Claude Cowork or Claude Code and needed to keep better watch of usage.<p>Once Opus 4.6 landed, I was quickly aware I needed to keep an eye on usage as I capped out session limits faster than ever. I got frustrated checking usage manually and then explored the other apps and widgets out there. They&#x27;re really good, and some are great, but they&#x27;re just so feature-heavy and complex with that ICP in mind I mentioned earlier.<p>Tokens just don&#x27;t feel like anything to me.... I&#x27;d watch them tick over in the thousands and then millions. So I just ignored them as I was planning and then inverted the usage metrics that Claude provides (i.e. start at 100% and not 0%), which helped me land on a battery concept. This felt good and definitely made sense to the people I asked. Then I was focused on adding only the bare minimum features... or &quot;Simplify, and add lightness&quot; in the words of Colin Chapman.<p>Anyway, that&#x27;s the story and yes, the app is largely vibe coded before folks start to go digging through commits. I quite enjoyed the process and I wouldn&#x27;t have hand-coded something like this myself (the issue-pain wouldn&#x27;t have met the effort-required threshold). If anyone is curious, my workflow and tools used were Claude Code, with ui-ux-pro-max for design, heavy usage of compound-engineering (plan &gt; work &gt; review &gt; compound). Strongly recommend this plugin. I also used Condcutor on and off, but 80% of the work ended up just being done with CC in iTerm2. It handled the agent teams much better.<p>Let me know if you have any bugs or feedback.","title":"Show HN: Claude Battery \u2013 usage at a glance. A minimalist macOS menu bar widget","updated_at":"2026-02-16T15:00:31Z","url":"https://github.com/Reebz/claude-battery"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"jorgtron"},"story_text":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude","workflows"],"value":"The problem: When vibe coding web apps and pages, I noticed I spent a lot of time giving <em>Claude</em> context about the UI changes I want to make.<p>For example, I would have to write things like &quot;change the text below the login form at localhost:3000/dashboard to say X instead of Y, but only on the mobile version&quot; to keep it from changing the wrong thing in the wrong way.<p>So I've built Chisel for <em>Claude</em> to solve this, and it works!<p>Now I just select the element I want to change using the Chisel Chrome  extension, say the thing I want to change: &quot;change this to Y&quot; and say a &quot;send phrase&quot; (I use &quot;Chisel&quot;, but you can choose your own) and it will send the instruction to <em>Claude</em> and start making the change completely hands free.<p>For me it's sped up my <em>workflow</em> by at least 2X. It let's me work primarily in the browser and remain in a creative flow state.<p>More benefits:<p>- 20+ languages supported\n- Start recording on clicking elements (optional)\n- Choose your own send and cancel phrases<p>Any feedback is welcome :)<p>Happy Chiseling!"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude","code"],"value":"Show HN: Chisel for <em>Claude</em>. Vibe <em>code</em> 2X faster using your voice"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude"],"value":"https://jorgtron.github.io/chisel-for-<em>claude</em>/"}},"_tags":["story","author_jorgtron","story_47033536","show_hn"],"author":"jorgtron","created_at":"2026-02-16T10:53:50Z","created_at_i":1771239230,"num_comments":0,"objectID":"47033536","points":1,"story_id":47033536,"story_text":"The problem: When vibe coding web apps and pages, I noticed I spent a lot of time giving Claude context about the UI changes I want to make.<p>For example, I would have to write things like &quot;change the text below the login form at localhost:3000&#x2F;dashboard to say X instead of Y, but only on the mobile version&quot; to keep it from changing the wrong thing in the wrong way.<p>So I&#x27;ve built Chisel for Claude to solve this, and it works!<p>Now I just select the element I want to change using the Chisel Chrome  extension, say the thing I want to change: &quot;change this to Y&quot; and say a &quot;send phrase&quot; (I use &quot;Chisel&quot;, but you can choose your own) and it will send the instruction to Claude and start making the change completely hands free.<p>For me it&#x27;s sped up my workflow by at least 2X. It let&#x27;s me work primarily in the browser and remain in a creative flow state.<p>More benefits:<p>- 20+ languages supported\n- Start recording on clicking elements (optional)\n- Choose your own send and cancel phrases<p>Any feedback is welcome :)<p>Happy Chiseling!","title":"Show HN: Chisel for Claude. Vibe code 2X faster using your voice","updated_at":"2026-02-16T10:56:59Z","url":"https://jorgtron.github.io/chisel-for-claude/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"tanmay001"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"AI agents often generate inconsistent Playwright tests because they do not understand your application\u2019s specific <em>workflows</em>, UI patterns, or constraints.<p>The Playwright Skill provides more than 70 structured markdown guides that teach patterns for locators, authentication, visual testing, CI configuration, and framework migration so agents can apply consistent solutions instead of guessing.<p>You install it with: npx skills add testdino-hq/playwright-skill.<p>The material is organized into five packs: core testing (46 guides), Playwright CLI usage for token\u2011efficient automation (10), Page Object Model patterns (2), CI/CD setup for major providers (9), and migrations from Cypress or Selenium (2).<p>Each guide follows the same structure\u2014when to use a pattern, when to avoid it, quick reference <em>code</em>, and complete implementations\u2014so learners can move from concept to reliable tests step by step.<p>The skill works with tools such as <em>Claude</em> <em>Code</em>, GitHub Copilot, Cursor, and any agent that implements the skills protocol, and it is MIT\u2011licensed so teams can adapt the content to their own standards and practices.<p>For a deeper walkthrough of the guides and structure, see the full article at <a href=\"https://testdino.com/blog/playwright-skill/\" rel=\"nofollow\">https://testdino.com/blog/playwright-skill/</a>."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Train AI Agents to Write Better Playwright Tests"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://testdino.com/blog/playwright-skill/"}},"_tags":["story","author_tanmay001","story_47032774","show_hn"],"author":"tanmay001","created_at":"2026-02-16T09:18:10Z","created_at_i":1771233490,"num_comments":0,"objectID":"47032774","points":2,"story_id":47032774,"story_text":"AI agents often generate inconsistent Playwright tests because they do not understand your application\u2019s specific workflows, UI patterns, or constraints.<p>The Playwright Skill provides more than 70 structured markdown guides that teach patterns for locators, authentication, visual testing, CI configuration, and framework migration so agents can apply consistent solutions instead of guessing.<p>You install it with: npx skills add testdino-hq&#x2F;playwright-skill.<p>The material is organized into five packs: core testing (46 guides), Playwright CLI usage for token\u2011efficient automation (10), Page Object Model patterns (2), CI&#x2F;CD setup for major providers (9), and migrations from Cypress or Selenium (2).<p>Each guide follows the same structure\u2014when to use a pattern, when to avoid it, quick reference code, and complete implementations\u2014so learners can move from concept to reliable tests step by step.<p>The skill works with tools such as Claude Code, GitHub Copilot, Cursor, and any agent that implements the skills protocol, and it is MIT\u2011licensed so teams can adapt the content to their own standards and practices.<p>For a deeper walkthrough of the guides and structure, see the full article at <a href=\"https:&#x2F;&#x2F;testdino.com&#x2F;blog&#x2F;playwright-skill&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;testdino.com&#x2F;blog&#x2F;playwright-skill&#x2F;</a>.","title":"Show HN: Train AI Agents to Write Better Playwright Tests","updated_at":"2026-02-16T09:24:28Z","url":"https://testdino.com/blog/playwright-skill/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"server-lab"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"I built DoScript, a domain-specific language for file automation. The goal: make scripts readable by anyone.\nDesign Goal\nInstead of:\nbashfind . -type f -mtime +30 -exec rm {} \\;\nWrite:\nfor_each file_in here\n    if_older_than {file_name} 30 days\n        delete file {file_path}\n    end_if\nend_for\nTrade power for clarity. Optimize for maintenance over terseness.\nKey Design Decisions\n1. Natural Language Keywords\nmake folder not mkdir, copy file not cp. Self-documenting.\n2. Implicit Metadata\nWhen iterating files, auto-inject: {file_name}, {file_path}, {file_size}, {file_modified}, {file_is_old_days}\nfor_each file_in &quot;Documents&quot;\n    say &quot;{file_name} is {file_size} bytes&quot;\nend_for\n3. Built-in Time Handling\nif_older_than {file_name} 30 days\nmake folder &quot;backup_{today}&quot;\nNo date arithmetic needed.\n4. Expression Evaluation\nFunction-based for simplicity:\nif greater_than {file_size} 1000000\nif and(equals({type}, &quot;pdf&quot;), greater_than({size}, 10000))\nIntentionally awkward for complex logic - signals you should use Python.\nImplementation<p>Python interpreter (~2000 LOC)\nRecursive descent parser\nContext-aware error reporting\nCustom exception types with file/line info<p>Visual Component\nBuilt a browser-based node editor (single HTML file, 1200 LOC). Drag boxes, wire them, generate DoScript <em>code</em>.\nWhy? Different learning styles, <em>workflow</em> visualization, non-programmer accessibility.\nWhat Worked<p>Natural syntax is immediately understandable\nMetadata injection removes boilerplate\nTime handling makes common cases trivial\nVisual IDE differentiates from text-only<p>What Didn't<p>Complex conditionals get awkward fast\nNo user-defined functions (only macros)\nLimited data structures\nPerformance not optimized<p>The Challenge\nBuilt for non-programmers. But they don't hang out on dev forums. Developers say &quot;just use Python&quot; - which misses the point.\nHow do you market dev tools to non-developers?\nTechnical Transparency\nI designed syntax and architecture. Most Python implementation was AI-assisted (<em>Claude</em>, Copilot). Focus on design, use tools for implementation.\nOpen Questions<p>When does a DSL become too limited?\nHow to market to non-developers?\nType system worth the complexity?\nShould DSLs provide escape hatches to host language?<p>GitHub: <a href=\"https://github.com/TheServer-lab/DoScript\" rel=\"nofollow\">https://github.com/TheServer-lab/DoScript</a>\nv0.6.5, includes interpreter, visual IDE, VS <em>Code</em> extension, examples.\nBuilt because bash was too cryptic for my friend to organize files. Turns out lots of people have this problem.\nWould love feedback from people who've built DSLs or struggled with similar trade-offs."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: DoScript \u2013 DSL for file automation with natural language syntax"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/TheServer-lab/DoScript"}},"_tags":["story","author_server-lab","story_47032476","show_hn"],"author":"server-lab","created_at":"2026-02-16T08:37:57Z","created_at_i":1771231077,"num_comments":0,"objectID":"47032476","points":1,"story_id":47032476,"story_text":"I built DoScript, a domain-specific language for file automation. The goal: make scripts readable by anyone.\nDesign Goal\nInstead of:\nbashfind . -type f -mtime +30 -exec rm {} \\;\nWrite:\nfor_each file_in here\n    if_older_than {file_name} 30 days\n        delete file {file_path}\n    end_if\nend_for\nTrade power for clarity. Optimize for maintenance over terseness.\nKey Design Decisions\n1. Natural Language Keywords\nmake folder not mkdir, copy file not cp. Self-documenting.\n2. Implicit Metadata\nWhen iterating files, auto-inject: {file_name}, {file_path}, {file_size}, {file_modified}, {file_is_old_days}\nfor_each file_in &quot;Documents&quot;\n    say &quot;{file_name} is {file_size} bytes&quot;\nend_for\n3. Built-in Time Handling\nif_older_than {file_name} 30 days\nmake folder &quot;backup_{today}&quot;\nNo date arithmetic needed.\n4. Expression Evaluation\nFunction-based for simplicity:\nif greater_than {file_size} 1000000\nif and(equals({type}, &quot;pdf&quot;), greater_than({size}, 10000))\nIntentionally awkward for complex logic - signals you should use Python.\nImplementation<p>Python interpreter (~2000 LOC)\nRecursive descent parser\nContext-aware error reporting\nCustom exception types with file&#x2F;line info<p>Visual Component\nBuilt a browser-based node editor (single HTML file, 1200 LOC). Drag boxes, wire them, generate DoScript code.\nWhy? Different learning styles, workflow visualization, non-programmer accessibility.\nWhat Worked<p>Natural syntax is immediately understandable\nMetadata injection removes boilerplate\nTime handling makes common cases trivial\nVisual IDE differentiates from text-only<p>What Didn&#x27;t<p>Complex conditionals get awkward fast\nNo user-defined functions (only macros)\nLimited data structures\nPerformance not optimized<p>The Challenge\nBuilt for non-programmers. But they don&#x27;t hang out on dev forums. Developers say &quot;just use Python&quot; - which misses the point.\nHow do you market dev tools to non-developers?\nTechnical Transparency\nI designed syntax and architecture. Most Python implementation was AI-assisted (Claude, Copilot). Focus on design, use tools for implementation.\nOpen Questions<p>When does a DSL become too limited?\nHow to market to non-developers?\nType system worth the complexity?\nShould DSLs provide escape hatches to host language?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;TheServer-lab&#x2F;DoScript\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;TheServer-lab&#x2F;DoScript</a>\nv0.6.5, includes interpreter, visual IDE, VS Code extension, examples.\nBuilt because bash was too cryptic for my friend to organize files. Turns out lots of people have this problem.\nWould love feedback from people who&#x27;ve built DSLs or struggled with similar trade-offs.","title":"Show HN: DoScript \u2013 DSL for file automation with natural language syntax","updated_at":"2026-02-16T08:42:59Z","url":"https://github.com/TheServer-lab/DoScript"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"alexhans"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"I've been working on a site [1] to give people control of their LLM <em>workflows</em> through AI evals - automated checks that, once defined, let you move fast without regressions and cut through hype with proof.<p>That one-liner is aimed at software engineers, but I've spent my career helping cross-functional teams collaborate, and that's really what this is about. AI agents make powerful <em>workflows</em> very plausible, but only if teams can grow them incrementally without losing control - no vendor lock-in, no discipline silos, no blind trust in outputs.<p>The site tries to meet different audiences where they are, with mostly practice over theory: tool comparisons, minimal approaches, and freedom to work at whatever level of complexity serves you - whether that's <em>Claude</em> <em>Code</em> with Agent Skills, local models, or custom Python agents.<p>As a fun &quot;eat your own dog food&quot; experiment, I use the site itself as the reproducible cookbook (&quot;eval-ception&quot;) [2]. It's the quickest way to feel what different eval tools are actually like in practice.<p>I welcome feedback, contributions, or stories. More on the project and what's coming [3]. It's a rewarding area once you realize you can keep control and move methodically - doesn't matter if it's the smallest model or a swarm.<p>[1] <a href=\"https://ai-evals.io/\" rel=\"nofollow\">https://ai-evals.io/</a><p>[2] <a href=\"https://ai-evals.io/cookbook/eval-ception.html\" rel=\"nofollow\">https://ai-evals.io/cookbook/eval-ception.html</a><p>[3] <a href=\"https://ai-evals.io/about/\" rel=\"nofollow\">https://ai-evals.io/about/</a>"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: AI-Evals.io \u2013 Evaluate this site with the tools it reviews"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://ai-evals.io/"}},"_tags":["story","author_alexhans","story_47026263","show_hn"],"author":"alexhans","children":[47043322],"created_at":"2026-02-15T18:49:49Z","created_at_i":1771181389,"num_comments":0,"objectID":"47026263","points":4,"story_id":47026263,"story_text":"I&#x27;ve been working on a site [1] to give people control of their LLM workflows through AI evals - automated checks that, once defined, let you move fast without regressions and cut through hype with proof.<p>That one-liner is aimed at software engineers, but I&#x27;ve spent my career helping cross-functional teams collaborate, and that&#x27;s really what this is about. AI agents make powerful workflows very plausible, but only if teams can grow them incrementally without losing control - no vendor lock-in, no discipline silos, no blind trust in outputs.<p>The site tries to meet different audiences where they are, with mostly practice over theory: tool comparisons, minimal approaches, and freedom to work at whatever level of complexity serves you - whether that&#x27;s Claude Code with Agent Skills, local models, or custom Python agents.<p>As a fun &quot;eat your own dog food&quot; experiment, I use the site itself as the reproducible cookbook (&quot;eval-ception&quot;) [2]. It&#x27;s the quickest way to feel what different eval tools are actually like in practice.<p>I welcome feedback, contributions, or stories. More on the project and what&#x27;s coming [3]. It&#x27;s a rewarding area once you realize you can keep control and move methodically - doesn&#x27;t matter if it&#x27;s the smallest model or a swarm.<p>[1] <a href=\"https:&#x2F;&#x2F;ai-evals.io&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;ai-evals.io&#x2F;</a><p>[2] <a href=\"https:&#x2F;&#x2F;ai-evals.io&#x2F;cookbook&#x2F;eval-ception.html\" rel=\"nofollow\">https:&#x2F;&#x2F;ai-evals.io&#x2F;cookbook&#x2F;eval-ception.html</a><p>[3] <a href=\"https:&#x2F;&#x2F;ai-evals.io&#x2F;about&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;ai-evals.io&#x2F;about&#x2F;</a>","title":"Show HN: AI-Evals.io \u2013 Evaluate this site with the tools it reviews","updated_at":"2026-02-17T03:20:46Z","url":"https://ai-evals.io/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"with_geun"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code","workflows"],"value":"Hi HN \u2014 I built alive-analysis, an open-source <em>workflow</em> kit that turns AI coding agents into structured data analysis partners.<p>Problem: AI-assisted analyses are often throwaway chats. A month later, you can\u2019t trace why you reached a conclusion.<p>Solution: It enforces a 5-stage loop (ASK \u2192 LOOK \u2192 INVESTIGATE \u2192 VOICE \u2192 EVOLVE) with checklists, and saves analyses as Git-tracked markdown files.\nQuick mode: 1 file. Full mode: 5 files.<p>Works in <em>Claude</em> <em>Code</em> and Cursor.<p>I\u2019d love feedback on:<p>1. Does the ALIVE loop match how you do investigations / experiment reviews?<p>2. Which checklist items feel missing or unnecessary?<p>3. What would make this usable in a team setting?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: Alive-analysis \u2013 Git-tracked analysis notes for AI agents"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/with-geun/alive-analysis"}},"_tags":["story","author_with_geun","story_47025175","show_hn"],"author":"with_geun","children":[47025179],"created_at":"2026-02-15T16:49:45Z","created_at_i":1771174185,"num_comments":1,"objectID":"47025175","points":1,"story_id":47025175,"story_text":"Hi HN \u2014 I built alive-analysis, an open-source workflow kit that turns AI coding agents into structured data analysis partners.<p>Problem: AI-assisted analyses are often throwaway chats. A month later, you can\u2019t trace why you reached a conclusion.<p>Solution: It enforces a 5-stage loop (ASK \u2192 LOOK \u2192 INVESTIGATE \u2192 VOICE \u2192 EVOLVE) with checklists, and saves analyses as Git-tracked markdown files.\nQuick mode: 1 file. Full mode: 5 files.<p>Works in Claude Code and Cursor.<p>I\u2019d love feedback on:<p>1. Does the ALIVE loop match how you do investigations &#x2F; experiment reviews?<p>2. Which checklist items feel missing or unnecessary?<p>3. What would make this usable in a team setting?","title":"Show HN: Alive-analysis \u2013 Git-tracked analysis notes for AI agents","updated_at":"2026-02-15T16:52:56Z","url":"https://github.com/with-geun/alive-analysis"}],"hitsPerPage":10,"nbHits":447,"nbPages":45,"page":0,"params":"query=Claude+Code+workflows&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":18,"processingTimingsMS":{"_request":{"queue":1,"roundTrip":20},"afterFetch":{"format":{"total":1}},"fetch":{"query":8,"scanning":8,"total":17},"total":18},"query":"Claude Code workflows","serverTimeMS":20}
