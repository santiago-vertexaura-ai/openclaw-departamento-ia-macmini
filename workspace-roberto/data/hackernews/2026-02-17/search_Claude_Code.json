{"exhaustive":{"nbHits":false,"typo":false},"exhaustiveNbHits":false,"exhaustiveTypo":false,"hits":[{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"everlier"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Hi<p>Agentic coding is rapidly changing our ways of developing software. Not everyone can afford a subscription, though, but they shouldn't be excluded from the process of learning these new tools.<p>Just wanted to share a few tips on running near-frontier agentic coding setup almost for free.<p>1. APIs. Most of the agentic coding tools use two types of APIs - OpenAI or Anthropic compatible. OpenAI is must more common, but Anthropic is associated with <em>Claude</em> <em>Code</em> ecosystem. There are also OSS adapters to convert between the two as needed. Essentially, you need to find providers that serve inference for free.<p>1. OpenRouter. They always have a few models that are completely free at the expense of storing and using everything you send to them. There are frequent promotional periods after new model releases. You need to top up your account by ~$10, though, to avoid rate limits as they are applied based on your balance. After that, ensure to use Model IDs with `:free` postfix and your balance will not be consumed, you can use those indefinitely.<p>2. OpenCode. This is a great agentic harness (albeit its heavily tuned for larger models), its parent company also provides inference APIs. Due to the popularity, many LLM providers offer free tiers of the models there. Same caveat applies - you data will be stored and used.<p>3. Local inference. If you happened to have a ~6-8GB VRAM and ~32GB RAM - then you should be able to run staple ~30B-sized MoE models. GLM-4.7-Flash is currently the best one for using inside a harness, it's even capable enough to drive simple tasks in OpenCode, but I recommend simpler harnesses for better results.<p>4. What to expect. Most of these offerings come with a compromise in terms of data collection and/or inference quality. For example, OpenCode's free Kimi 2.5 is clearly different from the paid one from official provider. In general - do not trust any claims that compare smaller open weight models with the cloud offering, they are not there yet. However you can get really far and models like Kimi 2.5 are still very capable.<p>Thanks!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Tell HN: Tips for (mostly) free agentic coding setup"}},"_tags":["story","author_everlier","story_47046601","ask_hn"],"author":"everlier","children":[47046611],"created_at":"2026-02-17T12:05:12Z","created_at_i":1771329912,"num_comments":1,"objectID":"47046601","points":1,"story_id":47046601,"story_text":"Hi<p>Agentic coding is rapidly changing our ways of developing software. Not everyone can afford a subscription, though, but they shouldn&#x27;t be excluded from the process of learning these new tools.<p>Just wanted to share a few tips on running near-frontier agentic coding setup almost for free.<p>1. APIs. Most of the agentic coding tools use two types of APIs - OpenAI or Anthropic compatible. OpenAI is must more common, but Anthropic is associated with Claude Code ecosystem. There are also OSS adapters to convert between the two as needed. Essentially, you need to find providers that serve inference for free.<p>1. OpenRouter. They always have a few models that are completely free at the expense of storing and using everything you send to them. There are frequent promotional periods after new model releases. You need to top up your account by ~$10, though, to avoid rate limits as they are applied based on your balance. After that, ensure to use Model IDs with `:free` postfix and your balance will not be consumed, you can use those indefinitely.<p>2. OpenCode. This is a great agentic harness (albeit its heavily tuned for larger models), its parent company also provides inference APIs. Due to the popularity, many LLM providers offer free tiers of the models there. Same caveat applies - you data will be stored and used.<p>3. Local inference. If you happened to have a ~6-8GB VRAM and ~32GB RAM - then you should be able to run staple ~30B-sized MoE models. GLM-4.7-Flash is currently the best one for using inside a harness, it&#x27;s even capable enough to drive simple tasks in OpenCode, but I recommend simpler harnesses for better results.<p>4. What to expect. Most of these offerings come with a compromise in terms of data collection and&#x2F;or inference quality. For example, OpenCode&#x27;s free Kimi 2.5 is clearly different from the paid one from official provider. In general - do not trust any claims that compare smaller open weight models with the cloud offering, they are not there yet. However you can get really far and models like Kimi 2.5 are still very capable.<p>Thanks!","title":"Tell HN: Tips for (mostly) free agentic coding setup","updated_at":"2026-02-17T12:07:32Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"ath_ray"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"<em>Code</em>x CLI vs. <em>Claude</em> <em>Code</em> on Autonomy"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"https://blog.nilenso.com/blog/2026/02/12/<em>code</em>x-cli-vs-<em>claude</em>-<em>code</em>-on-autonomy/"}},"_tags":["story","author_ath_ray","story_47046580"],"author":"ath_ray","created_at":"2026-02-17T12:01:04Z","created_at_i":1771329664,"num_comments":0,"objectID":"47046580","points":1,"story_id":47046580,"title":"Codex CLI vs. Claude Code on Autonomy","updated_at":"2026-02-17T12:02:17Z","url":"https://blog.nilenso.com/blog/2026/02/12/codex-cli-vs-claude-code-on-autonomy/"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"LowResBudget"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Hi. Poor developer here.<p>I'm trying to learn AI coding (already have multiple years experience with &quot;normal&quot; programming in various languages.) I want to know how to make my budget (about $30/month) go furthest.<p>At the moment, I am using:<p>Z.ai $6/month plan:<p>Ok model (GLM 4.7) It seems to rate limit/throttle aggressively if I use it a lot.<p>and<p>Github copilot $10/month plan:<p>Seems to reduce model context to 100k tokens, and only offers unlimited access to smaller model (GPT5-mini, Grok <em>Code</em> Fast 1 etc). These models are ok for making precise edits to specific <em>code</em>, but they seem to get stuck when the program is large and has a lot of concurrency etc.<p>I also have free plans for web/mobile-chat for every model I can find.<p>I only have older computers, so editors like Cursor or Antigravity are too slow to be usable. So I prefer something that can work with a CLI (opencode preferably).<p>Do I already have the best deal? Or is there something I am missing. When I try to compare plans, it is confusing and they are not often clear about actual usage limits.<p>Are Codex or <em>Claude</em> even options at this price point if I want to <em>code</em> for multiple hours per day?"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: What is the best bang for buck budget AI coding?"}},"_tags":["story","author_LowResBudget","story_47046139","ask_hn"],"author":"LowResBudget","children":[47046370,47046162],"created_at":"2026-02-17T11:08:37Z","created_at_i":1771326517,"num_comments":2,"objectID":"47046139","points":1,"story_id":47046139,"story_text":"Hi. Poor developer here.<p>I&#x27;m trying to learn AI coding (already have multiple years experience with &quot;normal&quot; programming in various languages.) I want to know how to make my budget (about $30&#x2F;month) go furthest.<p>At the moment, I am using:<p>Z.ai $6&#x2F;month plan:<p>Ok model (GLM 4.7) It seems to rate limit&#x2F;throttle aggressively if I use it a lot.<p>and<p>Github copilot $10&#x2F;month plan:<p>Seems to reduce model context to 100k tokens, and only offers unlimited access to smaller model (GPT5-mini, Grok Code Fast 1 etc). These models are ok for making precise edits to specific code, but they seem to get stuck when the program is large and has a lot of concurrency etc.<p>I also have free plans for web&#x2F;mobile-chat for every model I can find.<p>I only have older computers, so editors like Cursor or Antigravity are too slow to be usable. So I prefer something that can work with a CLI (opencode preferably).<p>Do I already have the best deal? Or is there something I am missing. When I try to compare plans, it is confusing and they are not often clear about actual usage limits.<p>Are Codex or Claude even options at this price point if I want to code for multiple hours per day?","title":"Ask HN: What is the best bang for buck budget AI coding?","updated_at":"2026-02-17T11:34:33Z"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"localforthewin"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Built because AI coding assistants burn massive context window reading entire files to answer structural questions.<p>mcp-<em>code</em>base-index parses your <em>code</em>base into functions, classes, imports, and dependency graphs, then exposes 17 query tools via MCP.<p>Measured results: 58-99% token reduction per query (87% average). In multi-turn conversations, 97%+ cumulative savings.<p>Zero dependencies (stdlib ast + regex). Works with <em>Claude</em> <em>Code</em>, Cursor, and any MCP client.<p><pre><code>  pip install &quot;mcp-<em>code</em>base-index[mcp]&quot;</code></pre>"},"title":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["code"],"value":"Show HN: MCP <em>Code</em>base Index \u2013 87% fewer tokens when AI navigates your <em>code</em>base"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["code"],"value":"https://github.com/MikeRecognex/mcp-<em>code</em>base-index"}},"_tags":["story","author_localforthewin","story_47045572","show_hn"],"author":"localforthewin","children":[47045585],"created_at":"2026-02-17T09:56:07Z","created_at_i":1771322167,"num_comments":0,"objectID":"47045572","points":1,"story_id":47045572,"story_text":"Built because AI coding assistants burn massive context window reading entire files to answer structural questions.<p>mcp-codebase-index parses your codebase into functions, classes, imports, and dependency graphs, then exposes 17 query tools via MCP.<p>Measured results: 58-99% token reduction per query (87% average). In multi-turn conversations, 97%+ cumulative savings.<p>Zero dependencies (stdlib ast + regex). Works with Claude Code, Cursor, and any MCP client.<p><pre><code>  pip install &quot;mcp-codebase-index[mcp]&quot;</code></pre>","title":"Show HN: MCP Codebase Index \u2013 87% fewer tokens when AI navigates your codebase","updated_at":"2026-02-17T10:14:02Z","url":"https://github.com/MikeRecognex/mcp-codebase-index"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"jeffchoi"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"I built an MCP server that lets AI assistants (<em>Claude</em>, Cursor, etc.) query multiple databases through a single, unified interface.<p>While using <em>Claude</em> <em>Code</em>, I found it painful to manage separate connections for MySQL, MongoDB, and AWS Athena. So I\nbuilt a server that provides one consistent set of tools (query, list_collections, describe_collection, etc.) that\nwork the same way across all supported databases.<p>Key features:\n- Read-only by default \u2013 Write access requires explicit opt-in, so you won't accidentally mutate production data\n- Multiple simultaneous connections \u2013 Tag them as PROD, STAGING, ANALYTICS, etc. and manage them all at once\n- Extensible \u2013 Add new database connectors by implementing the McpConnector interface<p>Built with TypeScript. Supports MySQL 5.7+, MongoDB 4.4+, and AWS Athena.<p>This is an open-source project \u2013 feedback, issues, and PRs are all welcome. If you try it out and have any\nsuggestions or ideas for improvement, please feel free to share!"},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: MCP Storage Map \u2013 One MCP Server for MySQL, MongoDB, and Athena"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/cyhoon/mcp-storage-map"}},"_tags":["story","author_jeffchoi","story_47045460","show_hn"],"author":"jeffchoi","created_at":"2026-02-17T09:39:10Z","created_at_i":1771321150,"num_comments":0,"objectID":"47045460","points":1,"story_id":47045460,"story_text":"I built an MCP server that lets AI assistants (Claude, Cursor, etc.) query multiple databases through a single, unified interface.<p>While using Claude Code, I found it painful to manage separate connections for MySQL, MongoDB, and AWS Athena. So I\nbuilt a server that provides one consistent set of tools (query, list_collections, describe_collection, etc.) that\nwork the same way across all supported databases.<p>Key features:\n- Read-only by default \u2013 Write access requires explicit opt-in, so you won&#x27;t accidentally mutate production data\n- Multiple simultaneous connections \u2013 Tag them as PROD, STAGING, ANALYTICS, etc. and manage them all at once\n- Extensible \u2013 Add new database connectors by implementing the McpConnector interface<p>Built with TypeScript. Supports MySQL 5.7+, MongoDB 4.4+, and AWS Athena.<p>This is an open-source project \u2013 feedback, issues, and PRs are all welcome. If you try it out and have any\nsuggestions or ideas for improvement, please feel free to share!","title":"Show HN: MCP Storage Map \u2013 One MCP Server for MySQL, MongoDB, and Athena","updated_at":"2026-02-17T09:40:32Z","url":"https://github.com/cyhoon/mcp-storage-map"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"albertnahas"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Share your <em>core</em> values with <em>Claude</em> Codd every time"},"url":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"https://github.com/albertnahas/<em>claude</em>-<em>core</em>-values"}},"_tags":["story","author_albertnahas","story_47044838"],"author":"albertnahas","children":[47044839],"created_at":"2026-02-17T08:00:46Z","created_at_i":1771315246,"num_comments":1,"objectID":"47044838","points":1,"story_id":47044838,"title":"Share your core values with Claude Codd every time","updated_at":"2026-02-17T09:11:47Z","url":"https://github.com/albertnahas/claude-core-values"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"WeberG619"},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Show HN: Agent Forge \u2013 Persistent memory and desktop automation for <em>Claude</em> <em>Code</em>"},"url":{"matchLevel":"none","matchedWords":[],"value":"https://github.com/WeberG619/agent-forge"}},"_tags":["story","author_WeberG619","story_47044282","show_hn"],"author":"WeberG619","created_at":"2026-02-17T06:16:11Z","created_at_i":1771308971,"num_comments":0,"objectID":"47044282","points":3,"story_id":47044282,"title":"Show HN: Agent Forge \u2013 Persistent memory and desktop automation for Claude Code","updated_at":"2026-02-17T08:50:47Z","url":"https://github.com/WeberG619/agent-forge"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"coolwulf"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Hey HN,<p>I've been building CoolWulf AI (<a href=\"https://coolwulfai.com\" rel=\"nofollow\">https://coolwulfai.com</a>), a self-hosted personal AI assistant. After seeing OpenClaw blow up, I wanted to share what I've been working on \u2014 a different approach to the same problem.<p>*Why I built this:*<p>I tried OpenClaw and found the <em>Node</em>.js/TypeScript stack heavy for what's essentially a local agent. pnpm, <em>Node</em> 22+, React \u2014 lots of moving parts. I wanted something that's a single binary, zero runtime dependencies, and feels native on macOS. So I built it in Go.<p>*How it's different from OpenClaw:*<p>- *Single binary, no runtime needed.* Download, set your API key, run. No <em>Node</em>.js, no pnpm, no build step. One ~100 MB binary.\n- *Built in Go.* Fast startup, low memory footprint, compiles to a native executable. No garbage collector pauses from a JS runtime sitting in the background.\n- *macOS-native integrations.* Deep AppleScript-based control of Apple Notes, Reminders, Calendar, Terminal.app, and WeChat desktop. These aren't browser hacks \u2014 they use the native accessibility and scripting APIs.\n- *WeChat support.* This was a big one for me. WeChat desktop on macOS is a native Cocoa/Qt app with a readable accessibility tree. CoolWulf can read messages, send messages, search contacts, and navigate chats \u2014 all via AX APIs and CGEvent. I haven't seen another AI agent do this.\n- *Simpler setup.* Web-based first-time wizard. Configure your LLM provider, connect Gmail/Calendar via OAuth, enable messaging connectors \u2014 all from the browser. No terminal wizards, no YAML files.<p>*What it does:*<p>- 20+ LLM providers (OpenAI, <em>Claude</em>, Gemini, DeepSeek, Groq, Ollama, local models via vLLM/LM Studio)\n- Messaging: WhatsApp, Teams, Telegram, Slack, WeChat\n- Email: Gmail and Yahoo Mail with full OAuth\n- Calendar: Google Calendar + Apple Calendar\n- Browser automation: Chrome CDP + Playwright via MCP\n- Task management: org-mode style with scheduled tasks, cron jobs, automatic execution\n- Persistent memory: SQLite + vector embeddings for semantic search across conversations\n- Background heartbeat: runs 24/7, monitors your systems, executes due tasks, sends alerts\n- Web dashboard for chat, tasks, scheduled jobs, and settings<p>*Architecture choices:*<p>Go was the right call. The binary compiles in seconds, cross-compiles trivially, and the concurrency model (goroutines for heartbeat, browser sessions, connector polling) maps perfectly to an always-on agent. SQLite with vector extensions (sqlite-vec) gives us semantic memory without running a separate vector DB.<p>Try it: <a href=\"https://coolwulfai.com\" rel=\"nofollow\">https://coolwulfai.com</a><p>Happy to answer questions about the Go implementation, WeChat automation, or the macOS accessibility approach."},"title":{"matchLevel":"none","matchedWords":[],"value":"Show HN: CoolWulf AI \u2013 A personal AI assistant built in Go, optimized for macOS"},"url":{"matchLevel":"none","matchedWords":[],"value":"http://coolwulfAI.com"}},"_tags":["story","author_coolwulf","story_47044036","show_hn"],"author":"coolwulf","children":[47044074],"created_at":"2026-02-17T05:32:16Z","created_at_i":1771306336,"num_comments":1,"objectID":"47044036","points":1,"story_id":47044036,"story_text":"Hey HN,<p>I&#x27;ve been building CoolWulf AI (<a href=\"https:&#x2F;&#x2F;coolwulfai.com\" rel=\"nofollow\">https:&#x2F;&#x2F;coolwulfai.com</a>), a self-hosted personal AI assistant. After seeing OpenClaw blow up, I wanted to share what I&#x27;ve been working on \u2014 a different approach to the same problem.<p>*Why I built this:*<p>I tried OpenClaw and found the Node.js&#x2F;TypeScript stack heavy for what&#x27;s essentially a local agent. pnpm, Node 22+, React \u2014 lots of moving parts. I wanted something that&#x27;s a single binary, zero runtime dependencies, and feels native on macOS. So I built it in Go.<p>*How it&#x27;s different from OpenClaw:*<p>- *Single binary, no runtime needed.* Download, set your API key, run. No Node.js, no pnpm, no build step. One ~100 MB binary.\n- *Built in Go.* Fast startup, low memory footprint, compiles to a native executable. No garbage collector pauses from a JS runtime sitting in the background.\n- *macOS-native integrations.* Deep AppleScript-based control of Apple Notes, Reminders, Calendar, Terminal.app, and WeChat desktop. These aren&#x27;t browser hacks \u2014 they use the native accessibility and scripting APIs.\n- *WeChat support.* This was a big one for me. WeChat desktop on macOS is a native Cocoa&#x2F;Qt app with a readable accessibility tree. CoolWulf can read messages, send messages, search contacts, and navigate chats \u2014 all via AX APIs and CGEvent. I haven&#x27;t seen another AI agent do this.\n- *Simpler setup.* Web-based first-time wizard. Configure your LLM provider, connect Gmail&#x2F;Calendar via OAuth, enable messaging connectors \u2014 all from the browser. No terminal wizards, no YAML files.<p>*What it does:*<p>- 20+ LLM providers (OpenAI, Claude, Gemini, DeepSeek, Groq, Ollama, local models via vLLM&#x2F;LM Studio)\n- Messaging: WhatsApp, Teams, Telegram, Slack, WeChat\n- Email: Gmail and Yahoo Mail with full OAuth\n- Calendar: Google Calendar + Apple Calendar\n- Browser automation: Chrome CDP + Playwright via MCP\n- Task management: org-mode style with scheduled tasks, cron jobs, automatic execution\n- Persistent memory: SQLite + vector embeddings for semantic search across conversations\n- Background heartbeat: runs 24&#x2F;7, monitors your systems, executes due tasks, sends alerts\n- Web dashboard for chat, tasks, scheduled jobs, and settings<p>*Architecture choices:*<p>Go was the right call. The binary compiles in seconds, cross-compiles trivially, and the concurrency model (goroutines for heartbeat, browser sessions, connector polling) maps perfectly to an always-on agent. SQLite with vector extensions (sqlite-vec) gives us semantic memory without running a separate vector DB.<p>Try it: <a href=\"https:&#x2F;&#x2F;coolwulfai.com\" rel=\"nofollow\">https:&#x2F;&#x2F;coolwulfai.com</a><p>Happy to answer questions about the Go implementation, WeChat automation, or the macOS accessibility approach.","title":"Show HN: CoolWulf AI \u2013 A personal AI assistant built in Go, optimized for macOS","updated_at":"2026-02-17T06:39:16Z","url":"http://coolwulfAI.com"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"rittermax"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Start a task, grab a coffee, come back to production-grade <em>code</em>.  \nTests enforced. Context preserved. Quality automated.<p><em>Claude</em> <em>Code</em> moves fast but without structure, it skips tests, loses context, and produces inconsistent results \u2014 especially on complex, established codebases. I tried other frameworks \u2014 they burned tokens on bloated prompts without adding real value. Some added process without enforcement. Others were prompt templates that <em>Claude</em> ignored when context got tight. None made <em>Claude</em> reliably produce production-grade <em>code</em>.<p>So I built Pilot. Instead of adding process on top, it bakes quality into every interaction. Linting, formatting, and type checking run as enforced hooks on every edit. TDD is mandatory, not suggested. Context is monitored and preserved across sessions. Every piece of work goes through verification before it's marked done.<p>Pilot optimizes for output quality, not system complexity. The rules are minimal and focused. There's no big learning curve, no project scaffolding to set up, no state files to manage. You install it in any existing project \u2014 no matter how complex \u2014 run `pilot`, then `/sync` to learn your codebase, and the quality guardrails are just there \u2014 hooks, TDD, type checking, formatting \u2014 enforced automatically on every edit, in every session.<p>The result: you can actually walk away. Start a `/spec` task, approve the plan, then go grab a coffee. When you come back, the work is done \u2014 tested, verified, formatted, and ready to ship. Hooks preserve state across compaction cycles, persistent memory carries context between sessions, quality hooks catch every mistake along the way, and verifier agents review the <em>code</em> before marking it complete. No babysitting required."},"title":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"Show HN: <em>Claude</em> Pilot \u2013 <em>Claude</em> <em>Code</em> is powerful. Pilot makes it reliable"},"url":{"fullyHighlighted":false,"matchLevel":"partial","matchedWords":["claude"],"value":"https://github.com/maxritter/<em>claude</em>-pilot"}},"_tags":["story","author_rittermax","story_47043847","show_hn"],"author":"rittermax","created_at":"2026-02-17T04:55:29Z","created_at_i":1771304129,"num_comments":0,"objectID":"47043847","points":1,"story_id":47043847,"story_text":"Start a task, grab a coffee, come back to production-grade code.  \nTests enforced. Context preserved. Quality automated.<p>Claude Code moves fast but without structure, it skips tests, loses context, and produces inconsistent results \u2014 especially on complex, established codebases. I tried other frameworks \u2014 they burned tokens on bloated prompts without adding real value. Some added process without enforcement. Others were prompt templates that Claude ignored when context got tight. None made Claude reliably produce production-grade code.<p>So I built Pilot. Instead of adding process on top, it bakes quality into every interaction. Linting, formatting, and type checking run as enforced hooks on every edit. TDD is mandatory, not suggested. Context is monitored and preserved across sessions. Every piece of work goes through verification before it&#x27;s marked done.<p>Pilot optimizes for output quality, not system complexity. The rules are minimal and focused. There&#x27;s no big learning curve, no project scaffolding to set up, no state files to manage. You install it in any existing project \u2014 no matter how complex \u2014 run `pilot`, then `&#x2F;sync` to learn your codebase, and the quality guardrails are just there \u2014 hooks, TDD, type checking, formatting \u2014 enforced automatically on every edit, in every session.<p>The result: you can actually walk away. Start a `&#x2F;spec` task, approve the plan, then go grab a coffee. When you come back, the work is done \u2014 tested, verified, formatted, and ready to ship. Hooks preserve state across compaction cycles, persistent memory carries context between sessions, quality hooks catch every mistake along the way, and verifier agents review the code before marking it complete. No babysitting required.","title":"Show HN: Claude Pilot \u2013 Claude Code is powerful. Pilot makes it reliable","updated_at":"2026-02-17T04:57:01Z","url":"https://github.com/maxritter/claude-pilot"},{"_highlightResult":{"author":{"matchLevel":"none","matchedWords":[],"value":"Poomba"},"story_text":{"fullyHighlighted":false,"matchLevel":"full","matchedWords":["claude","code"],"value":"I'm trying to decide whether to adopt Cursor for our company, but we're in a heavily regulated industry and our compliance team is flagging concerns about HIPAA/SOC2/audit trails.<p>The thing is, there are companies in regulated industries using it [1][2]. But Cursor has no HIPAA BAA, no FedRAMP certification, and is cloud-only with all requests routing through their AWS infrastructure. (This is probably true for <em>Claude</em> and other <em>codi</em>ng assistants, though I've only looked seriously at Cursor.)<p>So how are regulated companies actually making this work? Or do most just avoid Cursor and other AI <em>codi</em>ng tools altogether?<p>[1] 165 healthcare companies use Cursor according to Bloomberry: https://bloomberry.com/data/cursor/<p>[2] Cursor's customers include Sanofi, Johnson &amp; Johnson, and Neuralink: https://cursor.com/customers"},"title":{"matchLevel":"none","matchedWords":[],"value":"Ask HN: How do companies that use Cursor handle compliance?"}},"_tags":["story","author_Poomba","story_47043484","ask_hn"],"author":"Poomba","children":[47043995,47043504,47043810],"created_at":"2026-02-17T03:48:00Z","created_at_i":1771300080,"num_comments":2,"objectID":"47043484","points":6,"story_id":47043484,"story_text":"I&#x27;m trying to decide whether to adopt Cursor for our company, but we&#x27;re in a heavily regulated industry and our compliance team is flagging concerns about HIPAA&#x2F;SOC2&#x2F;audit trails.<p>The thing is, there are companies in regulated industries using it [1][2]. But Cursor has no HIPAA BAA, no FedRAMP certification, and is cloud-only with all requests routing through their AWS infrastructure. (This is probably true for Claude and other coding assistants, though I&#x27;ve only looked seriously at Cursor.)<p>So how are regulated companies actually making this work? Or do most just avoid Cursor and other AI coding tools altogether?<p>[1] 165 healthcare companies use Cursor according to Bloomberry: https:&#x2F;&#x2F;bloomberry.com&#x2F;data&#x2F;cursor&#x2F;<p>[2] Cursor&#x27;s customers include Sanofi, Johnson &amp; Johnson, and Neuralink: https:&#x2F;&#x2F;cursor.com&#x2F;customers","title":"Ask HN: How do companies that use Cursor handle compliance?","updated_at":"2026-02-17T12:40:02Z"}],"hitsPerPage":10,"nbHits":3913,"nbPages":100,"page":0,"params":"query=Claude+Code&tags=story&hitsPerPage=10&advancedSyntax=true&analyticsTags=backend","processingTimeMS":18,"processingTimingsMS":{"_request":{"roundTrip":16},"afterFetch":{"format":{"total":1}},"fetch":{"query":6,"scanning":10,"total":17},"total":18},"query":"Claude Code","serverTimeMS":19}
